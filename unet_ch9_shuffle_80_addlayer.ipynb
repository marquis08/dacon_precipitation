{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import tqdm\n",
    "import argparse\n",
    "import tqdm\n",
    "from collections import defaultdict, Counter\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import nn, cuda\n",
    "from torch.autograd import Variable \n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import CenterCrop\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR\n",
    "\n",
    "# from efficientnet_pytorch import EfficientNet\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim import Optimizer\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def mae(y_true, y_pred) :\n",
    "    y_true, y_pred = np.array(y_true.detach().numpy()), np.array(y_pred.detach().numpy())\n",
    "    y_true = y_true.reshape(1, -1)[0]\n",
    "    y_pred = y_pred.reshape(1, -1)[0]\n",
    "    over_threshold = y_true >= 0.1\n",
    "    return np.mean(np.abs(y_true[over_threshold] - y_pred[over_threshold]))\n",
    "\n",
    "def fscore(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true.detach().numpy()), np.array(y_pred.detach().numpy())\n",
    "    y_true = y_true.reshape(1, -1)[0]\n",
    "    y_pred = y_pred.reshape(1, -1)[0]\n",
    "    remove_NAs = y_true >= 0\n",
    "    y_true = np.where(y_true[remove_NAs] >= 0.1, 1, 0)\n",
    "    y_pred = np.where(y_pred[remove_NAs] >= 0.1, 1, 0)\n",
    "    return(f1_score(y_true, y_pred))\n",
    "\n",
    "def maeOverFscore(y_true, y_pred):\n",
    "    return mae(y_true, y_pred) / (fscore(y_true, y_pred) + 1e-07)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **File info**\n",
    "**ex. subset_010462_01**\n",
    "> **orbit 010462**\n",
    "\n",
    "> **subset 01**\n",
    "\n",
    "> **ortbit 별로 subset 개수는 다를 수 있고 연속적이지 않을 수도 있음**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>orbit</th>\n",
       "      <th>orbit_subset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../D_WEATHER//input/train/subset_010462_01.npy</td>\n",
       "      <td>10462</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../D_WEATHER//input/train/subset_010462_02.npy</td>\n",
       "      <td>10462</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../D_WEATHER//input/train/subset_010462_03.npy</td>\n",
       "      <td>10462</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../D_WEATHER//input/train/subset_010462_04.npy</td>\n",
       "      <td>10462</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../D_WEATHER//input/train/subset_010462_05.npy</td>\n",
       "      <td>10462</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             path  orbit  orbit_subset\n",
       "0  ../D_WEATHER//input/train/subset_010462_01.npy  10462             1\n",
       "1  ../D_WEATHER//input/train/subset_010462_02.npy  10462             2\n",
       "2  ../D_WEATHER//input/train/subset_010462_03.npy  10462             3\n",
       "3  ../D_WEATHER//input/train/subset_010462_04.npy  10462             4\n",
       "4  ../D_WEATHER//input/train/subset_010462_05.npy  10462             5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_df = pd.read_csv(\"../D_WEATHER//input/train_df.csv\")\n",
    "te_df = pd.read_csv(\"../D_WEATHER/input/test_df.csv\")\n",
    "tr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = tr_df[:int(len(tr_df)*0.8)]\n",
    "valid_df = tr_df[int(len(tr_df)*0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((61076, 3), (15269, 3))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, valid_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize(image, size=(80, 80)):\n",
    "    return cv2.resize(image, size)\n",
    "\n",
    "class Weather_Dataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        \n",
    "        self.image_list = []\n",
    "        self.label_list = []\n",
    "\n",
    "        for file in self.df['path']:\n",
    "            data = np.load(file)\n",
    "            image = data[:,:,:9] # use 14 channels except target\n",
    "            image = resize(image)\n",
    "            image = np.transpose(image, (2,0,1))\n",
    "            image = image.astype(np.float32)\n",
    "            self.image_list.append(image)\n",
    "            \n",
    "            \n",
    "            label = data[:,:,-1].reshape(40,40,1)\n",
    "            label = np.transpose(label, (2,0,1))\n",
    "            self.label_list.append(label)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        image = self.image_list[idx]\n",
    "        label = self.label_list[idx]\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def worker_init(worker_id):\n",
    "#     np.random.seed(SEED)\n",
    "\n",
    "def build_dataloader(df, batch_size, shuffle=False):\n",
    "    dataset = Weather_Dataset(df)\n",
    "    dataloader = DataLoader(\n",
    "                            dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=shuffle,\n",
    "                            num_workers=0,\n",
    "#                             worker_init_fn=worker_init\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "def build_te_dataloader(df, batch_size, shuffle=False):\n",
    "    dataset = Test_Dataset(df)\n",
    "    dataloader = DataLoader(\n",
    "                            dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=shuffle,\n",
    "                            num_workers=0,\n",
    "#                             worker_init_fn=worker_init\n",
    "                            )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=True):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels # \n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        self.down4 = Down(512, 1024)\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down5 = Down(1024, 2048 // factor)\n",
    "        \n",
    "        self.up1 = Up(2048, 1024, bilinear)\n",
    "        self.up2 = Up(1024, 512, bilinear)\n",
    "        self.up3 = Up(512, 256, bilinear)\n",
    "        self.up4 = Up(256, 128, bilinear)\n",
    "        self.up5 = Up(128, 64 * factor, bilinear)\n",
    "        self.outc = OutConv(64, n_classes)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x6 = self.down5(x5)\n",
    "        \n",
    "        x = self.up1(x6, x5)\n",
    "        x = self.up2(x, x4)\n",
    "        x = self.up3(x, x3)\n",
    "        x = self.up4(x, x2)\n",
    "        x = self.up5(x, x1)\n",
    "        x = self.pool(x)\n",
    "        logits = self.outc(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels // 2, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels , in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        diffY = torch.tensor([x2.size()[2] - x1.size()[2]])\n",
    "        diffX = torch.tensor([x2.size()[3] - x1.size()[3]])\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        # if you have padding issues, see\n",
    "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
    "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32.7 s, sys: 13.7 s, total: 46.4 s\n",
      "Wall time: 4min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_loader = build_dataloader(train_df, batch_size, shuffle=True)\n",
    "valid_loader = build_dataloader(valid_df, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enable gpu use\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "seed_everything(SEED)\n",
    "\n",
    "device = 'cuda:0'\n",
    "use_gpu = cuda.is_available()\n",
    "if use_gpu:\n",
    "    print(\"enable gpu use\")\n",
    "else:\n",
    "    print(\"enable cpu for debugging\")\n",
    "\n",
    "model = UNet(n_channels=9, n_classes=1, bilinear=False) # if bilinear = True -> non deterministic : not recommended\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 80, 80]           5,248\n",
      "       BatchNorm2d-2           [-1, 64, 80, 80]             128\n",
      "              ReLU-3           [-1, 64, 80, 80]               0\n",
      "            Conv2d-4           [-1, 64, 80, 80]          36,928\n",
      "       BatchNorm2d-5           [-1, 64, 80, 80]             128\n",
      "              ReLU-6           [-1, 64, 80, 80]               0\n",
      "        DoubleConv-7           [-1, 64, 80, 80]               0\n",
      "         MaxPool2d-8           [-1, 64, 40, 40]               0\n",
      "            Conv2d-9          [-1, 128, 40, 40]          73,856\n",
      "      BatchNorm2d-10          [-1, 128, 40, 40]             256\n",
      "             ReLU-11          [-1, 128, 40, 40]               0\n",
      "           Conv2d-12          [-1, 128, 40, 40]         147,584\n",
      "      BatchNorm2d-13          [-1, 128, 40, 40]             256\n",
      "             ReLU-14          [-1, 128, 40, 40]               0\n",
      "       DoubleConv-15          [-1, 128, 40, 40]               0\n",
      "             Down-16          [-1, 128, 40, 40]               0\n",
      "        MaxPool2d-17          [-1, 128, 20, 20]               0\n",
      "           Conv2d-18          [-1, 256, 20, 20]         295,168\n",
      "      BatchNorm2d-19          [-1, 256, 20, 20]             512\n",
      "             ReLU-20          [-1, 256, 20, 20]               0\n",
      "           Conv2d-21          [-1, 256, 20, 20]         590,080\n",
      "      BatchNorm2d-22          [-1, 256, 20, 20]             512\n",
      "             ReLU-23          [-1, 256, 20, 20]               0\n",
      "       DoubleConv-24          [-1, 256, 20, 20]               0\n",
      "             Down-25          [-1, 256, 20, 20]               0\n",
      "        MaxPool2d-26          [-1, 256, 10, 10]               0\n",
      "           Conv2d-27          [-1, 512, 10, 10]       1,180,160\n",
      "      BatchNorm2d-28          [-1, 512, 10, 10]           1,024\n",
      "             ReLU-29          [-1, 512, 10, 10]               0\n",
      "           Conv2d-30          [-1, 512, 10, 10]       2,359,808\n",
      "      BatchNorm2d-31          [-1, 512, 10, 10]           1,024\n",
      "             ReLU-32          [-1, 512, 10, 10]               0\n",
      "       DoubleConv-33          [-1, 512, 10, 10]               0\n",
      "             Down-34          [-1, 512, 10, 10]               0\n",
      "        MaxPool2d-35            [-1, 512, 5, 5]               0\n",
      "           Conv2d-36           [-1, 1024, 5, 5]       4,719,616\n",
      "      BatchNorm2d-37           [-1, 1024, 5, 5]           2,048\n",
      "             ReLU-38           [-1, 1024, 5, 5]               0\n",
      "           Conv2d-39           [-1, 1024, 5, 5]       9,438,208\n",
      "      BatchNorm2d-40           [-1, 1024, 5, 5]           2,048\n",
      "             ReLU-41           [-1, 1024, 5, 5]               0\n",
      "       DoubleConv-42           [-1, 1024, 5, 5]               0\n",
      "             Down-43           [-1, 1024, 5, 5]               0\n",
      "        MaxPool2d-44           [-1, 1024, 2, 2]               0\n",
      "           Conv2d-45           [-1, 2048, 2, 2]      18,876,416\n",
      "      BatchNorm2d-46           [-1, 2048, 2, 2]           4,096\n",
      "             ReLU-47           [-1, 2048, 2, 2]               0\n",
      "           Conv2d-48           [-1, 2048, 2, 2]      37,750,784\n",
      "      BatchNorm2d-49           [-1, 2048, 2, 2]           4,096\n",
      "             ReLU-50           [-1, 2048, 2, 2]               0\n",
      "       DoubleConv-51           [-1, 2048, 2, 2]               0\n",
      "             Down-52           [-1, 2048, 2, 2]               0\n",
      "  ConvTranspose2d-53           [-1, 1024, 4, 4]       8,389,632\n",
      "           Conv2d-54           [-1, 1024, 5, 5]      18,875,392\n",
      "      BatchNorm2d-55           [-1, 1024, 5, 5]           2,048\n",
      "             ReLU-56           [-1, 1024, 5, 5]               0\n",
      "           Conv2d-57           [-1, 1024, 5, 5]       9,438,208\n",
      "      BatchNorm2d-58           [-1, 1024, 5, 5]           2,048\n",
      "             ReLU-59           [-1, 1024, 5, 5]               0\n",
      "       DoubleConv-60           [-1, 1024, 5, 5]               0\n",
      "               Up-61           [-1, 1024, 5, 5]               0\n",
      "  ConvTranspose2d-62          [-1, 512, 10, 10]       2,097,664\n",
      "           Conv2d-63          [-1, 512, 10, 10]       4,719,104\n",
      "      BatchNorm2d-64          [-1, 512, 10, 10]           1,024\n",
      "             ReLU-65          [-1, 512, 10, 10]               0\n",
      "           Conv2d-66          [-1, 512, 10, 10]       2,359,808\n",
      "      BatchNorm2d-67          [-1, 512, 10, 10]           1,024\n",
      "             ReLU-68          [-1, 512, 10, 10]               0\n",
      "       DoubleConv-69          [-1, 512, 10, 10]               0\n",
      "               Up-70          [-1, 512, 10, 10]               0\n",
      "  ConvTranspose2d-71          [-1, 256, 20, 20]         524,544\n",
      "           Conv2d-72          [-1, 256, 20, 20]       1,179,904\n",
      "      BatchNorm2d-73          [-1, 256, 20, 20]             512\n",
      "             ReLU-74          [-1, 256, 20, 20]               0\n",
      "           Conv2d-75          [-1, 256, 20, 20]         590,080\n",
      "      BatchNorm2d-76          [-1, 256, 20, 20]             512\n",
      "             ReLU-77          [-1, 256, 20, 20]               0\n",
      "       DoubleConv-78          [-1, 256, 20, 20]               0\n",
      "               Up-79          [-1, 256, 20, 20]               0\n",
      "  ConvTranspose2d-80          [-1, 128, 40, 40]         131,200\n",
      "           Conv2d-81          [-1, 128, 40, 40]         295,040\n",
      "      BatchNorm2d-82          [-1, 128, 40, 40]             256\n",
      "             ReLU-83          [-1, 128, 40, 40]               0\n",
      "           Conv2d-84          [-1, 128, 40, 40]         147,584\n",
      "      BatchNorm2d-85          [-1, 128, 40, 40]             256\n",
      "             ReLU-86          [-1, 128, 40, 40]               0\n",
      "       DoubleConv-87          [-1, 128, 40, 40]               0\n",
      "               Up-88          [-1, 128, 40, 40]               0\n",
      "  ConvTranspose2d-89           [-1, 64, 80, 80]          32,832\n",
      "           Conv2d-90           [-1, 64, 80, 80]          73,792\n",
      "      BatchNorm2d-91           [-1, 64, 80, 80]             128\n",
      "             ReLU-92           [-1, 64, 80, 80]               0\n",
      "           Conv2d-93           [-1, 64, 80, 80]          36,928\n",
      "      BatchNorm2d-94           [-1, 64, 80, 80]             128\n",
      "             ReLU-95           [-1, 64, 80, 80]               0\n",
      "       DoubleConv-96           [-1, 64, 80, 80]               0\n",
      "               Up-97           [-1, 64, 80, 80]               0\n",
      "        MaxPool2d-98           [-1, 64, 40, 40]               0\n",
      "           Conv2d-99            [-1, 1, 40, 40]             577\n",
      "         OutConv-100            [-1, 1, 40, 40]               0\n",
      "================================================================\n",
      "Total params: 124,390,209\n",
      "Trainable params: 124,390,209\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.22\n",
      "Forward/backward pass size (MB): 102.54\n",
      "Params size (MB): 474.51\n",
      "Estimated Total Size (MB): 577.27\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model, input_size=(9,80,80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training\n",
      "================ ༼ つ ◕_◕ ༽つ BEST epoch : 1, MOF : 8.212625764804422 \n",
      "E 1/200 tr_loss: 42.64174 tr_mae: 1.71046 tr_fs: 0.57175 tr_mof: 3.34741 val_loss: 52.44971 val_mae: 1.78854 val_fs: 0.24516 val_mof: 8.21263 lr: 0.001000 elapsed: 306\n",
      "================ ༼ つ ◕_◕ ༽つ BEST epoch : 2, MOF : 5.903497852371778 \n",
      "E 2/200 tr_loss: 42.61876 tr_mae: 1.53132 tr_fs: 0.65577 tr_mof: 2.34345 val_loss: 52.19124 val_mae: 1.65210 val_fs: 0.29952 val_mof: 5.90350 lr: 0.001000 elapsed: 307\n",
      "================ ༼ つ ◕_◕ ༽つ BEST epoch : 3, MOF : 2.6167344075046275 \n",
      "E 3/200 tr_loss: 42.61553 tr_mae: 1.49910 tr_fs: 0.66749 tr_mof: 2.25218 val_loss: 52.06011 val_mae: 1.67151 val_fs: 0.63720 val_mof: 2.61673 lr: 0.001000 elapsed: 305\n",
      "E 4/200 tr_loss: 42.61275 tr_mae: 1.46271 tr_fs: 0.68183 tr_mof: 2.15095 val_loss: 52.04921 val_mae: 1.70354 val_fs: 0.64347 val_mof: 2.64899 lr: 0.001000 elapsed: 304\n",
      "E 5/200 tr_loss: 42.61196 tr_mae: 1.44434 tr_fs: 0.68936 tr_mof: 2.09799 val_loss: 52.05238 val_mae: 1.80031 val_fs: 0.54599 val_mof: 3.36902 lr: 0.001000 elapsed: 302\n",
      "E 6/200 tr_loss: 42.61010 tr_mae: 1.42125 tr_fs: 0.70317 tr_mof: 2.02196 val_loss: 52.04537 val_mae: 1.69844 val_fs: 0.59079 val_mof: 2.92217 lr: 0.001000 elapsed: 302\n",
      "E 7/200 tr_loss: 42.60891 tr_mae: 1.40366 tr_fs: 0.71269 tr_mof: 1.97117 val_loss: 52.04556 val_mae: 1.50342 val_fs: 0.57205 val_mof: 2.65056 lr: 0.001000 elapsed: 302\n",
      "E 8/200 tr_loss: 43.49036 tr_mae: 1.38884 tr_fs: 0.71411 tr_mof: 1.94681 val_loss: 52.10844 val_mae: 1.65194 val_fs: 0.31493 val_mof: 5.35964 lr: 0.001000 elapsed: 301\n",
      "================ ༼ つ ◕_◕ ༽つ BEST epoch : 9, MOF : 2.3029081696761744 \n",
      "E 9/200 tr_loss: 42.60797 tr_mae: 1.39308 tr_fs: 0.71626 tr_mof: 1.94742 val_loss: 52.03683 val_mae: 1.55133 val_fs: 0.67109 val_mof: 2.30291 lr: 0.001000 elapsed: 301\n",
      "E 10/200 tr_loss: 42.60668 tr_mae: 1.37321 tr_fs: 0.72296 tr_mof: 1.90069 val_loss: 52.06531 val_mae: 1.96443 val_fs: 0.33852 val_mof: 6.04382 lr: 0.001000 elapsed: 300\n",
      "E 11/200 tr_loss: 42.60797 tr_mae: 1.39381 tr_fs: 0.71462 tr_mof: 1.95444 val_loss: 52.06389 val_mae: 1.95303 val_fs: 0.45157 val_mof: 4.40164 lr: 0.001000 elapsed: 301\n",
      "E 12/200 tr_loss: 42.60600 tr_mae: 1.36897 tr_fs: 0.72416 tr_mof: 1.89116 val_loss: 52.05876 val_mae: 1.88572 val_fs: 0.44593 val_mof: 4.31937 lr: 0.001000 elapsed: 300\n",
      "E 13/200 tr_loss: 42.60598 tr_mae: 1.36920 tr_fs: 0.71884 tr_mof: 1.90571 val_loss: 52.04541 val_mae: 1.63754 val_fs: 0.69400 val_mof: 2.34843 lr: 0.001000 elapsed: 300\n",
      "E 14/200 tr_loss: 42.60455 tr_mae: 1.34698 tr_fs: 0.73437 tr_mof: 1.83350 val_loss: 52.03976 val_mae: 1.61390 val_fs: 0.59291 val_mof: 2.80774 lr: 0.001000 elapsed: 300\n",
      "E 15/200 tr_loss: 42.60375 tr_mae: 1.33494 tr_fs: 0.73663 tr_mof: 1.81219 val_loss: 52.03680 val_mae: 1.57880 val_fs: 0.64016 val_mof: 2.46288 lr: 0.001000 elapsed: 300\n",
      "================ ༼ つ ◕_◕ ༽つ BEST epoch : 16, MOF : 2.2189840526526754 \n",
      "E 16/200 tr_loss: 42.60234 tr_mae: 1.31721 tr_fs: 0.74558 tr_mof: 1.76649 val_loss: 52.04296 val_mae: 1.61415 val_fs: 0.72196 val_mof: 2.21898 lr: 0.000500 elapsed: 300\n",
      "E 17/200 tr_loss: 42.60229 tr_mae: 1.31561 tr_fs: 0.74525 tr_mof: 1.76560 val_loss: 52.05542 val_mae: 1.83960 val_fs: 0.40458 val_mof: 4.73984 lr: 0.000500 elapsed: 299\n",
      "E 18/200 tr_loss: 42.60184 tr_mae: 1.31102 tr_fs: 0.74761 tr_mof: 1.75456 val_loss: 52.03352 val_mae: 1.54972 val_fs: 0.60566 val_mof: 2.58237 lr: 0.000500 elapsed: 299\n",
      "E 19/200 tr_loss: 42.60175 tr_mae: 1.31219 tr_fs: 0.74633 tr_mof: 1.75843 val_loss: 53.29431 val_mae: 2.58902 val_fs: 0.26075 val_mof: 10.57479 lr: 0.000500 elapsed: 298\n",
      "E 20/200 tr_loss: 42.60187 tr_mae: 1.30673 tr_fs: 0.74931 tr_mof: 1.74266 val_loss: 52.07369 val_mae: 1.41687 val_fs: 0.42254 val_mof: 3.38375 lr: 0.000500 elapsed: 299\n",
      "E 21/200 tr_loss: 42.60145 tr_mae: 1.30145 tr_fs: 0.75062 tr_mof: 1.73383 val_loss: 52.04001 val_mae: 1.64031 val_fs: 0.61113 val_mof: 2.69766 lr: 0.000500 elapsed: 298\n",
      "================ ༼ つ ◕_◕ ༽つ BEST epoch : 22, MOF : 2.1699378994215883 \n",
      "E 22/200 tr_loss: 42.60122 tr_mae: 1.29811 tr_fs: 0.75106 tr_mof: 1.72806 val_loss: 52.03396 val_mae: 1.50660 val_fs: 0.69530 val_mof: 2.16994 lr: 0.000500 elapsed: 298\n",
      "================ ༼ つ ◕_◕ ༽つ BEST epoch : 23, MOF : 2.0997989054128308 \n",
      "E 23/200 tr_loss: 42.60146 tr_mae: 1.30354 tr_fs: 0.75047 tr_mof: 1.73657 val_loss: 52.03634 val_mae: 1.45402 val_fs: 0.68851 val_mof: 2.09980 lr: 0.000500 elapsed: 298\n",
      "E 24/200 tr_loss: 42.60093 tr_mae: 1.29797 tr_fs: 0.75312 tr_mof: 1.72368 val_loss: 52.03366 val_mae: 1.51990 val_fs: 0.67722 val_mof: 2.24122 lr: 0.000500 elapsed: 297\n",
      "E 25/200 tr_loss: 42.60087 tr_mae: 1.29323 tr_fs: 0.75403 tr_mof: 1.71508 val_loss: 52.03370 val_mae: 1.42851 val_fs: 0.65570 val_mof: 2.19310 lr: 0.000500 elapsed: 297\n",
      "E 26/200 tr_loss: 42.60102 tr_mae: 1.29566 tr_fs: 0.75183 tr_mof: 1.72362 val_loss: 52.04268 val_mae: 1.54480 val_fs: 0.71105 val_mof: 2.15663 lr: 0.000500 elapsed: 297\n",
      "E 27/200 tr_loss: 42.60069 tr_mae: 1.29467 tr_fs: 0.75475 tr_mof: 1.71584 val_loss: 52.09716 val_mae: 1.52273 val_fs: 0.29717 val_mof: 5.22963 lr: 0.000500 elapsed: 297\n",
      "E 28/200 tr_loss: 42.60062 tr_mae: 1.29081 tr_fs: 0.75462 tr_mof: 1.71070 val_loss: 52.06742 val_mae: 1.48660 val_fs: 0.51771 val_mof: 3.14184 lr: 0.000500 elapsed: 297\n",
      "E 29/200 tr_loss: 42.60098 tr_mae: 1.29586 tr_fs: 0.75185 tr_mof: 1.72318 val_loss: 52.64942 val_mae: 1.86793 val_fs: 0.33292 val_mof: 5.74607 lr: 0.000500 elapsed: 297\n",
      "================ ༼ つ ◕_◕ ༽つ BEST epoch : 30, MOF : 1.8870705474153344 \n",
      "E 30/200 tr_loss: 42.59949 tr_mae: 1.27318 tr_fs: 0.75996 tr_mof: 1.67398 val_loss: 52.02649 val_mae: 1.42301 val_fs: 0.75067 val_mof: 1.88707 lr: 0.000250 elapsed: 297\n",
      "E 31/200 tr_loss: 42.59941 tr_mae: 1.27086 tr_fs: 0.76023 tr_mof: 1.67081 val_loss: 52.04463 val_mae: 1.69304 val_fs: 0.64226 val_mof: 2.63132 lr: 0.000250 elapsed: 297\n",
      "E 32/200 tr_loss: 42.59972 tr_mae: 1.27324 tr_fs: 0.75885 tr_mof: 1.67772 val_loss: 52.04469 val_mae: 1.68820 val_fs: 0.60105 val_mof: 2.81065 lr: 0.000250 elapsed: 297\n",
      "E 33/200 tr_loss: 42.59930 tr_mae: 1.27137 tr_fs: 0.75974 tr_mof: 1.67249 val_loss: 52.04145 val_mae: 1.41655 val_fs: 0.68165 val_mof: 2.07288 lr: 0.000250 elapsed: 297\n",
      "E 34/200 tr_loss: 43.48170 tr_mae: 1.26829 tr_fs: 0.76192 tr_mof: 1.66312 val_loss: 52.03382 val_mae: 1.54564 val_fs: 0.68213 val_mof: 2.26002 lr: 0.000250 elapsed: 297\n",
      "E 35/200 tr_loss: 42.59898 tr_mae: 1.26690 tr_fs: 0.76304 tr_mof: 1.65987 val_loss: 52.09715 val_mae: 2.07039 val_fs: 0.68412 val_mof: 2.98298 lr: 0.000250 elapsed: 297\n",
      "E 36/200 tr_loss: 43.48166 tr_mae: 1.26843 tr_fs: 0.76166 tr_mof: 1.66472 val_loss: 52.10399 val_mae: 1.54789 val_fs: 0.55642 val_mof: 3.01322 lr: 0.000250 elapsed: 297\n",
      "E 37/200 tr_loss: 42.59902 tr_mae: 1.26395 tr_fs: 0.76528 tr_mof: 1.65070 val_loss: 52.02687 val_mae: 1.45724 val_fs: 0.69623 val_mof: 2.08627 lr: 0.000125 elapsed: 298\n",
      "E 38/200 tr_loss: 42.83998 tr_mae: 1.26230 tr_fs: 0.76524 tr_mof: 1.64967 val_loss: 52.02644 val_mae: 1.43869 val_fs: 0.72799 val_mof: 1.96880 lr: 0.000125 elapsed: 298\n",
      "E 39/200 tr_loss: 43.48106 tr_mae: 1.25819 tr_fs: 0.76551 tr_mof: 1.64286 val_loss: 52.03362 val_mae: 1.40534 val_fs: 0.70516 val_mof: 1.98737 lr: 0.000125 elapsed: 297\n",
      "================ ༼ つ ◕_◕ ༽つ BEST epoch : 40, MOF : 1.7718039570757376 \n",
      "E 40/200 tr_loss: 42.59813 tr_mae: 1.25319 tr_fs: 0.76641 tr_mof: 1.63411 val_loss: 52.02286 val_mae: 1.36167 val_fs: 0.76462 val_mof: 1.77180 lr: 0.000125 elapsed: 298\n",
      "E 41/200 tr_loss: 42.59810 tr_mae: 1.25546 tr_fs: 0.76715 tr_mof: 1.63634 val_loss: 52.03270 val_mae: 1.52419 val_fs: 0.69248 val_mof: 2.19354 lr: 0.000125 elapsed: 297\n",
      "E 42/200 tr_loss: 42.59814 tr_mae: 1.25776 tr_fs: 0.76654 tr_mof: 1.64032 val_loss: 52.02724 val_mae: 1.35659 val_fs: 0.74857 val_mof: 1.80657 lr: 0.000125 elapsed: 297\n",
      "E 43/200 tr_loss: 42.59816 tr_mae: 1.25206 tr_fs: 0.76685 tr_mof: 1.63188 val_loss: 52.02472 val_mae: 1.42907 val_fs: 0.71197 val_mof: 2.00111 lr: 0.000125 elapsed: 297\n",
      "E 44/200 tr_loss: 42.59814 tr_mae: 1.25274 tr_fs: 0.76628 tr_mof: 1.63439 val_loss: 52.02654 val_mae: 1.42747 val_fs: 0.74617 val_mof: 1.90186 lr: 0.000125 elapsed: 298\n",
      "E 45/200 tr_loss: 42.59792 tr_mae: 1.25404 tr_fs: 0.76740 tr_mof: 1.63314 val_loss: 52.10430 val_mae: 1.44527 val_fs: 0.47263 val_mof: 3.26297 lr: 0.000125 elapsed: 297\n",
      "E 46/200 tr_loss: 42.59799 tr_mae: 1.25346 tr_fs: 0.76734 tr_mof: 1.63368 val_loss: 52.02505 val_mae: 1.39677 val_fs: 0.74887 val_mof: 1.85537 lr: 0.000125 elapsed: 298\n",
      "================ ༼ つ ◕_◕ ༽つ BEST epoch : 47, MOF : 1.7195441686632085 \n",
      "E 47/200 tr_loss: 42.59752 tr_mae: 1.24606 tr_fs: 0.76919 tr_mof: 1.61924 val_loss: 52.02356 val_mae: 1.34094 val_fs: 0.77594 val_mof: 1.71954 lr: 0.000063 elapsed: 298\n",
      "E 48/200 tr_loss: 42.59741 tr_mae: 1.24423 tr_fs: 0.76934 tr_mof: 1.61612 val_loss: 52.02069 val_mae: 1.36562 val_fs: 0.75764 val_mof: 1.79535 lr: 0.000063 elapsed: 297\n",
      "E 49/200 tr_loss: 42.59735 tr_mae: 1.24160 tr_fs: 0.76873 tr_mof: 1.61352 val_loss: 52.02042 val_mae: 1.34507 val_fs: 0.76613 val_mof: 1.74737 lr: 0.000063 elapsed: 298\n",
      "E 50/200 tr_loss: 42.59740 tr_mae: 1.24513 tr_fs: 0.76973 tr_mof: 1.61690 val_loss: 52.02310 val_mae: 1.38089 val_fs: 0.75782 val_mof: 1.81268 lr: 0.000063 elapsed: 297\n",
      "E 51/200 tr_loss: 42.59749 tr_mae: 1.24508 tr_fs: 0.76997 tr_mof: 1.61708 val_loss: 52.02074 val_mae: 1.33829 val_fs: 0.77357 val_mof: 1.72231 lr: 0.000063 elapsed: 297\n",
      "E 52/200 tr_loss: 42.59744 tr_mae: 1.24405 tr_fs: 0.77014 tr_mof: 1.61425 val_loss: 52.02202 val_mae: 1.36217 val_fs: 0.75539 val_mof: 1.79432 lr: 0.000063 elapsed: 297\n",
      "E 53/200 tr_loss: 42.59747 tr_mae: 1.24471 tr_fs: 0.77075 tr_mof: 1.61422 val_loss: 52.02078 val_mae: 1.34618 val_fs: 0.76749 val_mof: 1.74554 lr: 0.000063 elapsed: 297\n",
      "E 54/200 tr_loss: 42.59728 tr_mae: 1.24095 tr_fs: 0.77133 tr_mof: 1.60803 val_loss: 52.01987 val_mae: 1.34331 val_fs: 0.76389 val_mof: 1.75022 lr: 0.000031 elapsed: 297\n",
      "================ ༼ つ ◕_◕ ༽つ BEST epoch : 55, MOF : 1.6988186286478884 \n",
      "E 55/200 tr_loss: 42.59700 tr_mae: 1.24134 tr_fs: 0.77097 tr_mof: 1.61022 val_loss: 52.02041 val_mae: 1.32528 val_fs: 0.77627 val_mof: 1.69882 lr: 0.000031 elapsed: 297\n",
      "E 56/200 tr_loss: 42.59718 tr_mae: 1.24486 tr_fs: 0.77113 tr_mof: 1.61411 val_loss: 52.01958 val_mae: 1.33338 val_fs: 0.77288 val_mof: 1.71721 lr: 0.000031 elapsed: 297\n",
      "E 57/200 tr_loss: 42.59730 tr_mae: 1.24187 tr_fs: 0.77151 tr_mof: 1.60932 val_loss: 52.02029 val_mae: 1.35978 val_fs: 0.75506 val_mof: 1.79310 lr: 0.000031 elapsed: 297\n",
      "E 58/200 tr_loss: 42.59701 tr_mae: 1.24161 tr_fs: 0.77065 tr_mof: 1.61093 val_loss: 52.02193 val_mae: 1.37125 val_fs: 0.76752 val_mof: 1.77808 lr: 0.000031 elapsed: 298\n",
      "E 59/200 tr_loss: 42.59698 tr_mae: 1.23833 tr_fs: 0.77028 tr_mof: 1.61298 val_loss: 52.04527 val_mae: 1.33746 val_fs: 0.60755 val_mof: 2.22682 lr: 0.000031 elapsed: 297\n",
      "E 60/200 tr_loss: 42.59711 tr_mae: 1.24208 tr_fs: 0.77009 tr_mof: 1.61233 val_loss: 52.01972 val_mae: 1.33763 val_fs: 0.76742 val_mof: 1.73521 lr: 0.000031 elapsed: 297\n",
      "E 61/200 tr_loss: 43.30139 tr_mae: 1.24019 tr_fs: 0.77141 tr_mof: 1.60697 val_loss: 52.02013 val_mae: 1.34031 val_fs: 0.77215 val_mof: 1.72745 lr: 0.000031 elapsed: 298\n",
      "E 62/200 tr_loss: 42.59680 tr_mae: 1.23471 tr_fs: 0.77169 tr_mof: 1.59900 val_loss: 52.01952 val_mae: 1.32915 val_fs: 0.77512 val_mof: 1.70657 lr: 0.000016 elapsed: 297\n",
      "E 63/200 tr_loss: 42.59687 tr_mae: 1.23853 tr_fs: 0.77101 tr_mof: 1.60611 val_loss: 52.01974 val_mae: 1.33607 val_fs: 0.77355 val_mof: 1.71902 lr: 0.000016 elapsed: 298\n",
      "E 64/200 tr_loss: 42.59682 tr_mae: 1.23872 tr_fs: 0.77216 tr_mof: 1.60406 val_loss: 52.01960 val_mae: 1.33763 val_fs: 0.76866 val_mof: 1.73187 lr: 0.000016 elapsed: 298\n",
      "E 65/200 tr_loss: 42.59697 tr_mae: 1.23554 tr_fs: 0.77178 tr_mof: 1.60034 val_loss: 52.01964 val_mae: 1.33779 val_fs: 0.77151 val_mof: 1.72596 lr: 0.000016 elapsed: 297\n",
      "E 66/200 tr_loss: 42.59679 tr_mae: 1.23680 tr_fs: 0.77163 tr_mof: 1.60219 val_loss: 52.01951 val_mae: 1.32765 val_fs: 0.77429 val_mof: 1.70622 lr: 0.000016 elapsed: 297\n",
      "================ ༼ つ ◕_◕ ༽つ BEST epoch : 67, MOF : 1.697959587098025 \n",
      "E 67/200 tr_loss: 42.59679 tr_mae: 1.23574 tr_fs: 0.77228 tr_mof: 1.59989 val_loss: 52.01992 val_mae: 1.32645 val_fs: 0.77739 val_mof: 1.69796 lr: 0.000016 elapsed: 297\n",
      "E 68/200 tr_loss: 42.59685 tr_mae: 1.23855 tr_fs: 0.77152 tr_mof: 1.60535 val_loss: 52.01944 val_mae: 1.32620 val_fs: 0.77606 val_mof: 1.70082 lr: 0.000016 elapsed: 297\n",
      "E 69/200 tr_loss: 42.59699 tr_mae: 1.23701 tr_fs: 0.77258 tr_mof: 1.60051 val_loss: 52.01948 val_mae: 1.32862 val_fs: 0.77375 val_mof: 1.70889 lr: 0.000016 elapsed: 298\n",
      "E 70/200 tr_loss: 42.59677 tr_mae: 1.23472 tr_fs: 0.77254 tr_mof: 1.59722 val_loss: 52.01940 val_mae: 1.32604 val_fs: 0.77508 val_mof: 1.70270 lr: 0.000016 elapsed: 297\n",
      "E 71/200 tr_loss: 42.59675 tr_mae: 1.23736 tr_fs: 0.77160 tr_mof: 1.60436 val_loss: 52.01962 val_mae: 1.33393 val_fs: 0.77259 val_mof: 1.71836 lr: 0.000016 elapsed: 297\n",
      "E 72/200 tr_loss: 42.59670 tr_mae: 1.23535 tr_fs: 0.77203 tr_mof: 1.59955 val_loss: 52.01952 val_mae: 1.33128 val_fs: 0.77412 val_mof: 1.71144 lr: 0.000016 elapsed: 298\n",
      "E 73/200 tr_loss: 42.59679 tr_mae: 1.23625 tr_fs: 0.77221 tr_mof: 1.60035 val_loss: 52.01967 val_mae: 1.32759 val_fs: 0.77231 val_mof: 1.71074 lr: 0.000016 elapsed: 298\n",
      "E 74/200 tr_loss: 43.47930 tr_mae: 1.23690 tr_fs: 0.77213 tr_mof: 1.60245 val_loss: 52.01971 val_mae: 1.34534 val_fs: 0.76113 val_mof: 1.75945 lr: 0.000008 elapsed: 298\n",
      "E 75/200 tr_loss: 42.59676 tr_mae: 1.23746 tr_fs: 0.77194 tr_mof: 1.60325 val_loss: 52.01959 val_mae: 1.33166 val_fs: 0.77607 val_mof: 1.70771 lr: 0.000008 elapsed: 297\n",
      "E 76/200 tr_loss: 42.59675 tr_mae: 1.23089 tr_fs: 0.77210 tr_mof: 1.59326 val_loss: 52.01930 val_mae: 1.32635 val_fs: 0.77688 val_mof: 1.69914 lr: 0.000008 elapsed: 298\n",
      "================ ༼ つ ◕_◕ ༽つ BEST epoch : 77, MOF : 1.69443803651865 \n",
      "E 77/200 tr_loss: 42.96342 tr_mae: 1.23610 tr_fs: 0.77233 tr_mof: 1.59974 val_loss: 52.01923 val_mae: 1.32304 val_fs: 0.77703 val_mof: 1.69444 lr: 0.000008 elapsed: 298\n",
      "E 78/200 tr_loss: 42.59666 tr_mae: 1.23509 tr_fs: 0.77222 tr_mof: 1.59906 val_loss: 52.01938 val_mae: 1.32977 val_fs: 0.77403 val_mof: 1.70973 lr: 0.000008 elapsed: 297\n",
      "E 79/200 tr_loss: 42.59679 tr_mae: 1.23373 tr_fs: 0.77227 tr_mof: 1.59751 val_loss: 52.01944 val_mae: 1.33232 val_fs: 0.77251 val_mof: 1.71660 lr: 0.000008 elapsed: 297\n",
      "E 80/200 tr_loss: 42.59664 tr_mae: 1.23366 tr_fs: 0.77187 tr_mof: 1.59857 val_loss: 52.01929 val_mae: 1.32311 val_fs: 0.77699 val_mof: 1.69472 lr: 0.000008 elapsed: 297\n",
      "E 81/200 tr_loss: 42.59666 tr_mae: 1.23500 tr_fs: 0.77251 tr_mof: 1.59855 val_loss: 52.01924 val_mae: 1.32699 val_fs: 0.77570 val_mof: 1.70256 lr: 0.000008 elapsed: 297\n",
      "E 82/200 tr_loss: 42.59662 tr_mae: 1.23194 tr_fs: 0.77256 tr_mof: 1.59401 val_loss: 52.01933 val_mae: 1.33029 val_fs: 0.77343 val_mof: 1.71188 lr: 0.000008 elapsed: 297\n",
      "E 83/200 tr_loss: 42.59670 tr_mae: 1.23510 tr_fs: 0.77228 tr_mof: 1.59877 val_loss: 52.01962 val_mae: 1.32726 val_fs: 0.77656 val_mof: 1.70097 lr: 0.000008 elapsed: 297\n",
      "E 84/200 tr_loss: 42.59676 tr_mae: 1.23276 tr_fs: 0.77243 tr_mof: 1.59502 val_loss: 52.01921 val_mae: 1.32546 val_fs: 0.77675 val_mof: 1.69834 lr: 0.000004 elapsed: 297\n",
      "E 85/200 tr_loss: 42.59670 tr_mae: 1.23385 tr_fs: 0.77297 tr_mof: 1.59623 val_loss: 52.01929 val_mae: 1.32626 val_fs: 0.77576 val_mof: 1.70172 lr: 0.000004 elapsed: 297\n",
      "================ ༼ つ ◕_◕ ༽つ BEST epoch : 86, MOF : 1.6746226185335669 \n",
      "E 86/200 tr_loss: 42.59683 tr_mae: 1.23477 tr_fs: 0.77079 tr_mof: 1.61694 val_loss: 52.01942 val_mae: 1.31485 val_fs: 0.78131 val_mof: 1.67462 lr: 0.000004 elapsed: 297\n",
      "E 87/200 tr_loss: 42.59659 tr_mae: 1.23674 tr_fs: 0.77235 tr_mof: 1.60124 val_loss: 52.01919 val_mae: 1.32895 val_fs: 0.77424 val_mof: 1.70831 lr: 0.000004 elapsed: 297\n",
      "E 88/200 tr_loss: 42.59653 tr_mae: 1.23208 tr_fs: 0.77231 tr_mof: 1.59466 val_loss: 52.01916 val_mae: 1.32728 val_fs: 0.77559 val_mof: 1.70327 lr: 0.000004 elapsed: 297\n",
      "E 89/200 tr_loss: 42.59664 tr_mae: 1.23397 tr_fs: 0.77230 tr_mof: 1.59691 val_loss: 52.01922 val_mae: 1.32718 val_fs: 0.77476 val_mof: 1.70480 lr: 0.000004 elapsed: 297\n",
      "E 90/200 tr_loss: 42.59674 tr_mae: 1.23352 tr_fs: 0.77277 tr_mof: 1.59517 val_loss: 52.01914 val_mae: 1.32332 val_fs: 0.77676 val_mof: 1.69544 lr: 0.000004 elapsed: 297\n",
      "E 91/200 tr_loss: 42.59660 tr_mae: 1.23133 tr_fs: 0.77245 tr_mof: 1.59410 val_loss: 52.01916 val_mae: 1.32670 val_fs: 0.77500 val_mof: 1.70370 lr: 0.000004 elapsed: 297\n",
      "E 92/200 tr_loss: 42.59660 tr_mae: 1.23433 tr_fs: 0.77304 tr_mof: 1.59616 val_loss: 52.01916 val_mae: 1.32619 val_fs: 0.77636 val_mof: 1.70007 lr: 0.000004 elapsed: 297\n",
      "E 93/200 tr_loss: 42.59656 tr_mae: 1.23503 tr_fs: 0.77320 tr_mof: 1.59686 val_loss: 52.01918 val_mae: 1.32300 val_fs: 0.77723 val_mof: 1.69412 lr: 0.000002 elapsed: 297\n",
      "E 94/200 tr_loss: 42.59658 tr_mae: 1.23372 tr_fs: 0.77300 tr_mof: 1.59536 val_loss: 52.01912 val_mae: 1.32572 val_fs: 0.77609 val_mof: 1.70015 lr: 0.000002 elapsed: 297\n",
      "E 95/200 tr_loss: 42.59662 tr_mae: 1.23130 tr_fs: 0.77269 tr_mof: 1.59269 val_loss: 52.01921 val_mae: 1.32602 val_fs: 0.77501 val_mof: 1.70282 lr: 0.000002 elapsed: 298\n",
      "E 96/200 tr_loss: 42.59659 tr_mae: 1.23343 tr_fs: 0.77288 tr_mof: 1.59489 val_loss: 52.01931 val_mae: 1.32831 val_fs: 0.77533 val_mof: 1.70495 lr: 0.000002 elapsed: 297\n",
      "E 97/200 tr_loss: 43.47916 tr_mae: 1.23165 tr_fs: 0.77244 tr_mof: 1.59354 val_loss: 52.01918 val_mae: 1.32794 val_fs: 0.77386 val_mof: 1.70784 lr: 0.000002 elapsed: 297\n",
      "E 98/200 tr_loss: 42.59664 tr_mae: 1.23273 tr_fs: 0.77274 tr_mof: 1.59446 val_loss: 52.01918 val_mae: 1.32727 val_fs: 0.77532 val_mof: 1.70368 lr: 0.000002 elapsed: 297\n",
      "E 99/200 tr_loss: 42.59650 tr_mae: 1.23335 tr_fs: 0.77296 tr_mof: 1.59594 val_loss: 52.01918 val_mae: 1.32336 val_fs: 0.77700 val_mof: 1.69498 lr: 0.000001 elapsed: 297\n",
      "E 100/200 tr_loss: 42.59665 tr_mae: 1.23430 tr_fs: 0.77305 tr_mof: 1.59683 val_loss: 52.01924 val_mae: 1.33248 val_fs: 0.77127 val_mof: 1.71954 lr: 0.000001 elapsed: 297\n",
      "E 101/200 tr_loss: 42.59676 tr_mae: 1.23756 tr_fs: 0.77217 tr_mof: 1.60220 val_loss: 52.01911 val_mae: 1.32582 val_fs: 0.77635 val_mof: 1.69963 lr: 0.000001 elapsed: 297\n",
      "E 102/200 tr_loss: 42.59662 tr_mae: 1.23420 tr_fs: 0.77350 tr_mof: 1.59554 val_loss: 52.01917 val_mae: 1.32824 val_fs: 0.77431 val_mof: 1.70728 lr: 0.000001 elapsed: 297\n",
      "E 103/200 tr_loss: 42.59651 tr_mae: 1.22974 tr_fs: 0.77267 tr_mof: 1.59047 val_loss: 52.01914 val_mae: 1.32459 val_fs: 0.77690 val_mof: 1.69678 lr: 0.000001 elapsed: 297\n",
      "E 104/200 tr_loss: 43.47905 tr_mae: 1.23655 tr_fs: 0.77263 tr_mof: 1.60070 val_loss: 52.01916 val_mae: 1.32333 val_fs: 0.77735 val_mof: 1.69421 lr: 0.000001 elapsed: 297\n",
      "E 105/200 tr_loss: 42.59648 tr_mae: 1.23320 tr_fs: 0.77304 tr_mof: 1.59512 val_loss: 52.01911 val_mae: 1.32550 val_fs: 0.77661 val_mof: 1.69874 lr: 0.000000 elapsed: 297\n",
      "E 106/200 tr_loss: 42.59657 tr_mae: 1.23460 tr_fs: 0.77242 tr_mof: 1.59812 val_loss: 52.01921 val_mae: 1.32755 val_fs: 0.77530 val_mof: 1.70412 lr: 0.000000 elapsed: 297\n",
      "E 107/200 tr_loss: 42.59653 tr_mae: 1.23657 tr_fs: 0.77302 tr_mof: 1.59957 val_loss: 52.01913 val_mae: 1.32579 val_fs: 0.77644 val_mof: 1.69938 lr: 0.000000 elapsed: 297\n",
      "E 108/200 tr_loss: 42.59653 tr_mae: 1.23302 tr_fs: 0.77276 tr_mof: 1.59521 val_loss: 52.01920 val_mae: 1.32407 val_fs: 0.77646 val_mof: 1.69706 lr: 0.000000 elapsed: 297\n",
      "E 109/200 tr_loss: 42.59653 tr_mae: 1.23318 tr_fs: 0.77289 tr_mof: 1.59524 val_loss: 52.01926 val_mae: 1.32813 val_fs: 0.77549 val_mof: 1.70439 lr: 0.000000 elapsed: 297\n",
      "E 110/200 tr_loss: 42.59647 tr_mae: 1.23146 tr_fs: 0.77262 tr_mof: 1.59336 val_loss: 52.01916 val_mae: 1.32975 val_fs: 0.77364 val_mof: 1.71073 lr: 0.000000 elapsed: 297\n",
      "E 111/200 tr_loss: 42.59658 tr_mae: 1.23073 tr_fs: 0.77304 tr_mof: 1.59138 val_loss: 52.01918 val_mae: 1.33179 val_fs: 0.77215 val_mof: 1.71669 lr: 0.000000 elapsed: 297\n",
      "E 112/200 tr_loss: 42.59648 tr_mae: 1.23200 tr_fs: 0.77339 tr_mof: 1.59224 val_loss: 52.01910 val_mae: 1.32281 val_fs: 0.77742 val_mof: 1.69342 lr: 0.000000 elapsed: 297\n",
      "E 113/200 tr_loss: 42.59671 tr_mae: 1.23352 tr_fs: 0.77298 tr_mof: 1.59526 val_loss: 52.01916 val_mae: 1.32808 val_fs: 0.77463 val_mof: 1.70639 lr: 0.000000 elapsed: 297\n",
      "E 114/200 tr_loss: 42.59654 tr_mae: 1.23024 tr_fs: 0.77240 tr_mof: 1.59183 val_loss: 52.01915 val_mae: 1.32618 val_fs: 0.77608 val_mof: 1.70062 lr: 0.000000 elapsed: 297\n",
      "E 115/200 tr_loss: 43.47928 tr_mae: 1.23318 tr_fs: 0.77351 tr_mof: 1.59395 val_loss: 52.01918 val_mae: 1.32733 val_fs: 0.77508 val_mof: 1.70446 lr: 0.000000 elapsed: 297\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-e509c028afbe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mmae_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-85-2ea58175cf90>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mx3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-86-f4664a058946>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;31m# TODO: if statement only here to tell the jit to skip emitting this when it is None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_batches_tracked\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_batches_tracked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_batches_tracked\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmomentum\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# use cumulative moving average\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                     \u001b[0mexponential_average_factor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_batches_tracked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr, weight_decay=0.00025)\n",
    "# optimizer = AdamW(model.parameters(), 2.5e-4, weight_decay=0.000025)\n",
    "#optimizer = optim.SGD(model.parameters(), args.lr, momentum=0.9, weight_decay=0.025)\n",
    "\n",
    "###### SCHEDULER #######\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n",
    "\n",
    "#eta_min = 0.00001\n",
    "#T_max = 10\n",
    "#T_mult = 1\n",
    "#restart_decay = 0.97\n",
    "#scheduler = CosineAnnealingWithRestartsLR(optimizer, T_max=T_max, eta_min=eta_min, T_mult=T_mult, restart_decay=restart_decay)\n",
    "\n",
    "#scheduler = StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss() \n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "def to_numpy(t):\n",
    "    return t.cpu().detach().numpy()\n",
    "\n",
    "best_mae_score = 999\n",
    "best_f_score = 999\n",
    "best_mof_score = 999\n",
    "grad_clip_step = 100\n",
    "grad_clip = 100\n",
    "step = 0\n",
    "# accumulation_step = 2\n",
    "EPOCH = 200\n",
    "\n",
    "model_fname = '../D_WEATHER/weight/unet_ch9_shuffle_80_addlayer.pt'\n",
    "# log file\n",
    "log_df = pd.DataFrame(columns=['epoch_idx', 'train_loss', 'train_mae', 'train_fs', 'train_mof', 'valid_loss', 'valid_mae', 'valid_fs', 'valid_mof'])\n",
    "\n",
    "print(\"start training\")\n",
    "\n",
    "for epoch_idx in range(1, EPOCH + 1):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss = 0\n",
    "    train_mae = 0\n",
    "    train_fs = 0\n",
    "    train_mof = 0 \n",
    "#     train_total_correct = 0\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for batch_idx, (image, labels) in enumerate(train_loader):\n",
    "        if use_gpu:\n",
    "            image = image.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "        output = model(image)\n",
    "        loss = criterion(output, labels)\n",
    "        mae_score = mae(labels.cpu(), output.cpu())\n",
    "        f_score = fscore(labels.cpu(), output.cpu())\n",
    "        mof_score = maeOverFscore(labels.cpu(), output.cpu())\n",
    "\n",
    "        # gradient explosion prevention\n",
    "        if step > grad_clip_step:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "\n",
    "        step += 1\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        train_loss += loss.item() / len(train_loader)\n",
    "        train_mae += mae_score.item() / len(train_loader)\n",
    "        train_fs += f_score.item() / len(train_loader)\n",
    "        train_mof += mof_score.item() / len(train_loader)\n",
    "\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    valid_mae = 0\n",
    "    valid_fs = 0\n",
    "    valid_mof = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (image, labels) in enumerate(valid_loader):\n",
    "            if use_gpu:\n",
    "                image = image.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "            output = model(image)\n",
    "            loss = criterion(output, labels)\n",
    "            mae_score = mae(labels.cpu(), output.cpu())\n",
    "            f_score = fscore(labels.cpu(), output.cpu())\n",
    "            mof_score = maeOverFscore(labels.cpu(), output.cpu())\n",
    "\n",
    "#             output_prob = F.sigmoid(output)\n",
    "\n",
    "            predict_vector = to_numpy(output)\n",
    "\n",
    "            valid_loss += loss.item() / len(valid_loader)\n",
    "            valid_mae += mae_score.item() / len(valid_loader)\n",
    "            valid_fs += f_score.item() / len(valid_loader)\n",
    "            valid_mof += mof_score.item() / len(valid_loader)\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "    # checkpoint\n",
    "    if valid_mof < best_mof_score:\n",
    "        best_mof_score = valid_mof\n",
    "#         print(\"Improved !! \")\n",
    "        torch.save(model.state_dict(), model_fname)\n",
    "        print(\"================ ༼ つ ◕_◕ ༽つ BEST epoch : {}, MOF : {} \".format(epoch_idx, best_mof_score))\n",
    "        #file_save_name = 'best_acc' + '_' + str(num_fold)\n",
    "        #print(file_save_name)\n",
    "#     else:\n",
    "#         print(\"val acc has not improved\")\n",
    "\n",
    "    lr = [_['lr'] for _ in optimizer.param_groups]\n",
    "\n",
    "    #if args.scheduler == 'plateau':\n",
    "    scheduler.step(valid_mof)\n",
    "    #else:\n",
    "    #    scheduler.step()\n",
    "\n",
    "    # nsml.save(epoch_idx)\n",
    "\n",
    "    print(\"E {}/{} tr_loss: {:.5f} tr_mae: {:.5f} tr_fs: {:.5f} tr_mof: {:.5f} val_loss: {:.5f} val_mae: {:.5f} val_fs: {:.5f} val_mof: {:.5f} lr: {:.6f} elapsed: {:.0f}\".format(\n",
    "           epoch_idx, EPOCH, train_loss, train_mae, train_fs, train_mof, valid_loss, valid_mae, valid_fs, valid_mof, lr[0], elapsed))\n",
    "            #epoch_idx, args.epochs, train_loss, valid_loss, val_acc, lr[0], elapsed\n",
    "    # log file element\n",
    "#     log = []\n",
    "    log_data = [epoch_idx, train_loss, train_mae, train_fs, train_mof, valid_loss, valid_mae, valid_fs, valid_mof]\n",
    "#     log.append(log_data)\n",
    "    log_df.loc[epoch_idx] = log_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df.to_csv(\"../D_WEATHER/log/unet_ch9_shuffle_80_addlayer.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test_Dataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        \n",
    "        self.image_list = []\n",
    "#         self.label_list = []\n",
    "\n",
    "        for file in self.df['path']:\n",
    "            data = np.load(file)\n",
    "#             image = data[:,:,:]\n",
    "            image = data[:,:,:9]#.reshape(40,40,-1)\n",
    "            image = resize(image)\n",
    "            image = np.transpose(image, (2,0,1))\n",
    "            image = image.astype(np.float32)\n",
    "            self.image_list.append(image)\n",
    "            \n",
    "#             label = data[:,:,-1].reshape(-1)\n",
    "#             self.label_list.append(label)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        image = self.image_list[idx]\n",
    "#         label = self.label_list[idx]\n",
    "        \n",
    "        return image#, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = build_te_dataloader(te_df, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2416, 3)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loader.dataset.df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 80, 80)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 80, 80)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loader.dataset[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict values check :  [-1.05879700e-03 -5.70502423e-04  2.64364062e-04 ...  1.41910368e-06\n",
      "  1.41910368e-06  1.41910368e-06]\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(model_fname))\n",
    "model.eval()\n",
    "predictions = np.zeros((len(test_loader.dataset), 1600))\n",
    "with torch.no_grad():\n",
    "    for i, image in enumerate(test_loader):\n",
    "        image = image.to(device)\n",
    "        output = model(image)\n",
    "        \n",
    "        predictions[i*batch_size: (i+1)*batch_size] = output.detach().cpu().numpy().reshape(-1, 1600)\n",
    "print(\"predict values check : \",predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2416, 1600)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.05879700e-03, -5.70502423e-04,  2.64364062e-04, ...,\n",
       "        1.41910368e-06,  1.41910368e-06,  1.41910368e-06])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv(\"../D_WEATHER/input/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>px_1</th>\n",
       "      <th>px_2</th>\n",
       "      <th>px_3</th>\n",
       "      <th>px_4</th>\n",
       "      <th>px_5</th>\n",
       "      <th>px_6</th>\n",
       "      <th>px_7</th>\n",
       "      <th>px_8</th>\n",
       "      <th>px_9</th>\n",
       "      <th>...</th>\n",
       "      <th>px_1591</th>\n",
       "      <th>px_1592</th>\n",
       "      <th>px_1593</th>\n",
       "      <th>px_1594</th>\n",
       "      <th>px_1595</th>\n",
       "      <th>px_1596</th>\n",
       "      <th>px_1597</th>\n",
       "      <th>px_1598</th>\n",
       "      <th>px_1599</th>\n",
       "      <th>px_1600</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>029858_01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>029858_02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>029858_03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>029858_05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>029858_07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1601 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  px_1  px_2  px_3  px_4  px_5  px_6  px_7  px_8  px_9  ...  \\\n",
       "0  029858_01   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "1  029858_02   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "2  029858_03   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "3  029858_05   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "4  029858_07   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "\n",
       "   px_1591  px_1592  px_1593  px_1594  px_1595  px_1596  px_1597  px_1598  \\\n",
       "0      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "1      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "2      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "3      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "4      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "   px_1599  px_1600  \n",
       "0      0.0      0.0  \n",
       "1      0.0      0.0  \n",
       "2      0.0      0.0  \n",
       "3      0.0      0.0  \n",
       "4      0.0      0.0  \n",
       "\n",
       "[5 rows x 1601 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.iloc[:,1:] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>px_1</th>\n",
       "      <th>px_2</th>\n",
       "      <th>px_3</th>\n",
       "      <th>px_4</th>\n",
       "      <th>px_5</th>\n",
       "      <th>px_6</th>\n",
       "      <th>px_7</th>\n",
       "      <th>px_8</th>\n",
       "      <th>px_9</th>\n",
       "      <th>...</th>\n",
       "      <th>px_1591</th>\n",
       "      <th>px_1592</th>\n",
       "      <th>px_1593</th>\n",
       "      <th>px_1594</th>\n",
       "      <th>px_1595</th>\n",
       "      <th>px_1596</th>\n",
       "      <th>px_1597</th>\n",
       "      <th>px_1598</th>\n",
       "      <th>px_1599</th>\n",
       "      <th>px_1600</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>029858_01</td>\n",
       "      <td>-0.001059</td>\n",
       "      <td>-0.000571</td>\n",
       "      <td>0.000264</td>\n",
       "      <td>-0.000099</td>\n",
       "      <td>-0.000539</td>\n",
       "      <td>0.000490</td>\n",
       "      <td>-0.000440</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>029858_02</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>029858_03</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.002480</td>\n",
       "      <td>0.194166</td>\n",
       "      <td>0.190048</td>\n",
       "      <td>-0.003235</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>029858_05</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>029858_07</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>...</td>\n",
       "      <td>1.412889</td>\n",
       "      <td>1.795902</td>\n",
       "      <td>1.728144</td>\n",
       "      <td>1.068403</td>\n",
       "      <td>1.261538</td>\n",
       "      <td>2.285024</td>\n",
       "      <td>5.272021</td>\n",
       "      <td>4.551890</td>\n",
       "      <td>2.911235</td>\n",
       "      <td>1.786888</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1601 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id      px_1      px_2      px_3      px_4      px_5      px_6  \\\n",
       "0  029858_01 -0.001059 -0.000571  0.000264 -0.000099 -0.000539  0.000490   \n",
       "1  029858_02  0.000001  0.000001  0.000001  0.000001  0.000001  0.000001   \n",
       "2  029858_03  0.000001  0.000001  0.002480  0.194166  0.190048 -0.003235   \n",
       "3  029858_05  0.000001  0.000001  0.000001  0.000001  0.000001  0.000001   \n",
       "4  029858_07  0.000001  0.000001  0.000001  0.000001  0.000001  0.000001   \n",
       "\n",
       "       px_7      px_8      px_9  ...   px_1591   px_1592   px_1593   px_1594  \\\n",
       "0 -0.000440  0.000061  0.000019  ...  0.000001  0.000001  0.000001  0.000001   \n",
       "1  0.000001  0.000001  0.000001  ...  0.000001  0.000001  0.000001  0.000001   \n",
       "2 -0.000035  0.000001  0.000002  ...  0.000001  0.000001  0.000001  0.000001   \n",
       "3  0.000001  0.000001  0.000001  ...  0.000001  0.000001  0.000001  0.000001   \n",
       "4  0.000001  0.000001  0.000001  ...  1.412889  1.795902  1.728144  1.068403   \n",
       "\n",
       "    px_1595   px_1596   px_1597   px_1598   px_1599   px_1600  \n",
       "0  0.000001  0.000001  0.000001  0.000001  0.000001  0.000001  \n",
       "1  0.000001  0.000001  0.000001  0.000001  0.000001  0.000001  \n",
       "2  0.000001  0.000001  0.000001  0.000001  0.000001  0.000001  \n",
       "3  0.000001  0.000001  0.000001  0.000001  0.000001  0.000001  \n",
       "4  1.261538  2.285024  5.272021  4.551890  2.911235  1.786888  \n",
       "\n",
       "[5 rows x 1601 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('../D_WEATHER/sub/unet_ch9_shuffle_80_addlayer.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sub = sub.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1600/1600 [00:01<00:00, 1348.72it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm.tqdm(range(1,1601)):\n",
    "    new_sub.loc[new_sub[new_sub.columns[i]]<0, new_sub.columns[i]] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>px_1</th>\n",
       "      <th>px_2</th>\n",
       "      <th>px_3</th>\n",
       "      <th>px_4</th>\n",
       "      <th>px_5</th>\n",
       "      <th>px_6</th>\n",
       "      <th>px_7</th>\n",
       "      <th>px_8</th>\n",
       "      <th>px_9</th>\n",
       "      <th>px_10</th>\n",
       "      <th>...</th>\n",
       "      <th>px_1591</th>\n",
       "      <th>px_1592</th>\n",
       "      <th>px_1593</th>\n",
       "      <th>px_1594</th>\n",
       "      <th>px_1595</th>\n",
       "      <th>px_1596</th>\n",
       "      <th>px_1597</th>\n",
       "      <th>px_1598</th>\n",
       "      <th>px_1599</th>\n",
       "      <th>px_1600</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.087703</td>\n",
       "      <td>0.098287</td>\n",
       "      <td>0.115198</td>\n",
       "      <td>0.130252</td>\n",
       "      <td>0.142543</td>\n",
       "      <td>0.152179</td>\n",
       "      <td>0.144450</td>\n",
       "      <td>0.145486</td>\n",
       "      <td>0.140736</td>\n",
       "      <td>0.137137</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136138</td>\n",
       "      <td>0.123536</td>\n",
       "      <td>0.127918</td>\n",
       "      <td>0.118868</td>\n",
       "      <td>0.111941</td>\n",
       "      <td>0.117313</td>\n",
       "      <td>0.117390</td>\n",
       "      <td>0.115735</td>\n",
       "      <td>0.118482</td>\n",
       "      <td>0.109388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.550729</td>\n",
       "      <td>0.638534</td>\n",
       "      <td>0.730233</td>\n",
       "      <td>0.841010</td>\n",
       "      <td>0.995037</td>\n",
       "      <td>1.174311</td>\n",
       "      <td>1.101458</td>\n",
       "      <td>1.083037</td>\n",
       "      <td>1.126396</td>\n",
       "      <td>1.319877</td>\n",
       "      <td>...</td>\n",
       "      <td>0.789923</td>\n",
       "      <td>0.626655</td>\n",
       "      <td>0.671476</td>\n",
       "      <td>0.723412</td>\n",
       "      <td>0.719121</td>\n",
       "      <td>0.761327</td>\n",
       "      <td>0.688016</td>\n",
       "      <td>0.652214</td>\n",
       "      <td>0.729394</td>\n",
       "      <td>0.691027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.012213</td>\n",
       "      <td>-0.006044</td>\n",
       "      <td>-0.006320</td>\n",
       "      <td>-0.007172</td>\n",
       "      <td>-0.010134</td>\n",
       "      <td>-0.004591</td>\n",
       "      <td>-0.007038</td>\n",
       "      <td>-0.007392</td>\n",
       "      <td>-0.006430</td>\n",
       "      <td>-0.006304</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004905</td>\n",
       "      <td>-0.006951</td>\n",
       "      <td>-0.004216</td>\n",
       "      <td>-0.006556</td>\n",
       "      <td>-0.007774</td>\n",
       "      <td>-0.007544</td>\n",
       "      <td>-0.011743</td>\n",
       "      <td>-0.008487</td>\n",
       "      <td>-0.013675</td>\n",
       "      <td>-0.003485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10.572797</td>\n",
       "      <td>15.632951</td>\n",
       "      <td>16.070831</td>\n",
       "      <td>19.921120</td>\n",
       "      <td>28.260490</td>\n",
       "      <td>39.819275</td>\n",
       "      <td>32.074749</td>\n",
       "      <td>33.545818</td>\n",
       "      <td>34.654499</td>\n",
       "      <td>40.028915</td>\n",
       "      <td>...</td>\n",
       "      <td>19.898088</td>\n",
       "      <td>10.025855</td>\n",
       "      <td>13.498530</td>\n",
       "      <td>19.613789</td>\n",
       "      <td>17.500278</td>\n",
       "      <td>16.960773</td>\n",
       "      <td>16.343311</td>\n",
       "      <td>13.875289</td>\n",
       "      <td>16.494083</td>\n",
       "      <td>15.824573</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 1600 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              px_1         px_2         px_3         px_4         px_5  \\\n",
       "count  2416.000000  2416.000000  2416.000000  2416.000000  2416.000000   \n",
       "mean      0.087703     0.098287     0.115198     0.130252     0.142543   \n",
       "std       0.550729     0.638534     0.730233     0.841010     0.995037   \n",
       "min      -0.012213    -0.006044    -0.006320    -0.007172    -0.010134   \n",
       "25%       0.000001     0.000001     0.000001     0.000001     0.000001   \n",
       "50%       0.000001     0.000001     0.000001     0.000001     0.000001   \n",
       "75%       0.000001     0.000001     0.000002     0.000002     0.000002   \n",
       "max      10.572797    15.632951    16.070831    19.921120    28.260490   \n",
       "\n",
       "              px_6         px_7         px_8         px_9        px_10  ...  \\\n",
       "count  2416.000000  2416.000000  2416.000000  2416.000000  2416.000000  ...   \n",
       "mean      0.152179     0.144450     0.145486     0.140736     0.137137  ...   \n",
       "std       1.174311     1.101458     1.083037     1.126396     1.319877  ...   \n",
       "min      -0.004591    -0.007038    -0.007392    -0.006430    -0.006304  ...   \n",
       "25%       0.000001     0.000001     0.000001     0.000001     0.000001  ...   \n",
       "50%       0.000001     0.000001     0.000001     0.000001     0.000001  ...   \n",
       "75%       0.000002     0.000002     0.000002     0.000002     0.000002  ...   \n",
       "max      39.819275    32.074749    33.545818    34.654499    40.028915  ...   \n",
       "\n",
       "           px_1591      px_1592      px_1593      px_1594      px_1595  \\\n",
       "count  2416.000000  2416.000000  2416.000000  2416.000000  2416.000000   \n",
       "mean      0.136138     0.123536     0.127918     0.118868     0.111941   \n",
       "std       0.789923     0.626655     0.671476     0.723412     0.719121   \n",
       "min      -0.004905    -0.006951    -0.004216    -0.006556    -0.007774   \n",
       "25%       0.000001     0.000001     0.000001     0.000001     0.000001   \n",
       "50%       0.000001     0.000001     0.000001     0.000001     0.000001   \n",
       "75%       0.000001     0.000001     0.000001     0.000001     0.000001   \n",
       "max      19.898088    10.025855    13.498530    19.613789    17.500278   \n",
       "\n",
       "           px_1596      px_1597      px_1598      px_1599      px_1600  \n",
       "count  2416.000000  2416.000000  2416.000000  2416.000000  2416.000000  \n",
       "mean      0.117313     0.117390     0.115735     0.118482     0.109388  \n",
       "std       0.761327     0.688016     0.652214     0.729394     0.691027  \n",
       "min      -0.007544    -0.011743    -0.008487    -0.013675    -0.003485  \n",
       "25%       0.000001     0.000001     0.000001     0.000001     0.000001  \n",
       "50%       0.000001     0.000001     0.000001     0.000001     0.000001  \n",
       "75%       0.000001     0.000001     0.000001     0.000001     0.000001  \n",
       "max      16.960773    16.343311    13.875289    16.494083    15.824573  \n",
       "\n",
       "[8 rows x 1600 columns]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>px_1</th>\n",
       "      <th>px_2</th>\n",
       "      <th>px_3</th>\n",
       "      <th>px_4</th>\n",
       "      <th>px_5</th>\n",
       "      <th>px_6</th>\n",
       "      <th>px_7</th>\n",
       "      <th>px_8</th>\n",
       "      <th>px_9</th>\n",
       "      <th>px_10</th>\n",
       "      <th>...</th>\n",
       "      <th>px_1591</th>\n",
       "      <th>px_1592</th>\n",
       "      <th>px_1593</th>\n",
       "      <th>px_1594</th>\n",
       "      <th>px_1595</th>\n",
       "      <th>px_1596</th>\n",
       "      <th>px_1597</th>\n",
       "      <th>px_1598</th>\n",
       "      <th>px_1599</th>\n",
       "      <th>px_1600</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.087726</td>\n",
       "      <td>0.098296</td>\n",
       "      <td>0.115222</td>\n",
       "      <td>0.130266</td>\n",
       "      <td>0.142561</td>\n",
       "      <td>0.152202</td>\n",
       "      <td>0.144467</td>\n",
       "      <td>0.145500</td>\n",
       "      <td>0.140760</td>\n",
       "      <td>0.137155</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136152</td>\n",
       "      <td>0.123548</td>\n",
       "      <td>0.127929</td>\n",
       "      <td>0.118877</td>\n",
       "      <td>0.111967</td>\n",
       "      <td>0.117328</td>\n",
       "      <td>0.117407</td>\n",
       "      <td>0.115754</td>\n",
       "      <td>0.118499</td>\n",
       "      <td>0.109392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.550726</td>\n",
       "      <td>0.638532</td>\n",
       "      <td>0.730229</td>\n",
       "      <td>0.841008</td>\n",
       "      <td>0.995034</td>\n",
       "      <td>1.174308</td>\n",
       "      <td>1.101456</td>\n",
       "      <td>1.083036</td>\n",
       "      <td>1.126393</td>\n",
       "      <td>1.319875</td>\n",
       "      <td>...</td>\n",
       "      <td>0.789920</td>\n",
       "      <td>0.626653</td>\n",
       "      <td>0.671474</td>\n",
       "      <td>0.723410</td>\n",
       "      <td>0.719117</td>\n",
       "      <td>0.761325</td>\n",
       "      <td>0.688012</td>\n",
       "      <td>0.652210</td>\n",
       "      <td>0.729391</td>\n",
       "      <td>0.691026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10.572797</td>\n",
       "      <td>15.632951</td>\n",
       "      <td>16.070831</td>\n",
       "      <td>19.921120</td>\n",
       "      <td>28.260490</td>\n",
       "      <td>39.819275</td>\n",
       "      <td>32.074749</td>\n",
       "      <td>33.545818</td>\n",
       "      <td>34.654499</td>\n",
       "      <td>40.028915</td>\n",
       "      <td>...</td>\n",
       "      <td>19.898088</td>\n",
       "      <td>10.025855</td>\n",
       "      <td>13.498530</td>\n",
       "      <td>19.613789</td>\n",
       "      <td>17.500278</td>\n",
       "      <td>16.960773</td>\n",
       "      <td>16.343311</td>\n",
       "      <td>13.875289</td>\n",
       "      <td>16.494083</td>\n",
       "      <td>15.824573</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 1600 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              px_1         px_2         px_3         px_4         px_5  \\\n",
       "count  2416.000000  2416.000000  2416.000000  2416.000000  2416.000000   \n",
       "mean      0.087726     0.098296     0.115222     0.130266     0.142561   \n",
       "std       0.550726     0.638532     0.730229     0.841008     0.995034   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000001     0.000001     0.000001     0.000001     0.000001   \n",
       "50%       0.000001     0.000001     0.000001     0.000001     0.000001   \n",
       "75%       0.000001     0.000001     0.000002     0.000002     0.000002   \n",
       "max      10.572797    15.632951    16.070831    19.921120    28.260490   \n",
       "\n",
       "              px_6         px_7         px_8         px_9        px_10  ...  \\\n",
       "count  2416.000000  2416.000000  2416.000000  2416.000000  2416.000000  ...   \n",
       "mean      0.152202     0.144467     0.145500     0.140760     0.137155  ...   \n",
       "std       1.174308     1.101456     1.083036     1.126393     1.319875  ...   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "25%       0.000001     0.000001     0.000001     0.000001     0.000001  ...   \n",
       "50%       0.000001     0.000001     0.000001     0.000001     0.000001  ...   \n",
       "75%       0.000002     0.000002     0.000002     0.000002     0.000002  ...   \n",
       "max      39.819275    32.074749    33.545818    34.654499    40.028915  ...   \n",
       "\n",
       "           px_1591      px_1592      px_1593      px_1594      px_1595  \\\n",
       "count  2416.000000  2416.000000  2416.000000  2416.000000  2416.000000   \n",
       "mean      0.136152     0.123548     0.127929     0.118877     0.111967   \n",
       "std       0.789920     0.626653     0.671474     0.723410     0.719117   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000001     0.000001     0.000001     0.000001     0.000001   \n",
       "50%       0.000001     0.000001     0.000001     0.000001     0.000001   \n",
       "75%       0.000001     0.000001     0.000001     0.000001     0.000001   \n",
       "max      19.898088    10.025855    13.498530    19.613789    17.500278   \n",
       "\n",
       "           px_1596      px_1597      px_1598      px_1599      px_1600  \n",
       "count  2416.000000  2416.000000  2416.000000  2416.000000  2416.000000  \n",
       "mean      0.117328     0.117407     0.115754     0.118499     0.109392  \n",
       "std       0.761325     0.688012     0.652210     0.729391     0.691026  \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "25%       0.000001     0.000001     0.000001     0.000001     0.000001  \n",
       "50%       0.000001     0.000001     0.000001     0.000001     0.000001  \n",
       "75%       0.000001     0.000001     0.000001     0.000001     0.000001  \n",
       "max      16.960773    16.343311    13.875289    16.494083    15.824573  \n",
       "\n",
       "[8 rows x 1600 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_sub.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>px_1</th>\n",
       "      <th>px_2</th>\n",
       "      <th>px_3</th>\n",
       "      <th>px_4</th>\n",
       "      <th>px_5</th>\n",
       "      <th>px_6</th>\n",
       "      <th>px_7</th>\n",
       "      <th>px_8</th>\n",
       "      <th>px_9</th>\n",
       "      <th>...</th>\n",
       "      <th>px_1591</th>\n",
       "      <th>px_1592</th>\n",
       "      <th>px_1593</th>\n",
       "      <th>px_1594</th>\n",
       "      <th>px_1595</th>\n",
       "      <th>px_1596</th>\n",
       "      <th>px_1597</th>\n",
       "      <th>px_1598</th>\n",
       "      <th>px_1599</th>\n",
       "      <th>px_1600</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>029858_01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000264</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000490</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>029858_02</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>029858_03</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.002480</td>\n",
       "      <td>0.194166</td>\n",
       "      <td>0.190048</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>029858_05</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>029858_07</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>...</td>\n",
       "      <td>1.412889</td>\n",
       "      <td>1.795902</td>\n",
       "      <td>1.728144</td>\n",
       "      <td>1.068403</td>\n",
       "      <td>1.261538</td>\n",
       "      <td>2.285024</td>\n",
       "      <td>5.272021</td>\n",
       "      <td>4.551890</td>\n",
       "      <td>2.911235</td>\n",
       "      <td>1.786888</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1601 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id      px_1      px_2      px_3      px_4      px_5      px_6  \\\n",
       "0  029858_01  0.000000  0.000000  0.000264  0.000000  0.000000  0.000490   \n",
       "1  029858_02  0.000001  0.000001  0.000001  0.000001  0.000001  0.000001   \n",
       "2  029858_03  0.000001  0.000001  0.002480  0.194166  0.190048  0.000000   \n",
       "3  029858_05  0.000001  0.000001  0.000001  0.000001  0.000001  0.000001   \n",
       "4  029858_07  0.000001  0.000001  0.000001  0.000001  0.000001  0.000001   \n",
       "\n",
       "       px_7      px_8      px_9  ...   px_1591   px_1592   px_1593   px_1594  \\\n",
       "0  0.000000  0.000061  0.000019  ...  0.000001  0.000001  0.000001  0.000001   \n",
       "1  0.000001  0.000001  0.000001  ...  0.000001  0.000001  0.000001  0.000001   \n",
       "2  0.000000  0.000001  0.000002  ...  0.000001  0.000001  0.000001  0.000001   \n",
       "3  0.000001  0.000001  0.000001  ...  0.000001  0.000001  0.000001  0.000001   \n",
       "4  0.000001  0.000001  0.000001  ...  1.412889  1.795902  1.728144  1.068403   \n",
       "\n",
       "    px_1595   px_1596   px_1597   px_1598   px_1599   px_1600  \n",
       "0  0.000001  0.000001  0.000001  0.000001  0.000001  0.000001  \n",
       "1  0.000001  0.000001  0.000001  0.000001  0.000001  0.000001  \n",
       "2  0.000001  0.000001  0.000001  0.000001  0.000001  0.000001  \n",
       "3  0.000001  0.000001  0.000001  0.000001  0.000001  0.000001  \n",
       "4  1.261538  2.285024  5.272021  4.551890  2.911235  1.786888  \n",
       "\n",
       "[5 rows x 1601 columns]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sub.to_csv('../D_WEATHER/sub/unet_ch9_shuffle_80_addlayer_postpro.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit ('pytorch': conda)",
   "language": "python",
   "name": "python37564bitpytorchconda133dde54c45c40c2946593d30b593426"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
