{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import tqdm\n",
    "import argparse\n",
    "import tqdm\n",
    "from collections import defaultdict, Counter\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import nn, cuda\n",
    "from torch.autograd import Variable \n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import CenterCrop\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR\n",
    "\n",
    "# from efficientnet_pytorch import EfficientNet\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim import Optimizer\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def mae(y_true, y_pred) :\n",
    "    y_true, y_pred = np.array(y_true.detach().numpy()), np.array(y_pred.detach().numpy())\n",
    "    y_true = y_true.reshape(1, -1)[0]\n",
    "    y_pred = y_pred.reshape(1, -1)[0]\n",
    "    over_threshold = y_true >= 0.1\n",
    "    return np.mean(np.abs(y_true[over_threshold] - y_pred[over_threshold]))\n",
    "\n",
    "def fscore(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true.detach().numpy()), np.array(y_pred.detach().numpy())\n",
    "    y_true = y_true.reshape(1, -1)[0]\n",
    "    y_pred = y_pred.reshape(1, -1)[0]\n",
    "    remove_NAs = y_true >= 0\n",
    "    y_true = np.where(y_true[remove_NAs] >= 0.1, 1, 0)\n",
    "    y_pred = np.where(y_pred[remove_NAs] >= 0.1, 1, 0)\n",
    "    return(f1_score(y_true, y_pred))\n",
    "\n",
    "def maeOverFscore(y_true, y_pred):\n",
    "    return mae(y_true, y_pred) / (fscore(y_true, y_pred) + 1e-07)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **File info**\n",
    "**ex. subset_010462_01**\n",
    "> **orbit 010462**\n",
    "\n",
    "> **subset 01**\n",
    "\n",
    "> **ortbit 별로 subset 개수는 다를 수 있고 연속적이지 않을 수도 있음**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>orbit</th>\n",
       "      <th>orbit_subset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../D_WEATHER//input/train/subset_010462_01.npy</td>\n",
       "      <td>10462</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../D_WEATHER//input/train/subset_010462_02.npy</td>\n",
       "      <td>10462</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../D_WEATHER//input/train/subset_010462_03.npy</td>\n",
       "      <td>10462</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../D_WEATHER//input/train/subset_010462_04.npy</td>\n",
       "      <td>10462</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../D_WEATHER//input/train/subset_010462_05.npy</td>\n",
       "      <td>10462</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             path  orbit  orbit_subset\n",
       "0  ../D_WEATHER//input/train/subset_010462_01.npy  10462             1\n",
       "1  ../D_WEATHER//input/train/subset_010462_02.npy  10462             2\n",
       "2  ../D_WEATHER//input/train/subset_010462_03.npy  10462             3\n",
       "3  ../D_WEATHER//input/train/subset_010462_04.npy  10462             4\n",
       "4  ../D_WEATHER//input/train/subset_010462_05.npy  10462             5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_df = pd.read_csv(\"../D_WEATHER//input/train_df.csv\")\n",
    "te_df = pd.read_csv(\"../D_WEATHER/input/test_df.csv\")\n",
    "tr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((73825, 3), (2520, 3))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = tr_df[:int(len(tr_df)*0.967)]\n",
    "valid_df = tr_df[int(len(tr_df)*0.967):]\n",
    "\n",
    "train_df.shape, valid_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Weather_Dataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        \n",
    "        self.image_list = []\n",
    "        self.label_list = []\n",
    "\n",
    "        for file in self.df['path']:\n",
    "            data = np.load(file)\n",
    "            image = data[:,:,:9] # use 14 channels except target\n",
    "            image = np.transpose(image, (2,0,1))\n",
    "            image = image.astype(np.float32)\n",
    "            self.image_list.append(image)\n",
    "            \n",
    "            label = data[:,:,-1].reshape(40,40,1)\n",
    "            label = np.transpose(label, (2,0,1))\n",
    "            self.label_list.append(label)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        image = self.image_list[idx]\n",
    "        label = self.label_list[idx]\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def worker_init(worker_id):\n",
    "#     np.random.seed(SEED)\n",
    "\n",
    "def build_dataloader(df, batch_size, shuffle=False):\n",
    "    dataset = Weather_Dataset(df)\n",
    "    dataloader = DataLoader(\n",
    "                            dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=shuffle,\n",
    "                            num_workers=0,\n",
    "#                             worker_init_fn=worker_init\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "def build_te_dataloader(df, batch_size, shuffle=False):\n",
    "    dataset = Test_Dataset(df)\n",
    "    dataloader = DataLoader(\n",
    "                            dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=shuffle,\n",
    "                            num_workers=0,\n",
    "#                             worker_init_fn=worker_init\n",
    "                            )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=True):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels # \n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = Down(512, 1024 // factor)\n",
    "        self.up1 = Up(1024, 512, bilinear)\n",
    "        self.up2 = Up(512, 256, bilinear)\n",
    "        self.up3 = Up(256, 128, bilinear)\n",
    "        self.up4 = Up(128, 64 * factor, bilinear)\n",
    "        self.outc = OutConv(64, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels // 2, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels , in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        diffY = torch.tensor([x2.size()[2] - x1.size()[2]])\n",
    "        diffX = torch.tensor([x2.size()[3] - x1.size()[3]])\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        # if you have padding issues, see\n",
    "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
    "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "batch_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = build_dataloader(train_df, batch_size, shuffle=True)\n",
    "valid_loader = build_dataloader(valid_df, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enable gpu use\n",
      "start training\n",
      "================ ༼ つ ◕_◕ ༽つ BEST epoch : 1, MOF : 4.337951931529036 \n",
      "E 1/200 tr_loss: 43.59271 tr_mae: 1.88686 tr_fs: 0.50712 tr_mof: 4.48752 val_loss: 67.94887 val_mae: 1.41520 val_fs: 0.33149 val_mof: 4.33795 lr: 0.001000 elapsed: 108\n",
      "================ ༼ つ ◕_◕ ༽つ BEST epoch : 2, MOF : 3.3124532975215435 \n",
      "E 2/200 tr_loss: 43.56270 tr_mae: 1.69539 tr_fs: 0.62136 tr_mof: 2.75339 val_loss: 67.91639 val_mae: 1.50260 val_fs: 0.45620 val_mof: 3.31245 lr: 0.001000 elapsed: 110\n",
      "E 3/200 tr_loss: 44.70528 tr_mae: 1.58946 tr_fs: 0.66193 tr_mof: 2.40418 val_loss: 67.92654 val_mae: 1.73249 val_fs: 0.23346 val_mof: 7.63960 lr: 0.001000 elapsed: 109\n",
      "E 4/200 tr_loss: 44.07357 tr_mae: 1.52527 tr_fs: 0.68419 tr_mof: 2.23004 val_loss: 68.90085 val_mae: 1.87325 val_fs: 0.13479 val_mof: 14.12116 lr: 0.001000 elapsed: 108\n",
      "E 5/200 tr_loss: 44.12167 tr_mae: 1.49418 tr_fs: 0.69452 tr_mof: 2.15391 val_loss: 68.08244 val_mae: 1.28989 val_fs: 0.28385 val_mof: 4.73636 lr: 0.001000 elapsed: 108\n",
      "================ ༼ つ ◕_◕ ༽つ BEST epoch : 6, MOF : 2.3725340631238376 \n",
      "E 6/200 tr_loss: 43.54427 tr_mae: 1.48278 tr_fs: 0.69427 tr_mof: 2.13643 val_loss: 67.91732 val_mae: 1.30177 val_fs: 0.54863 val_mof: 2.37253 lr: 0.001000 elapsed: 108\n",
      "E 7/200 tr_loss: 43.54223 tr_mae: 1.46192 tr_fs: 0.70154 tr_mof: 2.08514 val_loss: 67.92633 val_mae: 1.34969 val_fs: 0.54736 val_mof: 2.46913 lr: 0.001000 elapsed: 108\n",
      "E 8/200 tr_loss: 43.85863 tr_mae: 1.45094 tr_fs: 0.70422 tr_mof: 2.06192 val_loss: 67.92137 val_mae: 1.27951 val_fs: 0.45166 val_mof: 2.89367 lr: 0.001000 elapsed: 108\n",
      "E 9/200 tr_loss: 44.11619 tr_mae: 1.42832 tr_fs: 0.71197 tr_mof: 2.00748 val_loss: 67.90742 val_mae: 1.41337 val_fs: 0.47394 val_mof: 3.00474 lr: 0.001000 elapsed: 108\n",
      "E 10/200 tr_loss: 43.54078 tr_mae: 1.44238 tr_fs: 0.70765 tr_mof: 2.03931 val_loss: 67.91625 val_mae: 1.55316 val_fs: 0.40989 val_mof: 3.83495 lr: 0.001000 elapsed: 108\n",
      "E 11/200 tr_loss: 44.11575 tr_mae: 1.42526 tr_fs: 0.71402 tr_mof: 1.99657 val_loss: 68.15807 val_mae: 1.57097 val_fs: 0.16223 val_mof: 10.51560 lr: 0.001000 elapsed: 107\n",
      "================ ༼ つ ◕_◕ ༽つ BEST epoch : 12, MOF : 2.190737403298169 \n",
      "E 12/200 tr_loss: 43.53837 tr_mae: 1.41465 tr_fs: 0.71963 tr_mof: 1.96690 val_loss: 67.90043 val_mae: 1.28086 val_fs: 0.58537 val_mof: 2.19074 lr: 0.001000 elapsed: 107\n",
      "================ ༼ つ ◕_◕ ༽つ BEST epoch : 13, MOF : 2.0699696820723252 \n",
      "E 13/200 tr_loss: 43.98572 tr_mae: 1.40861 tr_fs: 0.72380 tr_mof: 1.94732 val_loss: 67.90021 val_mae: 1.23607 val_fs: 0.59705 val_mof: 2.06997 lr: 0.001000 elapsed: 107\n",
      "E 14/200 tr_loss: 44.54866 tr_mae: 1.40942 tr_fs: 0.71928 tr_mof: 1.96080 val_loss: 67.92395 val_mae: 1.70919 val_fs: 0.23208 val_mof: 7.56937 lr: 0.001000 elapsed: 107\n",
      "E 15/200 tr_loss: 43.53896 tr_mae: 1.41930 tr_fs: 0.71675 tr_mof: 1.98261 val_loss: 67.90504 val_mae: 1.34121 val_fs: 0.58760 val_mof: 2.28274 lr: 0.001000 elapsed: 107\n",
      "E 16/200 tr_loss: 44.11280 tr_mae: 1.39241 tr_fs: 0.73285 tr_mof: 1.90065 val_loss: 67.90513 val_mae: 1.35051 val_fs: 0.52405 val_mof: 2.58338 lr: 0.001000 elapsed: 108\n",
      "E 17/200 tr_loss: 43.53996 tr_mae: 1.42019 tr_fs: 0.71556 tr_mof: 1.98898 val_loss: 67.91563 val_mae: 1.31436 val_fs: 0.41253 val_mof: 3.27952 lr: 0.001000 elapsed: 107\n",
      "E 18/200 tr_loss: 43.97180 tr_mae: 1.38604 tr_fs: 0.73454 tr_mof: 1.88816 val_loss: 67.91142 val_mae: 1.49642 val_fs: 0.42474 val_mof: 3.55947 lr: 0.001000 elapsed: 107\n",
      "================ ༼ つ ◕_◕ ༽つ BEST epoch : 19, MOF : 1.8929610563847235 \n",
      "E 19/200 tr_loss: 44.11291 tr_mae: 1.39052 tr_fs: 0.73234 tr_mof: 1.90021 val_loss: 67.90027 val_mae: 1.22571 val_fs: 0.64612 val_mof: 1.89296 lr: 0.001000 elapsed: 107\n",
      "E 20/200 tr_loss: 44.11185 tr_mae: 1.38007 tr_fs: 0.73569 tr_mof: 1.87703 val_loss: 68.71872 val_mae: 1.55027 val_fs: 0.23488 val_mof: 6.83511 lr: 0.001000 elapsed: 107\n",
      "E 21/200 tr_loss: 44.01535 tr_mae: 1.38671 tr_fs: 0.73359 tr_mof: 1.89190 val_loss: 67.90292 val_mae: 1.27985 val_fs: 0.59301 val_mof: 2.16350 lr: 0.001000 elapsed: 107\n",
      "E 22/200 tr_loss: 44.11214 tr_mae: 1.38287 tr_fs: 0.73585 tr_mof: 1.88012 val_loss: 67.90894 val_mae: 1.29923 val_fs: 0.58222 val_mof: 2.23313 lr: 0.001000 elapsed: 107\n",
      "================ ༼ つ ◕_◕ ༽つ BEST epoch : 23, MOF : 1.784758695537883 \n",
      "E 23/200 tr_loss: 43.53425 tr_mae: 1.35991 tr_fs: 0.74538 tr_mof: 1.82481 val_loss: 67.89806 val_mae: 1.19345 val_fs: 0.66753 val_mof: 1.78476 lr: 0.001000 elapsed: 107\n",
      "E 24/200 tr_loss: 44.11068 tr_mae: 1.36517 tr_fs: 0.74327 tr_mof: 1.83712 val_loss: 67.91012 val_mae: 1.16337 val_fs: 0.57090 val_mof: 2.03238 lr: 0.001000 elapsed: 107\n",
      "E 25/200 tr_loss: 44.09418 tr_mae: 1.39393 tr_fs: 0.72907 tr_mof: 1.91480 val_loss: 68.00348 val_mae: 1.33148 val_fs: 0.31030 val_mof: 4.46925 lr: 0.001000 elapsed: 107\n",
      "E 26/200 tr_loss: 45.36480 tr_mae: 1.43461 tr_fs: 0.70153 tr_mof: 2.05575 val_loss: 67.90952 val_mae: 1.30159 val_fs: 0.53645 val_mof: 2.43702 lr: 0.001000 elapsed: 107\n",
      "E 27/200 tr_loss: 44.11402 tr_mae: 1.41521 tr_fs: 0.71989 tr_mof: 1.97029 val_loss: 67.89945 val_mae: 1.26399 val_fs: 0.62011 val_mof: 2.04051 lr: 0.001000 elapsed: 106\n",
      "E 28/200 tr_loss: 43.53554 tr_mae: 1.37957 tr_fs: 0.73857 tr_mof: 1.86845 val_loss: 67.90039 val_mae: 1.23957 val_fs: 0.65076 val_mof: 1.90193 lr: 0.001000 elapsed: 107\n",
      "E 29/200 tr_loss: 44.11041 tr_mae: 1.36310 tr_fs: 0.74434 tr_mof: 1.83210 val_loss: 67.89758 val_mae: 1.22145 val_fs: 0.64735 val_mof: 1.88519 lr: 0.001000 elapsed: 107\n",
      "E 30/200 tr_loss: 44.10893 tr_mae: 1.34389 tr_fs: 0.75099 tr_mof: 1.79000 val_loss: 67.91192 val_mae: 1.29324 val_fs: 0.65448 val_mof: 1.97159 lr: 0.000500 elapsed: 106\n",
      "================ ༼ つ ◕_◕ ༽つ BEST epoch : 31, MOF : 1.6892623666421414 \n",
      "E 31/200 tr_loss: 43.53233 tr_mae: 1.34173 tr_fs: 0.75279 tr_mof: 1.78272 val_loss: 67.89531 val_mae: 1.15582 val_fs: 0.68326 val_mof: 1.68926 lr: 0.000500 elapsed: 107\n",
      "E 32/200 tr_loss: 43.53218 tr_mae: 1.33876 tr_fs: 0.75571 tr_mof: 1.77187 val_loss: 67.89998 val_mae: 1.23686 val_fs: 0.67533 val_mof: 1.82698 lr: 0.000500 elapsed: 106\n",
      "E 33/200 tr_loss: 44.10830 tr_mae: 1.33222 tr_fs: 0.75669 tr_mof: 1.76082 val_loss: 68.03060 val_mae: 1.29586 val_fs: 0.30387 val_mof: 4.52269 lr: 0.000500 elapsed: 107\n",
      "E 34/200 tr_loss: 43.53254 tr_mae: 1.34041 tr_fs: 0.75264 tr_mof: 1.78105 val_loss: 67.90344 val_mae: 1.35604 val_fs: 0.48167 val_mof: 2.83429 lr: 0.000500 elapsed: 107\n",
      "E 35/200 tr_loss: 43.53158 tr_mae: 1.33059 tr_fs: 0.75737 tr_mof: 1.75705 val_loss: 67.94663 val_mae: 1.14005 val_fs: 0.41794 val_mof: 2.72945 lr: 0.000500 elapsed: 107\n",
      "E 36/200 tr_loss: 43.53189 tr_mae: 1.33018 tr_fs: 0.75651 tr_mof: 1.75845 val_loss: 67.89475 val_mae: 1.17551 val_fs: 0.65385 val_mof: 1.79512 lr: 0.000500 elapsed: 107\n",
      "E 37/200 tr_loss: 43.53155 tr_mae: 1.32656 tr_fs: 0.75823 tr_mof: 1.74952 val_loss: 67.90343 val_mae: 1.28219 val_fs: 0.68361 val_mof: 1.87122 lr: 0.000500 elapsed: 107\n",
      "E 38/200 tr_loss: 43.53119 tr_mae: 1.32305 tr_fs: 0.76062 tr_mof: 1.73951 val_loss: 67.89434 val_mae: 1.16096 val_fs: 0.68588 val_mof: 1.68949 lr: 0.000250 elapsed: 107\n",
      "================ ༼ つ ◕_◕ ༽つ BEST epoch : 39, MOF : 1.599904355592408 \n",
      "E 39/200 tr_loss: 43.53066 tr_mae: 1.31755 tr_fs: 0.76294 tr_mof: 1.72713 val_loss: 67.89603 val_mae: 1.12485 val_fs: 0.70188 val_mof: 1.59990 lr: 0.000250 elapsed: 107\n",
      "E 40/200 tr_loss: 43.53068 tr_mae: 1.31583 tr_fs: 0.76492 tr_mof: 1.71987 val_loss: 67.89474 val_mae: 1.16187 val_fs: 0.69416 val_mof: 1.67018 lr: 0.000250 elapsed: 107\n",
      "================ ༼ つ ◕_◕ ༽つ BEST epoch : 41, MOF : 1.560935173221809 \n",
      "E 41/200 tr_loss: 43.53052 tr_mae: 1.31468 tr_fs: 0.76518 tr_mof: 1.71808 val_loss: 67.89477 val_mae: 1.11340 val_fs: 0.71192 val_mof: 1.56094 lr: 0.000250 elapsed: 107\n",
      "E 42/200 tr_loss: 44.68283 tr_mae: 1.30936 tr_fs: 0.76670 tr_mof: 1.70793 val_loss: 67.89927 val_mae: 1.19718 val_fs: 0.67314 val_mof: 1.77630 lr: 0.000250 elapsed: 107\n",
      "E 43/200 tr_loss: 43.53014 tr_mae: 1.30912 tr_fs: 0.76584 tr_mof: 1.70960 val_loss: 67.89483 val_mae: 1.17569 val_fs: 0.67742 val_mof: 1.73272 lr: 0.000250 elapsed: 107\n",
      "E 44/200 tr_loss: 43.52963 tr_mae: 1.30104 tr_fs: 0.76684 tr_mof: 1.69661 val_loss: 67.94032 val_mae: 1.24264 val_fs: 0.32662 val_mof: 3.94984 lr: 0.000250 elapsed: 107\n",
      "E 45/200 tr_loss: 43.87589 tr_mae: 1.31968 tr_fs: 0.75825 tr_mof: 1.74087 val_loss: 67.91292 val_mae: 1.10132 val_fs: 0.56877 val_mof: 1.95376 lr: 0.000250 elapsed: 107\n",
      "E 46/200 tr_loss: 44.44169 tr_mae: 1.30864 tr_fs: 0.76532 tr_mof: 1.71020 val_loss: 67.89950 val_mae: 1.18432 val_fs: 0.69668 val_mof: 1.69591 lr: 0.000250 elapsed: 107\n",
      "E 47/200 tr_loss: 44.10602 tr_mae: 1.30263 tr_fs: 0.76704 tr_mof: 1.69836 val_loss: 67.89624 val_mae: 1.13360 val_fs: 0.71240 val_mof: 1.58836 lr: 0.000250 elapsed: 107\n",
      "E 48/200 tr_loss: 44.10559 tr_mae: 1.29686 tr_fs: 0.77009 tr_mof: 1.68418 val_loss: 67.89608 val_mae: 1.12512 val_fs: 0.71770 val_mof: 1.56504 lr: 0.000125 elapsed: 107\n",
      "E 49/200 tr_loss: 44.68167 tr_mae: 1.29511 tr_fs: 0.77129 tr_mof: 1.67909 val_loss: 67.89531 val_mae: 1.15125 val_fs: 0.68026 val_mof: 1.69079 lr: 0.000125 elapsed: 107\n",
      "E 50/200 tr_loss: 43.52940 tr_mae: 1.29734 tr_fs: 0.77065 tr_mof: 1.68387 val_loss: 67.89247 val_mae: 1.11881 val_fs: 0.69957 val_mof: 1.59638 lr: 0.000125 elapsed: 107\n",
      "E 51/200 tr_loss: 43.52881 tr_mae: 1.29035 tr_fs: 0.77181 tr_mof: 1.67167 val_loss: 67.89262 val_mae: 1.11087 val_fs: 0.70644 val_mof: 1.56981 lr: 0.000125 elapsed: 107\n",
      "================ ༼ つ ◕_◕ ༽つ BEST epoch : 52, MOF : 1.5139153975406923 \n",
      "E 52/200 tr_loss: 43.52908 tr_mae: 1.29406 tr_fs: 0.77109 tr_mof: 1.67839 val_loss: 67.89523 val_mae: 1.09525 val_fs: 0.72214 val_mof: 1.51392 lr: 0.000125 elapsed: 107\n",
      "E 53/200 tr_loss: 43.52866 tr_mae: 1.28675 tr_fs: 0.77273 tr_mof: 1.66515 val_loss: 67.89252 val_mae: 1.10463 val_fs: 0.71606 val_mof: 1.53972 lr: 0.000125 elapsed: 107\n",
      "E 54/200 tr_loss: 44.10488 tr_mae: 1.28617 tr_fs: 0.77317 tr_mof: 1.66349 val_loss: 67.89446 val_mae: 1.13756 val_fs: 0.70751 val_mof: 1.60516 lr: 0.000125 elapsed: 107\n",
      "E 55/200 tr_loss: 44.09241 tr_mae: 1.29178 tr_fs: 0.77316 tr_mof: 1.67097 val_loss: 67.89227 val_mae: 1.10761 val_fs: 0.70827 val_mof: 1.56096 lr: 0.000125 elapsed: 107\n",
      "E 56/200 tr_loss: 43.52866 tr_mae: 1.28949 tr_fs: 0.77410 tr_mof: 1.66609 val_loss: 67.89452 val_mae: 1.08956 val_fs: 0.71486 val_mof: 1.52095 lr: 0.000125 elapsed: 107\n",
      "E 57/200 tr_loss: 43.52904 tr_mae: 1.28966 tr_fs: 0.77367 tr_mof: 1.66703 val_loss: 67.89768 val_mae: 1.19670 val_fs: 0.69455 val_mof: 1.72046 lr: 0.000125 elapsed: 107\n",
      "E 58/200 tr_loss: 44.14330 tr_mae: 1.28377 tr_fs: 0.77419 tr_mof: 1.65806 val_loss: 67.89260 val_mae: 1.10571 val_fs: 0.71350 val_mof: 1.54717 lr: 0.000125 elapsed: 107\n",
      "E 59/200 tr_loss: 43.52801 tr_mae: 1.27989 tr_fs: 0.77534 tr_mof: 1.65065 val_loss: 67.89291 val_mae: 1.10208 val_fs: 0.72343 val_mof: 1.52056 lr: 0.000063 elapsed: 107\n",
      "E 60/200 tr_loss: 43.52808 tr_mae: 1.27949 tr_fs: 0.77653 tr_mof: 1.64764 val_loss: 67.89190 val_mae: 1.10907 val_fs: 0.71166 val_mof: 1.55584 lr: 0.000063 elapsed: 106\n",
      "================ ༼ つ ◕_◕ ༽つ BEST epoch : 61, MOF : 1.4927783079781052 \n",
      "E 61/200 tr_loss: 43.52806 tr_mae: 1.27913 tr_fs: 0.77664 tr_mof: 1.64700 val_loss: 67.89245 val_mae: 1.08752 val_fs: 0.72716 val_mof: 1.49278 lr: 0.000063 elapsed: 107\n",
      "E 62/200 tr_loss: 43.52779 tr_mae: 1.27608 tr_fs: 0.77664 tr_mof: 1.64293 val_loss: 67.89204 val_mae: 1.11912 val_fs: 0.70596 val_mof: 1.58270 lr: 0.000063 elapsed: 107\n",
      "E 63/200 tr_loss: 43.52807 tr_mae: 1.27838 tr_fs: 0.77627 tr_mof: 1.64675 val_loss: 67.89271 val_mae: 1.13773 val_fs: 0.68527 val_mof: 1.65790 lr: 0.000063 elapsed: 107\n",
      "E 64/200 tr_loss: 44.10433 tr_mae: 1.27801 tr_fs: 0.77674 tr_mof: 1.64554 val_loss: 67.89182 val_mae: 1.10278 val_fs: 0.70935 val_mof: 1.55206 lr: 0.000063 elapsed: 107\n",
      "E 65/200 tr_loss: 43.52791 tr_mae: 1.27630 tr_fs: 0.77727 tr_mof: 1.64196 val_loss: 67.89198 val_mae: 1.10642 val_fs: 0.70365 val_mof: 1.56972 lr: 0.000063 elapsed: 107\n",
      "E 66/200 tr_loss: 43.52778 tr_mae: 1.27426 tr_fs: 0.77730 tr_mof: 1.63914 val_loss: 67.89471 val_mae: 1.10474 val_fs: 0.66890 val_mof: 1.65143 lr: 0.000063 elapsed: 107\n",
      "E 67/200 tr_loss: 44.10403 tr_mae: 1.27674 tr_fs: 0.77691 tr_mof: 1.64351 val_loss: 67.89557 val_mae: 1.06067 val_fs: 0.70432 val_mof: 1.50268 lr: 0.000063 elapsed: 107\n",
      "E 68/200 tr_loss: 43.52792 tr_mae: 1.27433 tr_fs: 0.77849 tr_mof: 1.63681 val_loss: 67.89150 val_mae: 1.10295 val_fs: 0.71662 val_mof: 1.53678 lr: 0.000031 elapsed: 106\n",
      "================ ༼ つ ◕_◕ ༽つ BEST epoch : 69, MOF : 1.4839004829695415 \n",
      "E 69/200 tr_loss: 43.96314 tr_mae: 1.26911 tr_fs: 0.77873 tr_mof: 1.62968 val_loss: 67.89141 val_mae: 1.08240 val_fs: 0.72813 val_mof: 1.48390 lr: 0.000031 elapsed: 107\n",
      "E 70/200 tr_loss: 43.52767 tr_mae: 1.27350 tr_fs: 0.77871 tr_mof: 1.63542 val_loss: 67.89147 val_mae: 1.08998 val_fs: 0.72353 val_mof: 1.50401 lr: 0.000031 elapsed: 106\n",
      "================ ༼ つ ◕_◕ ༽つ BEST epoch : 71, MOF : 1.482461501892133 \n",
      "E 71/200 tr_loss: 43.52742 tr_mae: 1.27029 tr_fs: 0.77876 tr_mof: 1.63110 val_loss: 67.89187 val_mae: 1.08457 val_fs: 0.73036 val_mof: 1.48246 lr: 0.000031 elapsed: 107\n",
      "E 72/200 tr_loss: 44.10399 tr_mae: 1.27436 tr_fs: 0.77890 tr_mof: 1.63611 val_loss: 67.89168 val_mae: 1.08993 val_fs: 0.72681 val_mof: 1.49709 lr: 0.000031 elapsed: 107\n",
      "E 73/200 tr_loss: 43.52724 tr_mae: 1.26749 tr_fs: 0.77836 tr_mof: 1.62829 val_loss: 67.89129 val_mae: 1.08764 val_fs: 0.72466 val_mof: 1.49827 lr: 0.000031 elapsed: 107\n",
      "================ ༼ つ ◕_◕ ༽つ BEST epoch : 74, MOF : 1.4789369564445574 \n",
      "E 74/200 tr_loss: 43.52742 tr_mae: 1.27027 tr_fs: 0.77931 tr_mof: 1.63018 val_loss: 67.89150 val_mae: 1.08251 val_fs: 0.73066 val_mof: 1.47894 lr: 0.000031 elapsed: 107\n",
      "E 75/200 tr_loss: 43.71393 tr_mae: 1.27206 tr_fs: 0.77930 tr_mof: 1.63228 val_loss: 67.89165 val_mae: 1.09891 val_fs: 0.72121 val_mof: 1.52142 lr: 0.000031 elapsed: 107\n",
      "================ ༼ つ ◕_◕ ༽つ BEST epoch : 76, MOF : 1.4579763692612877 \n",
      "E 76/200 tr_loss: 43.52738 tr_mae: 1.26939 tr_fs: 0.77955 tr_mof: 1.62867 val_loss: 67.89152 val_mae: 1.07234 val_fs: 0.73419 val_mof: 1.45798 lr: 0.000031 elapsed: 106\n",
      "E 77/200 tr_loss: 43.52756 tr_mae: 1.26833 tr_fs: 0.77974 tr_mof: 1.62641 val_loss: 67.89163 val_mae: 1.10685 val_fs: 0.70846 val_mof: 1.55972 lr: 0.000031 elapsed: 107\n",
      "E 78/200 tr_loss: 44.67964 tr_mae: 1.26841 tr_fs: 0.77986 tr_mof: 1.62657 val_loss: 67.89151 val_mae: 1.08150 val_fs: 0.73162 val_mof: 1.47573 lr: 0.000031 elapsed: 107\n",
      "E 79/200 tr_loss: 43.52721 tr_mae: 1.26548 tr_fs: 0.77981 tr_mof: 1.62277 val_loss: 67.89143 val_mae: 1.09465 val_fs: 0.72266 val_mof: 1.51231 lr: 0.000031 elapsed: 107\n",
      "E 80/200 tr_loss: 44.10369 tr_mae: 1.26730 tr_fs: 0.77999 tr_mof: 1.62469 val_loss: 67.89136 val_mae: 1.09284 val_fs: 0.71774 val_mof: 1.52021 lr: 0.000031 elapsed: 107\n",
      "E 81/200 tr_loss: 43.80663 tr_mae: 1.26648 tr_fs: 0.77972 tr_mof: 1.62394 val_loss: 67.89134 val_mae: 1.09277 val_fs: 0.72278 val_mof: 1.50925 lr: 0.000031 elapsed: 107\n",
      "E 82/200 tr_loss: 43.52732 tr_mae: 1.26712 tr_fs: 0.78027 tr_mof: 1.62375 val_loss: 67.89172 val_mae: 1.08209 val_fs: 0.73243 val_mof: 1.47483 lr: 0.000031 elapsed: 106\n",
      "E 83/200 tr_loss: 43.52699 tr_mae: 1.26385 tr_fs: 0.78049 tr_mof: 1.61905 val_loss: 67.89118 val_mae: 1.08827 val_fs: 0.72475 val_mof: 1.49903 lr: 0.000016 elapsed: 107\n",
      "E 84/200 tr_loss: 45.25584 tr_mae: 1.26314 tr_fs: 0.78081 tr_mof: 1.61784 val_loss: 67.89122 val_mae: 1.09625 val_fs: 0.71639 val_mof: 1.52768 lr: 0.000016 elapsed: 107\n",
      "E 85/200 tr_loss: 44.67985 tr_mae: 1.26746 tr_fs: 0.78133 tr_mof: 1.62201 val_loss: 67.89123 val_mae: 1.09619 val_fs: 0.72074 val_mof: 1.51853 lr: 0.000016 elapsed: 106\n",
      "E 86/200 tr_loss: 43.52690 tr_mae: 1.26093 tr_fs: 0.78093 tr_mof: 1.61436 val_loss: 67.89120 val_mae: 1.07996 val_fs: 0.72965 val_mof: 1.47759 lr: 0.000016 elapsed: 107\n",
      "E 87/200 tr_loss: 43.74638 tr_mae: 1.26421 tr_fs: 0.78124 tr_mof: 1.61811 val_loss: 67.89124 val_mae: 1.08574 val_fs: 0.72508 val_mof: 1.49499 lr: 0.000016 elapsed: 107\n",
      "E 88/200 tr_loss: 44.10302 tr_mae: 1.26089 tr_fs: 0.78098 tr_mof: 1.61459 val_loss: 67.89110 val_mae: 1.08820 val_fs: 0.72450 val_mof: 1.49962 lr: 0.000016 elapsed: 106\n",
      "E 89/200 tr_loss: 43.52685 tr_mae: 1.26284 tr_fs: 0.78121 tr_mof: 1.61639 val_loss: 67.89114 val_mae: 1.08353 val_fs: 0.72867 val_mof: 1.48462 lr: 0.000008 elapsed: 107\n",
      "E 90/200 tr_loss: 43.52679 tr_mae: 1.26236 tr_fs: 0.78145 tr_mof: 1.61537 val_loss: 67.89110 val_mae: 1.08348 val_fs: 0.72788 val_mof: 1.48597 lr: 0.000008 elapsed: 107\n",
      "E 91/200 tr_loss: 43.52677 tr_mae: 1.26208 tr_fs: 0.78112 tr_mof: 1.61562 val_loss: 67.89116 val_mae: 1.08325 val_fs: 0.72806 val_mof: 1.48531 lr: 0.000008 elapsed: 107\n",
      "E 92/200 tr_loss: 44.10336 tr_mae: 1.26375 tr_fs: 0.78156 tr_mof: 1.61681 val_loss: 67.89115 val_mae: 1.08243 val_fs: 0.72892 val_mof: 1.48259 lr: 0.000008 elapsed: 107\n",
      "E 93/200 tr_loss: 43.52692 tr_mae: 1.26006 tr_fs: 0.78168 tr_mof: 1.61174 val_loss: 67.89128 val_mae: 1.07530 val_fs: 0.73375 val_mof: 1.46303 lr: 0.000008 elapsed: 106\n",
      "E 94/200 tr_loss: 43.52680 tr_mae: 1.26068 tr_fs: 0.78184 tr_mof: 1.61244 val_loss: 67.89104 val_mae: 1.08766 val_fs: 0.72374 val_mof: 1.50029 lr: 0.000008 elapsed: 106\n",
      "E 95/200 tr_loss: 43.98395 tr_mae: 1.26229 tr_fs: 0.78163 tr_mof: 1.61500 val_loss: 67.89110 val_mae: 1.08077 val_fs: 0.73056 val_mof: 1.47699 lr: 0.000004 elapsed: 107\n",
      "E 96/200 tr_loss: 43.52690 tr_mae: 1.26250 tr_fs: 0.78176 tr_mof: 1.61463 val_loss: 67.89105 val_mae: 1.08380 val_fs: 0.72787 val_mof: 1.48653 lr: 0.000004 elapsed: 107\n",
      "E 97/200 tr_loss: 43.52679 tr_mae: 1.26098 tr_fs: 0.78009 tr_mof: 1.61772 val_loss: 67.89108 val_mae: 1.07828 val_fs: 0.73211 val_mof: 1.47039 lr: 0.000004 elapsed: 107\n",
      "E 98/200 tr_loss: 43.75682 tr_mae: 1.25960 tr_fs: 0.78192 tr_mof: 1.61088 val_loss: 67.89104 val_mae: 1.07982 val_fs: 0.73039 val_mof: 1.47595 lr: 0.000004 elapsed: 107\n",
      "E 99/200 tr_loss: 44.10298 tr_mae: 1.25968 tr_fs: 0.78173 tr_mof: 1.61131 val_loss: 67.89105 val_mae: 1.08432 val_fs: 0.72729 val_mof: 1.48854 lr: 0.000004 elapsed: 107\n",
      "E 100/200 tr_loss: 43.52698 tr_mae: 1.26095 tr_fs: 0.78227 tr_mof: 1.61225 val_loss: 67.89103 val_mae: 1.08577 val_fs: 0.72667 val_mof: 1.49168 lr: 0.000004 elapsed: 107\n",
      "E 101/200 tr_loss: 43.52655 tr_mae: 1.26037 tr_fs: 0.78142 tr_mof: 1.61286 val_loss: 67.89106 val_mae: 1.07883 val_fs: 0.73129 val_mof: 1.47286 lr: 0.000002 elapsed: 106\n",
      "E 102/200 tr_loss: 43.52665 tr_mae: 1.26130 tr_fs: 0.78199 tr_mof: 1.61304 val_loss: 67.89101 val_mae: 1.08187 val_fs: 0.72916 val_mof: 1.48125 lr: 0.000002 elapsed: 106\n",
      "E 103/200 tr_loss: 44.10294 tr_mae: 1.26215 tr_fs: 0.78198 tr_mof: 1.61428 val_loss: 67.89101 val_mae: 1.08204 val_fs: 0.72831 val_mof: 1.48323 lr: 0.000002 elapsed: 107\n",
      "E 104/200 tr_loss: 43.52674 tr_mae: 1.25966 tr_fs: 0.78173 tr_mof: 1.61116 val_loss: 67.89105 val_mae: 1.08025 val_fs: 0.72961 val_mof: 1.47816 lr: 0.000002 elapsed: 107\n",
      "E 105/200 tr_loss: 43.52667 tr_mae: 1.26032 tr_fs: 0.78175 tr_mof: 1.61215 val_loss: 67.89104 val_mae: 1.08300 val_fs: 0.72885 val_mof: 1.48348 lr: 0.000002 elapsed: 107\n",
      "E 106/200 tr_loss: 43.52660 tr_mae: 1.25998 tr_fs: 0.78174 tr_mof: 1.61169 val_loss: 67.89103 val_mae: 1.08096 val_fs: 0.72982 val_mof: 1.47867 lr: 0.000002 elapsed: 106\n",
      "E 107/200 tr_loss: 43.52656 tr_mae: 1.26018 tr_fs: 0.78201 tr_mof: 1.61164 val_loss: 67.89101 val_mae: 1.07963 val_fs: 0.73030 val_mof: 1.47591 lr: 0.000001 elapsed: 107\n",
      "E 108/200 tr_loss: 43.52666 tr_mae: 1.26012 tr_fs: 0.78176 tr_mof: 1.61173 val_loss: 67.89103 val_mae: 1.07996 val_fs: 0.73053 val_mof: 1.47590 lr: 0.000001 elapsed: 107\n",
      "E 109/200 tr_loss: 43.52669 tr_mae: 1.25968 tr_fs: 0.78215 tr_mof: 1.61043 val_loss: 67.89099 val_mae: 1.08182 val_fs: 0.72897 val_mof: 1.48164 lr: 0.000001 elapsed: 107\n",
      "E 110/200 tr_loss: 44.88986 tr_mae: 1.25946 tr_fs: 0.78153 tr_mof: 1.61131 val_loss: 67.89106 val_mae: 1.08065 val_fs: 0.73035 val_mof: 1.47725 lr: 0.000001 elapsed: 106\n",
      "E 111/200 tr_loss: 43.52655 tr_mae: 1.25853 tr_fs: 0.78168 tr_mof: 1.60973 val_loss: 67.89100 val_mae: 1.08047 val_fs: 0.73029 val_mof: 1.47708 lr: 0.000001 elapsed: 107\n",
      "E 112/200 tr_loss: 43.52666 tr_mae: 1.25956 tr_fs: 0.78257 tr_mof: 1.60980 val_loss: 67.89103 val_mae: 1.08155 val_fs: 0.72935 val_mof: 1.48047 lr: 0.000001 elapsed: 106\n",
      "E 113/200 tr_loss: 43.52674 tr_mae: 1.25999 tr_fs: 0.78224 tr_mof: 1.61052 val_loss: 67.89103 val_mae: 1.08230 val_fs: 0.72892 val_mof: 1.48242 lr: 0.000000 elapsed: 107\n",
      "E 114/200 tr_loss: 43.52684 tr_mae: 1.26128 tr_fs: 0.78195 tr_mof: 1.61313 val_loss: 67.89101 val_mae: 1.08286 val_fs: 0.72920 val_mof: 1.48256 lr: 0.000000 elapsed: 107\n",
      "E 115/200 tr_loss: 43.52665 tr_mae: 1.26063 tr_fs: 0.78229 tr_mof: 1.61155 val_loss: 67.89110 val_mae: 1.07895 val_fs: 0.73135 val_mof: 1.47287 lr: 0.000000 elapsed: 107\n",
      "E 116/200 tr_loss: 43.52738 tr_mae: 1.26030 tr_fs: 0.78172 tr_mof: 1.61198 val_loss: 67.89102 val_mae: 1.07943 val_fs: 0.73075 val_mof: 1.47466 lr: 0.000000 elapsed: 107\n",
      "E 117/200 tr_loss: 43.52661 tr_mae: 1.25915 tr_fs: 0.78146 tr_mof: 1.61115 val_loss: 67.89105 val_mae: 1.08069 val_fs: 0.72998 val_mof: 1.47800 lr: 0.000000 elapsed: 107\n",
      "E 118/200 tr_loss: 43.52669 tr_mae: 1.26024 tr_fs: 0.78216 tr_mof: 1.61093 val_loss: 67.89104 val_mae: 1.08094 val_fs: 0.72997 val_mof: 1.47836 lr: 0.000000 elapsed: 107\n",
      "E 119/200 tr_loss: 44.10291 tr_mae: 1.25995 tr_fs: 0.78211 tr_mof: 1.61092 val_loss: 67.89102 val_mae: 1.08165 val_fs: 0.72913 val_mof: 1.48106 lr: 0.000000 elapsed: 107\n",
      "E 120/200 tr_loss: 43.99053 tr_mae: 1.25939 tr_fs: 0.78187 tr_mof: 1.61079 val_loss: 67.89102 val_mae: 1.08117 val_fs: 0.72956 val_mof: 1.47954 lr: 0.000000 elapsed: 107\n",
      "E 121/200 tr_loss: 43.52664 tr_mae: 1.25887 tr_fs: 0.78228 tr_mof: 1.60936 val_loss: 67.89105 val_mae: 1.08167 val_fs: 0.72961 val_mof: 1.48007 lr: 0.000000 elapsed: 107\n",
      "E 122/200 tr_loss: 43.52667 tr_mae: 1.26015 tr_fs: 0.78186 tr_mof: 1.61178 val_loss: 67.89102 val_mae: 1.08014 val_fs: 0.73017 val_mof: 1.47683 lr: 0.000000 elapsed: 107\n",
      "E 123/200 tr_loss: 43.74062 tr_mae: 1.26109 tr_fs: 0.78192 tr_mof: 1.61256 val_loss: 67.89111 val_mae: 1.07897 val_fs: 0.73131 val_mof: 1.47296 lr: 0.000000 elapsed: 107\n",
      "E 124/200 tr_loss: 43.52680 tr_mae: 1.26153 tr_fs: 0.78207 tr_mof: 1.61310 val_loss: 67.89105 val_mae: 1.08112 val_fs: 0.72985 val_mof: 1.47885 lr: 0.000000 elapsed: 107\n",
      "E 125/200 tr_loss: 44.67915 tr_mae: 1.25845 tr_fs: 0.78215 tr_mof: 1.60891 val_loss: 67.89100 val_mae: 1.08219 val_fs: 0.72883 val_mof: 1.48243 lr: 0.000000 elapsed: 107\n",
      "E 126/200 tr_loss: 44.10303 tr_mae: 1.26246 tr_fs: 0.78204 tr_mof: 1.61434 val_loss: 67.89103 val_mae: 1.08094 val_fs: 0.72951 val_mof: 1.47929 lr: 0.000000 elapsed: 106\n",
      "E 127/200 tr_loss: 43.52666 tr_mae: 1.26069 tr_fs: 0.78200 tr_mof: 1.61220 val_loss: 67.89105 val_mae: 1.08048 val_fs: 0.73052 val_mof: 1.47665 lr: 0.000000 elapsed: 107\n",
      "E 128/200 tr_loss: 43.52679 tr_mae: 1.26184 tr_fs: 0.78227 tr_mof: 1.61301 val_loss: 67.89102 val_mae: 1.08205 val_fs: 0.72945 val_mof: 1.48099 lr: 0.000000 elapsed: 107\n",
      "E 129/200 tr_loss: 43.52661 tr_mae: 1.25926 tr_fs: 0.78215 tr_mof: 1.61017 val_loss: 67.89105 val_mae: 1.08004 val_fs: 0.73059 val_mof: 1.47586 lr: 0.000000 elapsed: 107\n",
      "E 130/200 tr_loss: 43.52666 tr_mae: 1.25934 tr_fs: 0.78229 tr_mof: 1.60984 val_loss: 67.89105 val_mae: 1.07933 val_fs: 0.73087 val_mof: 1.47435 lr: 0.000000 elapsed: 107\n",
      "E 131/200 tr_loss: 43.52688 tr_mae: 1.26273 tr_fs: 0.78203 tr_mof: 1.61461 val_loss: 67.89105 val_mae: 1.08100 val_fs: 0.73033 val_mof: 1.47773 lr: 0.000000 elapsed: 107\n",
      "E 132/200 tr_loss: 43.84510 tr_mae: 1.26076 tr_fs: 0.78211 tr_mof: 1.61200 val_loss: 67.89102 val_mae: 1.08088 val_fs: 0.72966 val_mof: 1.47889 lr: 0.000000 elapsed: 107\n",
      "E 133/200 tr_loss: 43.52691 tr_mae: 1.26031 tr_fs: 0.78208 tr_mof: 1.61139 val_loss: 67.89098 val_mae: 1.08465 val_fs: 0.72735 val_mof: 1.48882 lr: 0.000000 elapsed: 107\n",
      "E 134/200 tr_loss: 43.52694 tr_mae: 1.26100 tr_fs: 0.78226 tr_mof: 1.61198 val_loss: 67.89100 val_mae: 1.08408 val_fs: 0.72794 val_mof: 1.48687 lr: 0.000000 elapsed: 107\n",
      "E 135/200 tr_loss: 43.52668 tr_mae: 1.25963 tr_fs: 0.78220 tr_mof: 1.61029 val_loss: 67.89105 val_mae: 1.07880 val_fs: 0.73126 val_mof: 1.47286 lr: 0.000000 elapsed: 107\n",
      "E 136/200 tr_loss: 44.10290 tr_mae: 1.25989 tr_fs: 0.78173 tr_mof: 1.61170 val_loss: 67.89103 val_mae: 1.08061 val_fs: 0.73046 val_mof: 1.47693 lr: 0.000000 elapsed: 107\n",
      "E 137/200 tr_loss: 44.10310 tr_mae: 1.26087 tr_fs: 0.78179 tr_mof: 1.61294 val_loss: 67.89104 val_mae: 1.08243 val_fs: 0.72970 val_mof: 1.48100 lr: 0.000000 elapsed: 107\n",
      "E 138/200 tr_loss: 44.10308 tr_mae: 1.25914 tr_fs: 0.78201 tr_mof: 1.60992 val_loss: 67.89101 val_mae: 1.08243 val_fs: 0.72889 val_mof: 1.48261 lr: 0.000000 elapsed: 107\n",
      "E 139/200 tr_loss: 43.52660 tr_mae: 1.25848 tr_fs: 0.78208 tr_mof: 1.60870 val_loss: 67.89105 val_mae: 1.08023 val_fs: 0.73036 val_mof: 1.47660 lr: 0.000000 elapsed: 107\n",
      "E 140/200 tr_loss: 43.52710 tr_mae: 1.26120 tr_fs: 0.78226 tr_mof: 1.61234 val_loss: 67.89104 val_mae: 1.09011 val_fs: 0.72451 val_mof: 1.50223 lr: 0.000000 elapsed: 106\n",
      "E 141/200 tr_loss: 44.10282 tr_mae: 1.25987 tr_fs: 0.78161 tr_mof: 1.61177 val_loss: 67.89103 val_mae: 1.07816 val_fs: 0.73156 val_mof: 1.47135 lr: 0.000000 elapsed: 106\n",
      "E 142/200 tr_loss: 43.52664 tr_mae: 1.25962 tr_fs: 0.78216 tr_mof: 1.61051 val_loss: 67.89101 val_mae: 1.08149 val_fs: 0.72944 val_mof: 1.48022 lr: 0.000000 elapsed: 107\n",
      "E 143/200 tr_loss: 43.52665 tr_mae: 1.25957 tr_fs: 0.78221 tr_mof: 1.61009 val_loss: 67.89104 val_mae: 1.08126 val_fs: 0.72946 val_mof: 1.47990 lr: 0.000000 elapsed: 107\n",
      "E 144/200 tr_loss: 43.52681 tr_mae: 1.25925 tr_fs: 0.78214 tr_mof: 1.60983 val_loss: 67.89107 val_mae: 1.08624 val_fs: 0.72617 val_mof: 1.49350 lr: 0.000000 elapsed: 107\n",
      "E 145/200 tr_loss: 44.00895 tr_mae: 1.26003 tr_fs: 0.78205 tr_mof: 1.61126 val_loss: 67.89106 val_mae: 1.08157 val_fs: 0.72968 val_mof: 1.47982 lr: 0.000000 elapsed: 107\n",
      "E 146/200 tr_loss: 43.90029 tr_mae: 1.25959 tr_fs: 0.78184 tr_mof: 1.61082 val_loss: 67.89101 val_mae: 1.08462 val_fs: 0.72713 val_mof: 1.48925 lr: 0.000000 elapsed: 106\n",
      "E 147/200 tr_loss: 43.52681 tr_mae: 1.26281 tr_fs: 0.78181 tr_mof: 1.61515 val_loss: 67.89100 val_mae: 1.08200 val_fs: 0.72969 val_mof: 1.48042 lr: 0.000000 elapsed: 107\n",
      "E 148/200 tr_loss: 43.52669 tr_mae: 1.26213 tr_fs: 0.78202 tr_mof: 1.61396 val_loss: 67.89107 val_mae: 1.08031 val_fs: 0.73044 val_mof: 1.47655 lr: 0.000000 elapsed: 107\n",
      "E 149/200 tr_loss: 43.52665 tr_mae: 1.26028 tr_fs: 0.78206 tr_mof: 1.61120 val_loss: 67.89102 val_mae: 1.08080 val_fs: 0.72970 val_mof: 1.47873 lr: 0.000000 elapsed: 106\n",
      "E 150/200 tr_loss: 44.10293 tr_mae: 1.25925 tr_fs: 0.78236 tr_mof: 1.60942 val_loss: 67.89101 val_mae: 1.08087 val_fs: 0.72965 val_mof: 1.47894 lr: 0.000000 elapsed: 107\n",
      "E 151/200 tr_loss: 43.52675 tr_mae: 1.25944 tr_fs: 0.78226 tr_mof: 1.61008 val_loss: 67.89099 val_mae: 1.08308 val_fs: 0.72857 val_mof: 1.48418 lr: 0.000000 elapsed: 107\n",
      "E 152/200 tr_loss: 43.52647 tr_mae: 1.25868 tr_fs: 0.78201 tr_mof: 1.60956 val_loss: 67.89105 val_mae: 1.07974 val_fs: 0.73085 val_mof: 1.47495 lr: 0.000000 elapsed: 107\n",
      "E 153/200 tr_loss: 43.52681 tr_mae: 1.26287 tr_fs: 0.78193 tr_mof: 1.61531 val_loss: 67.89100 val_mae: 1.08212 val_fs: 0.72902 val_mof: 1.48194 lr: 0.000000 elapsed: 106\n",
      "E 154/200 tr_loss: 44.10290 tr_mae: 1.25836 tr_fs: 0.78206 tr_mof: 1.60899 val_loss: 67.89106 val_mae: 1.07952 val_fs: 0.73071 val_mof: 1.47491 lr: 0.000000 elapsed: 107\n",
      "E 155/200 tr_loss: 44.10298 tr_mae: 1.25879 tr_fs: 0.78212 tr_mof: 1.60934 val_loss: 67.89102 val_mae: 1.08372 val_fs: 0.72843 val_mof: 1.48533 lr: 0.000000 elapsed: 107\n",
      "E 156/200 tr_loss: 43.52659 tr_mae: 1.26021 tr_fs: 0.78238 tr_mof: 1.61079 val_loss: 67.89101 val_mae: 1.07905 val_fs: 0.73081 val_mof: 1.47405 lr: 0.000000 elapsed: 107\n",
      "E 157/200 tr_loss: 43.52647 tr_mae: 1.25700 tr_fs: 0.78184 tr_mof: 1.60745 val_loss: 67.89106 val_mae: 1.08189 val_fs: 0.72882 val_mof: 1.48203 lr: 0.000000 elapsed: 107\n",
      "E 158/200 tr_loss: 43.83200 tr_mae: 1.25862 tr_fs: 0.78200 tr_mof: 1.60924 val_loss: 67.89105 val_mae: 1.08037 val_fs: 0.73039 val_mof: 1.47675 lr: 0.000000 elapsed: 107\n",
      "E 159/200 tr_loss: 44.10301 tr_mae: 1.25843 tr_fs: 0.78238 tr_mof: 1.60832 val_loss: 67.89104 val_mae: 1.08077 val_fs: 0.73007 val_mof: 1.47792 lr: 0.000000 elapsed: 107\n",
      "E 160/200 tr_loss: 44.67910 tr_mae: 1.25763 tr_fs: 0.78199 tr_mof: 1.60778 val_loss: 67.89104 val_mae: 1.08006 val_fs: 0.73030 val_mof: 1.47649 lr: 0.000000 elapsed: 107\n",
      "E 161/200 tr_loss: 43.52659 tr_mae: 1.25594 tr_fs: 0.78224 tr_mof: 1.60550 val_loss: 67.89102 val_mae: 1.07959 val_fs: 0.73044 val_mof: 1.47556 lr: 0.000000 elapsed: 107\n",
      "E 162/200 tr_loss: 43.52659 tr_mae: 1.25946 tr_fs: 0.78176 tr_mof: 1.61092 val_loss: 67.89107 val_mae: 1.07964 val_fs: 0.73099 val_mof: 1.47453 lr: 0.000000 elapsed: 107\n",
      "E 163/200 tr_loss: 43.52665 tr_mae: 1.26131 tr_fs: 0.78195 tr_mof: 1.61303 val_loss: 67.89107 val_mae: 1.07985 val_fs: 0.73123 val_mof: 1.47437 lr: 0.000000 elapsed: 107\n",
      "E 164/200 tr_loss: 43.52656 tr_mae: 1.25993 tr_fs: 0.78198 tr_mof: 1.61103 val_loss: 67.89105 val_mae: 1.08069 val_fs: 0.72981 val_mof: 1.47837 lr: 0.000000 elapsed: 107\n",
      "E 165/200 tr_loss: 43.52667 tr_mae: 1.26002 tr_fs: 0.78189 tr_mof: 1.61152 val_loss: 67.89101 val_mae: 1.08196 val_fs: 0.72905 val_mof: 1.48166 lr: 0.000000 elapsed: 107\n",
      "E 166/200 tr_loss: 43.52660 tr_mae: 1.25921 tr_fs: 0.78224 tr_mof: 1.60956 val_loss: 67.89104 val_mae: 1.08065 val_fs: 0.72984 val_mof: 1.47824 lr: 0.000000 elapsed: 107\n",
      "E 167/200 tr_loss: 43.52669 tr_mae: 1.26138 tr_fs: 0.78214 tr_mof: 1.61286 val_loss: 67.89100 val_mae: 1.08239 val_fs: 0.72887 val_mof: 1.48264 lr: 0.000000 elapsed: 107\n",
      "E 168/200 tr_loss: 44.10307 tr_mae: 1.26053 tr_fs: 0.78206 tr_mof: 1.61182 val_loss: 67.89103 val_mae: 1.08103 val_fs: 0.72994 val_mof: 1.47859 lr: 0.000000 elapsed: 107\n",
      "E 169/200 tr_loss: 43.52740 tr_mae: 1.26330 tr_fs: 0.78209 tr_mof: 1.61515 val_loss: 67.89105 val_mae: 1.08912 val_fs: 0.72600 val_mof: 1.49772 lr: 0.000000 elapsed: 107\n",
      "E 170/200 tr_loss: 44.67941 tr_mae: 1.26095 tr_fs: 0.78210 tr_mof: 1.61239 val_loss: 67.89102 val_mae: 1.08488 val_fs: 0.72791 val_mof: 1.48799 lr: 0.000000 elapsed: 107\n",
      "E 171/200 tr_loss: 43.52695 tr_mae: 1.26056 tr_fs: 0.78233 tr_mof: 1.61116 val_loss: 67.89102 val_mae: 1.08729 val_fs: 0.72607 val_mof: 1.49513 lr: 0.000000 elapsed: 107\n",
      "E 172/200 tr_loss: 43.52707 tr_mae: 1.26720 tr_fs: 0.78154 tr_mof: 1.62287 val_loss: 67.89099 val_mae: 1.08380 val_fs: 0.72825 val_mof: 1.48580 lr: 0.000000 elapsed: 107\n",
      "E 173/200 tr_loss: 44.10301 tr_mae: 1.26034 tr_fs: 0.78218 tr_mof: 1.61121 val_loss: 67.89108 val_mae: 1.08133 val_fs: 0.72959 val_mof: 1.47975 lr: 0.000000 elapsed: 107\n",
      "E 174/200 tr_loss: 44.10274 tr_mae: 1.25670 tr_fs: 0.78169 tr_mof: 1.60736 val_loss: 67.89100 val_mae: 1.08048 val_fs: 0.72929 val_mof: 1.47910 lr: 0.000000 elapsed: 107\n",
      "E 175/200 tr_loss: 43.52681 tr_mae: 1.26077 tr_fs: 0.78201 tr_mof: 1.61208 val_loss: 67.89100 val_mae: 1.08310 val_fs: 0.72815 val_mof: 1.48509 lr: 0.000000 elapsed: 107\n",
      "E 176/200 tr_loss: 43.52666 tr_mae: 1.25815 tr_fs: 0.78171 tr_mof: 1.60908 val_loss: 67.89101 val_mae: 1.08114 val_fs: 0.72994 val_mof: 1.47874 lr: 0.000000 elapsed: 107\n",
      "E 177/200 tr_loss: 43.96238 tr_mae: 1.25695 tr_fs: 0.78194 tr_mof: 1.60722 val_loss: 67.89103 val_mae: 1.08072 val_fs: 0.72964 val_mof: 1.47877 lr: 0.000000 elapsed: 107\n",
      "E 178/200 tr_loss: 43.52669 tr_mae: 1.26171 tr_fs: 0.78216 tr_mof: 1.61310 val_loss: 67.89103 val_mae: 1.08033 val_fs: 0.73050 val_mof: 1.47646 lr: 0.000000 elapsed: 107\n",
      "E 179/200 tr_loss: 43.52668 tr_mae: 1.25857 tr_fs: 0.78210 tr_mof: 1.60911 val_loss: 67.89100 val_mae: 1.08288 val_fs: 0.72822 val_mof: 1.48465 lr: 0.000000 elapsed: 106\n",
      "E 180/200 tr_loss: 43.52665 tr_mae: 1.25907 tr_fs: 0.78193 tr_mof: 1.61023 val_loss: 67.89101 val_mae: 1.08129 val_fs: 0.72958 val_mof: 1.47965 lr: 0.000000 elapsed: 106\n",
      "E 181/200 tr_loss: 44.10286 tr_mae: 1.26107 tr_fs: 0.78210 tr_mof: 1.61249 val_loss: 67.89103 val_mae: 1.07998 val_fs: 0.73051 val_mof: 1.47595 lr: 0.000000 elapsed: 107\n",
      "E 182/200 tr_loss: 43.52669 tr_mae: 1.26140 tr_fs: 0.78184 tr_mof: 1.61345 val_loss: 67.89105 val_mae: 1.08090 val_fs: 0.72976 val_mof: 1.47878 lr: 0.000000 elapsed: 107\n",
      "E 183/200 tr_loss: 43.52653 tr_mae: 1.25859 tr_fs: 0.78169 tr_mof: 1.60995 val_loss: 67.89104 val_mae: 1.07981 val_fs: 0.73060 val_mof: 1.47559 lr: 0.000000 elapsed: 107\n",
      "E 184/200 tr_loss: 43.52682 tr_mae: 1.25969 tr_fs: 0.78203 tr_mof: 1.61072 val_loss: 67.89111 val_mae: 1.07965 val_fs: 0.73140 val_mof: 1.47373 lr: 0.000000 elapsed: 107\n",
      "E 185/200 tr_loss: 43.52685 tr_mae: 1.26157 tr_fs: 0.78227 tr_mof: 1.61259 val_loss: 67.89100 val_mae: 1.08228 val_fs: 0.72915 val_mof: 1.48188 lr: 0.000000 elapsed: 107\n",
      "E 186/200 tr_loss: 44.10283 tr_mae: 1.25938 tr_fs: 0.78198 tr_mof: 1.61055 val_loss: 67.89103 val_mae: 1.08012 val_fs: 0.73043 val_mof: 1.47632 lr: 0.000000 elapsed: 107\n",
      "E 187/200 tr_loss: 44.10292 tr_mae: 1.26015 tr_fs: 0.78232 tr_mof: 1.61062 val_loss: 67.89109 val_mae: 1.07900 val_fs: 0.73133 val_mof: 1.47294 lr: 0.000000 elapsed: 107\n",
      "E 188/200 tr_loss: 43.52666 tr_mae: 1.26198 tr_fs: 0.78213 tr_mof: 1.61341 val_loss: 67.89106 val_mae: 1.07923 val_fs: 0.73096 val_mof: 1.47403 lr: 0.000000 elapsed: 106\n",
      "E 189/200 tr_loss: 43.86444 tr_mae: 1.25966 tr_fs: 0.78194 tr_mof: 1.61081 val_loss: 67.89105 val_mae: 1.08051 val_fs: 0.73010 val_mof: 1.47751 lr: 0.000000 elapsed: 106\n",
      "E 190/200 tr_loss: 45.83190 tr_mae: 1.25873 tr_fs: 0.78216 tr_mof: 1.60903 val_loss: 67.89103 val_mae: 1.08158 val_fs: 0.72924 val_mof: 1.48075 lr: 0.000000 elapsed: 107\n",
      "E 191/200 tr_loss: 43.52666 tr_mae: 1.25852 tr_fs: 0.78251 tr_mof: 1.60817 val_loss: 67.89112 val_mae: 1.08014 val_fs: 0.73057 val_mof: 1.47604 lr: 0.000000 elapsed: 107\n",
      "E 192/200 tr_loss: 44.10304 tr_mae: 1.26107 tr_fs: 0.78172 tr_mof: 1.61319 val_loss: 67.89102 val_mae: 1.08218 val_fs: 0.72892 val_mof: 1.48221 lr: 0.000000 elapsed: 106\n",
      "E 193/200 tr_loss: 44.10298 tr_mae: 1.25973 tr_fs: 0.78195 tr_mof: 1.61078 val_loss: 67.89105 val_mae: 1.08120 val_fs: 0.73026 val_mof: 1.47815 lr: 0.000000 elapsed: 106\n",
      "E 194/200 tr_loss: 44.10298 tr_mae: 1.25752 tr_fs: 0.78214 tr_mof: 1.60745 val_loss: 67.89102 val_mae: 1.08335 val_fs: 0.72869 val_mof: 1.48431 lr: 0.000000 elapsed: 106\n",
      "E 195/200 tr_loss: 43.52671 tr_mae: 1.25977 tr_fs: 0.78180 tr_mof: 1.61135 val_loss: 67.89102 val_mae: 1.08106 val_fs: 0.72966 val_mof: 1.47915 lr: 0.000000 elapsed: 106\n",
      "E 196/200 tr_loss: 44.10301 tr_mae: 1.25985 tr_fs: 0.78195 tr_mof: 1.61106 val_loss: 67.89102 val_mae: 1.08209 val_fs: 0.72934 val_mof: 1.48124 lr: 0.000000 elapsed: 106\n",
      "E 197/200 tr_loss: 43.97433 tr_mae: 1.25746 tr_fs: 0.78218 tr_mof: 1.60754 val_loss: 67.89100 val_mae: 1.08138 val_fs: 0.72948 val_mof: 1.48001 lr: 0.000000 elapsed: 107\n",
      "E 198/200 tr_loss: 43.52694 tr_mae: 1.26012 tr_fs: 0.78237 tr_mof: 1.61080 val_loss: 67.89100 val_mae: 1.08454 val_fs: 0.72731 val_mof: 1.48878 lr: 0.000000 elapsed: 106\n",
      "E 199/200 tr_loss: 43.98875 tr_mae: 1.25977 tr_fs: 0.78216 tr_mof: 1.61037 val_loss: 67.89106 val_mae: 1.07994 val_fs: 0.73040 val_mof: 1.47614 lr: 0.000000 elapsed: 106\n",
      "E 200/200 tr_loss: 43.52666 tr_mae: 1.25833 tr_fs: 0.78190 tr_mof: 1.60934 val_loss: 67.89102 val_mae: 1.08078 val_fs: 0.72964 val_mof: 1.47881 lr: 0.000000 elapsed: 107\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "seed_everything(SEED)\n",
    "\n",
    "device = 'cuda:0'\n",
    "use_gpu = cuda.is_available()\n",
    "if use_gpu:\n",
    "    print(\"enable gpu use\")\n",
    "else:\n",
    "    print(\"enable cpu for debugging\")\n",
    "\n",
    "model = UNet(n_channels=9, n_classes=1, bilinear=False) # if bilinear = True -> non deterministic : not recommended\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr, weight_decay=0.00025)\n",
    "# optimizer = AdamW(model.parameters(), 2.5e-4, weight_decay=0.000025)\n",
    "#optimizer = optim.SGD(model.parameters(), args.lr, momentum=0.9, weight_decay=0.025)\n",
    "\n",
    "###### SCHEDULER #######\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n",
    "\n",
    "#eta_min = 0.00001\n",
    "#T_max = 10\n",
    "#T_mult = 1\n",
    "#restart_decay = 0.97\n",
    "#scheduler = CosineAnnealingWithRestartsLR(optimizer, T_max=T_max, eta_min=eta_min, T_mult=T_mult, restart_decay=restart_decay)\n",
    "\n",
    "#scheduler = StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss() \n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "def to_numpy(t):\n",
    "    return t.cpu().detach().numpy()\n",
    "\n",
    "best_mae_score = 999\n",
    "best_f_score = 999\n",
    "best_mof_score = 999\n",
    "grad_clip_step = 100\n",
    "grad_clip = 100\n",
    "step = 0\n",
    "# accumulation_step = 2\n",
    "EPOCH = 200\n",
    "\n",
    "model_fname = '../D_WEATHER/weight/unet_ch9_shuffle_ho0.967.pt'\n",
    "# log file\n",
    "log_df = pd.DataFrame(columns=['epoch_idx', 'train_loss', 'train_mae', 'train_fs', 'train_mof', 'valid_loss', 'valid_mae', 'valid_fs', 'valid_mof'])\n",
    "\n",
    "print(\"start training\")\n",
    "\n",
    "for epoch_idx in range(1, EPOCH + 1):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss = 0\n",
    "    train_mae = 0\n",
    "    train_fs = 0\n",
    "    train_mof = 0 \n",
    "#     train_total_correct = 0\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for batch_idx, (image, labels) in enumerate(train_loader):\n",
    "        if use_gpu:\n",
    "            image = image.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "        output = model(image)\n",
    "        loss = criterion(output, labels)\n",
    "        mae_score = mae(labels.cpu(), output.cpu())\n",
    "        f_score = fscore(labels.cpu(), output.cpu())\n",
    "        mof_score = maeOverFscore(labels.cpu(), output.cpu())\n",
    "\n",
    "        # gradient explosion prevention\n",
    "        if step > grad_clip_step:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "\n",
    "        step += 1\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        train_loss += loss.item() / len(train_loader)\n",
    "        train_mae += mae_score.item() / len(train_loader)\n",
    "        train_fs += f_score.item() / len(train_loader)\n",
    "        train_mof += mof_score.item() / len(train_loader)\n",
    "\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    valid_mae = 0\n",
    "    valid_fs = 0\n",
    "    valid_mof = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (image, labels) in enumerate(valid_loader):\n",
    "            if use_gpu:\n",
    "                image = image.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "            output = model(image)\n",
    "            loss = criterion(output, labels)\n",
    "            mae_score = mae(labels.cpu(), output.cpu())\n",
    "            f_score = fscore(labels.cpu(), output.cpu())\n",
    "            mof_score = maeOverFscore(labels.cpu(), output.cpu())\n",
    "\n",
    "#             output_prob = F.sigmoid(output)\n",
    "\n",
    "            predict_vector = to_numpy(output)\n",
    "\n",
    "            valid_loss += loss.item() / len(valid_loader)\n",
    "            valid_mae += mae_score.item() / len(valid_loader)\n",
    "            valid_fs += f_score.item() / len(valid_loader)\n",
    "            valid_mof += mof_score.item() / len(valid_loader)\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "    # checkpoint\n",
    "    if valid_mof < best_mof_score:\n",
    "        best_mof_score = valid_mof\n",
    "#         print(\"Improved !! \")\n",
    "        torch.save(model.state_dict(), model_fname)\n",
    "        print(\"================ ༼ つ ◕_◕ ༽つ BEST epoch : {}, MOF : {} \".format(epoch_idx, best_mof_score))\n",
    "        #file_save_name = 'best_acc' + '_' + str(num_fold)\n",
    "        #print(file_save_name)\n",
    "#     else:\n",
    "#         print(\"val acc has not improved\")\n",
    "\n",
    "    lr = [_['lr'] for _ in optimizer.param_groups]\n",
    "\n",
    "    #if args.scheduler == 'plateau':\n",
    "    scheduler.step(valid_mof)\n",
    "    #else:\n",
    "    #    scheduler.step()\n",
    "\n",
    "    # nsml.save(epoch_idx)\n",
    "\n",
    "    print(\"E {}/{} tr_loss: {:.5f} tr_mae: {:.5f} tr_fs: {:.5f} tr_mof: {:.5f} val_loss: {:.5f} val_mae: {:.5f} val_fs: {:.5f} val_mof: {:.5f} lr: {:.6f} elapsed: {:.0f}\".format(\n",
    "           epoch_idx, EPOCH, train_loss, train_mae, train_fs, train_mof, valid_loss, valid_mae, valid_fs, valid_mof, lr[0], elapsed))\n",
    "            #epoch_idx, args.epochs, train_loss, valid_loss, val_acc, lr[0], elapsed\n",
    "    # log file element\n",
    "#     log = []\n",
    "    log_data = [epoch_idx, train_loss, train_mae, train_fs, train_mof, valid_loss, valid_mae, valid_fs, valid_mof]\n",
    "#     log.append(log_data)\n",
    "    log_df.loc[epoch_idx] = log_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df.to_csv(\"../D_WEATHER/log/unet_ch9_shuffle_ho0.967.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test_Dataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        \n",
    "        self.image_list = []\n",
    "#         self.label_list = []\n",
    "\n",
    "        for file in self.df['path']:\n",
    "            data = np.load(file)\n",
    "#             image = data[:,:,:]\n",
    "            image = data[:,:,:9]#.reshape(40,40,-1)\n",
    "            image = np.transpose(image, (2,0,1))\n",
    "            image = image.astype(np.float32)\n",
    "            self.image_list.append(image)\n",
    "            \n",
    "#             label = data[:,:,-1].reshape(-1)\n",
    "#             self.label_list.append(label)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        image = self.image_list[idx]\n",
    "#         label = self.label_list[idx]\n",
    "        \n",
    "        return image#, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = build_te_dataloader(te_df, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2416, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loader.dataset.df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 40, 40)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 40, 40)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loader.dataset[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict values check :  [-5.19867535e-05  1.06577631e-04 -7.99479370e-04 ...  8.01769511e-06\n",
      "  8.01769511e-06  8.01769511e-06]\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(model_fname))\n",
    "model.eval()\n",
    "predictions = np.zeros((len(test_loader.dataset), 1600))\n",
    "with torch.no_grad():\n",
    "    for i, image in enumerate(test_loader):\n",
    "        image = image.to(device)\n",
    "        output = model(image)\n",
    "        \n",
    "        predictions[i*batch_size: (i+1)*batch_size] = output.detach().cpu().numpy().reshape(-1, 1600)\n",
    "print(\"predict values check : \",predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2416, 1600)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-5.19867535e-05,  1.06577631e-04, -7.99479370e-04, ...,\n",
       "        8.01769511e-06,  8.01769511e-06,  8.01769511e-06])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv(\"../D_WEATHER/input/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>px_1</th>\n",
       "      <th>px_2</th>\n",
       "      <th>px_3</th>\n",
       "      <th>px_4</th>\n",
       "      <th>px_5</th>\n",
       "      <th>px_6</th>\n",
       "      <th>px_7</th>\n",
       "      <th>px_8</th>\n",
       "      <th>px_9</th>\n",
       "      <th>...</th>\n",
       "      <th>px_1591</th>\n",
       "      <th>px_1592</th>\n",
       "      <th>px_1593</th>\n",
       "      <th>px_1594</th>\n",
       "      <th>px_1595</th>\n",
       "      <th>px_1596</th>\n",
       "      <th>px_1597</th>\n",
       "      <th>px_1598</th>\n",
       "      <th>px_1599</th>\n",
       "      <th>px_1600</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>029858_01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>029858_02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>029858_03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>029858_05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>029858_07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1601 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  px_1  px_2  px_3  px_4  px_5  px_6  px_7  px_8  px_9  ...  \\\n",
       "0  029858_01   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "1  029858_02   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "2  029858_03   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "3  029858_05   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "4  029858_07   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "\n",
       "   px_1591  px_1592  px_1593  px_1594  px_1595  px_1596  px_1597  px_1598  \\\n",
       "0      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "1      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "2      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "3      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "4      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "   px_1599  px_1600  \n",
       "0      0.0      0.0  \n",
       "1      0.0      0.0  \n",
       "2      0.0      0.0  \n",
       "3      0.0      0.0  \n",
       "4      0.0      0.0  \n",
       "\n",
       "[5 rows x 1601 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.iloc[:,1:] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>px_1</th>\n",
       "      <th>px_2</th>\n",
       "      <th>px_3</th>\n",
       "      <th>px_4</th>\n",
       "      <th>px_5</th>\n",
       "      <th>px_6</th>\n",
       "      <th>px_7</th>\n",
       "      <th>px_8</th>\n",
       "      <th>px_9</th>\n",
       "      <th>...</th>\n",
       "      <th>px_1591</th>\n",
       "      <th>px_1592</th>\n",
       "      <th>px_1593</th>\n",
       "      <th>px_1594</th>\n",
       "      <th>px_1595</th>\n",
       "      <th>px_1596</th>\n",
       "      <th>px_1597</th>\n",
       "      <th>px_1598</th>\n",
       "      <th>px_1599</th>\n",
       "      <th>px_1600</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>029858_01</td>\n",
       "      <td>-0.000052</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>-0.000799</td>\n",
       "      <td>-0.000487</td>\n",
       "      <td>0.000432</td>\n",
       "      <td>0.000232</td>\n",
       "      <td>-0.000193</td>\n",
       "      <td>-0.002183</td>\n",
       "      <td>0.002107</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>-0.000356</td>\n",
       "      <td>-0.000463</td>\n",
       "      <td>-0.000062</td>\n",
       "      <td>-0.000115</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>029858_02</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>-0.000063</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>029858_03</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>-0.000399</td>\n",
       "      <td>0.000194</td>\n",
       "      <td>0.293054</td>\n",
       "      <td>0.001815</td>\n",
       "      <td>-0.001204</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>029858_05</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>029858_07</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>...</td>\n",
       "      <td>1.949110</td>\n",
       "      <td>2.725132</td>\n",
       "      <td>2.120112</td>\n",
       "      <td>0.847163</td>\n",
       "      <td>0.921058</td>\n",
       "      <td>2.304371</td>\n",
       "      <td>5.406007</td>\n",
       "      <td>5.375118</td>\n",
       "      <td>2.853923</td>\n",
       "      <td>1.846065</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1601 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id      px_1      px_2      px_3      px_4      px_5      px_6  \\\n",
       "0  029858_01 -0.000052  0.000107 -0.000799 -0.000487  0.000432  0.000232   \n",
       "1  029858_02  0.000008  0.000008  0.000008  0.000008  0.000008  0.000008   \n",
       "2  029858_03  0.000008 -0.000399  0.000194  0.293054  0.001815 -0.001204   \n",
       "3  029858_05  0.000008  0.000008  0.000008  0.000008  0.000008  0.000008   \n",
       "4  029858_07  0.000008  0.000008  0.000008  0.000008  0.000008  0.000008   \n",
       "\n",
       "       px_7      px_8      px_9  ...   px_1591   px_1592   px_1593   px_1594  \\\n",
       "0 -0.000193 -0.002183  0.002107  ...  0.000008  0.000008  0.000008 -0.000356   \n",
       "1  0.000008  0.000008  0.000008  ...  0.000008  0.000008  0.000008  0.000008   \n",
       "2  0.000100  0.000008  0.000008  ...  0.000008  0.000008  0.000008  0.000008   \n",
       "3  0.000008  0.000008  0.000008  ...  0.000008  0.000008  0.000008  0.000008   \n",
       "4  0.000008  0.000008  0.000008  ...  1.949110  2.725132  2.120112  0.847163   \n",
       "\n",
       "    px_1595   px_1596   px_1597   px_1598   px_1599   px_1600  \n",
       "0 -0.000463 -0.000062 -0.000115  0.000008  0.000008  0.000008  \n",
       "1  0.000008  0.000008  0.000009 -0.000063  0.000017  0.000017  \n",
       "2  0.000008  0.000008  0.000008  0.000008  0.000008  0.000008  \n",
       "3  0.000008  0.000008  0.000008  0.000008  0.000008  0.000008  \n",
       "4  0.921058  2.304371  5.406007  5.375118  2.853923  1.846065  \n",
       "\n",
       "[5 rows x 1601 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('../D_WEATHER/sub/unet_ch9_shuffle_ho0.967.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sub = sub.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1600/1600 [00:01<00:00, 1303.99it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm.tqdm(range(1,1601)):\n",
    "    new_sub.loc[new_sub[new_sub.columns[i]]<0, new_sub.columns[i]] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>px_1</th>\n",
       "      <th>px_2</th>\n",
       "      <th>px_3</th>\n",
       "      <th>px_4</th>\n",
       "      <th>px_5</th>\n",
       "      <th>px_6</th>\n",
       "      <th>px_7</th>\n",
       "      <th>px_8</th>\n",
       "      <th>px_9</th>\n",
       "      <th>px_10</th>\n",
       "      <th>...</th>\n",
       "      <th>px_1591</th>\n",
       "      <th>px_1592</th>\n",
       "      <th>px_1593</th>\n",
       "      <th>px_1594</th>\n",
       "      <th>px_1595</th>\n",
       "      <th>px_1596</th>\n",
       "      <th>px_1597</th>\n",
       "      <th>px_1598</th>\n",
       "      <th>px_1599</th>\n",
       "      <th>px_1600</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.101228</td>\n",
       "      <td>0.113996</td>\n",
       "      <td>0.117531</td>\n",
       "      <td>0.139786</td>\n",
       "      <td>0.159077</td>\n",
       "      <td>0.170347</td>\n",
       "      <td>0.159954</td>\n",
       "      <td>0.162345</td>\n",
       "      <td>0.155707</td>\n",
       "      <td>0.145703</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136842</td>\n",
       "      <td>0.127752</td>\n",
       "      <td>0.132556</td>\n",
       "      <td>0.125462</td>\n",
       "      <td>0.119331</td>\n",
       "      <td>0.124714</td>\n",
       "      <td>0.121305</td>\n",
       "      <td>0.123893</td>\n",
       "      <td>0.127392</td>\n",
       "      <td>0.108653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.619113</td>\n",
       "      <td>0.733516</td>\n",
       "      <td>0.728896</td>\n",
       "      <td>0.889469</td>\n",
       "      <td>1.158407</td>\n",
       "      <td>1.295223</td>\n",
       "      <td>1.201258</td>\n",
       "      <td>1.189644</td>\n",
       "      <td>1.186416</td>\n",
       "      <td>1.233553</td>\n",
       "      <td>...</td>\n",
       "      <td>0.760819</td>\n",
       "      <td>0.653119</td>\n",
       "      <td>0.705653</td>\n",
       "      <td>0.760365</td>\n",
       "      <td>0.798686</td>\n",
       "      <td>0.821225</td>\n",
       "      <td>0.761279</td>\n",
       "      <td>0.752403</td>\n",
       "      <td>0.833069</td>\n",
       "      <td>0.686868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.001725</td>\n",
       "      <td>-0.018742</td>\n",
       "      <td>-0.011321</td>\n",
       "      <td>-0.009275</td>\n",
       "      <td>-0.013536</td>\n",
       "      <td>-0.005783</td>\n",
       "      <td>-0.005260</td>\n",
       "      <td>-0.014250</td>\n",
       "      <td>-0.017711</td>\n",
       "      <td>-0.009442</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010602</td>\n",
       "      <td>-0.007913</td>\n",
       "      <td>-0.013335</td>\n",
       "      <td>-0.007432</td>\n",
       "      <td>-0.008785</td>\n",
       "      <td>-0.010910</td>\n",
       "      <td>-0.008544</td>\n",
       "      <td>-0.015689</td>\n",
       "      <td>-0.010573</td>\n",
       "      <td>-0.012381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>11.714757</td>\n",
       "      <td>17.522091</td>\n",
       "      <td>14.838377</td>\n",
       "      <td>18.528639</td>\n",
       "      <td>35.296669</td>\n",
       "      <td>41.371910</td>\n",
       "      <td>32.666821</td>\n",
       "      <td>34.406021</td>\n",
       "      <td>37.121033</td>\n",
       "      <td>34.294769</td>\n",
       "      <td>...</td>\n",
       "      <td>17.918730</td>\n",
       "      <td>11.835845</td>\n",
       "      <td>14.124582</td>\n",
       "      <td>16.644712</td>\n",
       "      <td>21.221218</td>\n",
       "      <td>18.406443</td>\n",
       "      <td>19.921083</td>\n",
       "      <td>17.896057</td>\n",
       "      <td>18.871094</td>\n",
       "      <td>15.686680</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 1600 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              px_1         px_2         px_3         px_4         px_5  \\\n",
       "count  2416.000000  2416.000000  2416.000000  2416.000000  2416.000000   \n",
       "mean      0.101228     0.113996     0.117531     0.139786     0.159077   \n",
       "std       0.619113     0.733516     0.728896     0.889469     1.158407   \n",
       "min      -0.001725    -0.018742    -0.011321    -0.009275    -0.013536   \n",
       "25%       0.000008     0.000008     0.000008     0.000008     0.000008   \n",
       "50%       0.000008     0.000008     0.000008     0.000008     0.000008   \n",
       "75%       0.000008     0.000008     0.000008     0.000008     0.000008   \n",
       "max      11.714757    17.522091    14.838377    18.528639    35.296669   \n",
       "\n",
       "              px_6         px_7         px_8         px_9        px_10  ...  \\\n",
       "count  2416.000000  2416.000000  2416.000000  2416.000000  2416.000000  ...   \n",
       "mean      0.170347     0.159954     0.162345     0.155707     0.145703  ...   \n",
       "std       1.295223     1.201258     1.189644     1.186416     1.233553  ...   \n",
       "min      -0.005783    -0.005260    -0.014250    -0.017711    -0.009442  ...   \n",
       "25%       0.000008     0.000008     0.000008     0.000008     0.000008  ...   \n",
       "50%       0.000008     0.000008     0.000008     0.000008     0.000008  ...   \n",
       "75%       0.000008     0.000008     0.000008     0.000008     0.000008  ...   \n",
       "max      41.371910    32.666821    34.406021    37.121033    34.294769  ...   \n",
       "\n",
       "           px_1591      px_1592      px_1593      px_1594      px_1595  \\\n",
       "count  2416.000000  2416.000000  2416.000000  2416.000000  2416.000000   \n",
       "mean      0.136842     0.127752     0.132556     0.125462     0.119331   \n",
       "std       0.760819     0.653119     0.705653     0.760365     0.798686   \n",
       "min      -0.010602    -0.007913    -0.013335    -0.007432    -0.008785   \n",
       "25%       0.000008     0.000008     0.000008     0.000008     0.000008   \n",
       "50%       0.000008     0.000008     0.000008     0.000008     0.000008   \n",
       "75%       0.000008     0.000008     0.000008     0.000008     0.000008   \n",
       "max      17.918730    11.835845    14.124582    16.644712    21.221218   \n",
       "\n",
       "           px_1596      px_1597      px_1598      px_1599      px_1600  \n",
       "count  2416.000000  2416.000000  2416.000000  2416.000000  2416.000000  \n",
       "mean      0.124714     0.121305     0.123893     0.127392     0.108653  \n",
       "std       0.821225     0.761279     0.752403     0.833069     0.686868  \n",
       "min      -0.010910    -0.008544    -0.015689    -0.010573    -0.012381  \n",
       "25%       0.000008     0.000008     0.000008     0.000008     0.000008  \n",
       "50%       0.000008     0.000008     0.000008     0.000008     0.000008  \n",
       "75%       0.000008     0.000008     0.000008     0.000008     0.000008  \n",
       "max      18.406443    19.921083    17.896057    18.871094    15.686680  \n",
       "\n",
       "[8 rows x 1600 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>px_1</th>\n",
       "      <th>px_2</th>\n",
       "      <th>px_3</th>\n",
       "      <th>px_4</th>\n",
       "      <th>px_5</th>\n",
       "      <th>px_6</th>\n",
       "      <th>px_7</th>\n",
       "      <th>px_8</th>\n",
       "      <th>px_9</th>\n",
       "      <th>px_10</th>\n",
       "      <th>...</th>\n",
       "      <th>px_1591</th>\n",
       "      <th>px_1592</th>\n",
       "      <th>px_1593</th>\n",
       "      <th>px_1594</th>\n",
       "      <th>px_1595</th>\n",
       "      <th>px_1596</th>\n",
       "      <th>px_1597</th>\n",
       "      <th>px_1598</th>\n",
       "      <th>px_1599</th>\n",
       "      <th>px_1600</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.101234</td>\n",
       "      <td>0.114019</td>\n",
       "      <td>0.117561</td>\n",
       "      <td>0.139817</td>\n",
       "      <td>0.159133</td>\n",
       "      <td>0.170372</td>\n",
       "      <td>0.159973</td>\n",
       "      <td>0.162389</td>\n",
       "      <td>0.155737</td>\n",
       "      <td>0.145732</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136883</td>\n",
       "      <td>0.127801</td>\n",
       "      <td>0.132605</td>\n",
       "      <td>0.125490</td>\n",
       "      <td>0.119396</td>\n",
       "      <td>0.124758</td>\n",
       "      <td>0.121349</td>\n",
       "      <td>0.123944</td>\n",
       "      <td>0.127446</td>\n",
       "      <td>0.108685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.619112</td>\n",
       "      <td>0.733512</td>\n",
       "      <td>0.728891</td>\n",
       "      <td>0.889464</td>\n",
       "      <td>1.158399</td>\n",
       "      <td>1.295220</td>\n",
       "      <td>1.201256</td>\n",
       "      <td>1.189638</td>\n",
       "      <td>1.186412</td>\n",
       "      <td>1.233550</td>\n",
       "      <td>...</td>\n",
       "      <td>0.760811</td>\n",
       "      <td>0.653109</td>\n",
       "      <td>0.705644</td>\n",
       "      <td>0.760361</td>\n",
       "      <td>0.798676</td>\n",
       "      <td>0.821218</td>\n",
       "      <td>0.761272</td>\n",
       "      <td>0.752394</td>\n",
       "      <td>0.833061</td>\n",
       "      <td>0.686863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>11.714757</td>\n",
       "      <td>17.522091</td>\n",
       "      <td>14.838377</td>\n",
       "      <td>18.528639</td>\n",
       "      <td>35.296669</td>\n",
       "      <td>41.371910</td>\n",
       "      <td>32.666821</td>\n",
       "      <td>34.406021</td>\n",
       "      <td>37.121033</td>\n",
       "      <td>34.294769</td>\n",
       "      <td>...</td>\n",
       "      <td>17.918730</td>\n",
       "      <td>11.835845</td>\n",
       "      <td>14.124582</td>\n",
       "      <td>16.644712</td>\n",
       "      <td>21.221218</td>\n",
       "      <td>18.406443</td>\n",
       "      <td>19.921083</td>\n",
       "      <td>17.896057</td>\n",
       "      <td>18.871094</td>\n",
       "      <td>15.686680</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 1600 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              px_1         px_2         px_3         px_4         px_5  \\\n",
       "count  2416.000000  2416.000000  2416.000000  2416.000000  2416.000000   \n",
       "mean      0.101234     0.114019     0.117561     0.139817     0.159133   \n",
       "std       0.619112     0.733512     0.728891     0.889464     1.158399   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000008     0.000008     0.000008     0.000008     0.000008   \n",
       "50%       0.000008     0.000008     0.000008     0.000008     0.000008   \n",
       "75%       0.000008     0.000008     0.000008     0.000008     0.000008   \n",
       "max      11.714757    17.522091    14.838377    18.528639    35.296669   \n",
       "\n",
       "              px_6         px_7         px_8         px_9        px_10  ...  \\\n",
       "count  2416.000000  2416.000000  2416.000000  2416.000000  2416.000000  ...   \n",
       "mean      0.170372     0.159973     0.162389     0.155737     0.145732  ...   \n",
       "std       1.295220     1.201256     1.189638     1.186412     1.233550  ...   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "25%       0.000008     0.000008     0.000008     0.000008     0.000008  ...   \n",
       "50%       0.000008     0.000008     0.000008     0.000008     0.000008  ...   \n",
       "75%       0.000008     0.000008     0.000008     0.000008     0.000008  ...   \n",
       "max      41.371910    32.666821    34.406021    37.121033    34.294769  ...   \n",
       "\n",
       "           px_1591      px_1592      px_1593      px_1594      px_1595  \\\n",
       "count  2416.000000  2416.000000  2416.000000  2416.000000  2416.000000   \n",
       "mean      0.136883     0.127801     0.132605     0.125490     0.119396   \n",
       "std       0.760811     0.653109     0.705644     0.760361     0.798676   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000008     0.000008     0.000008     0.000008     0.000008   \n",
       "50%       0.000008     0.000008     0.000008     0.000008     0.000008   \n",
       "75%       0.000008     0.000008     0.000008     0.000008     0.000008   \n",
       "max      17.918730    11.835845    14.124582    16.644712    21.221218   \n",
       "\n",
       "           px_1596      px_1597      px_1598      px_1599      px_1600  \n",
       "count  2416.000000  2416.000000  2416.000000  2416.000000  2416.000000  \n",
       "mean      0.124758     0.121349     0.123944     0.127446     0.108685  \n",
       "std       0.821218     0.761272     0.752394     0.833061     0.686863  \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "25%       0.000008     0.000008     0.000008     0.000008     0.000008  \n",
       "50%       0.000008     0.000008     0.000008     0.000008     0.000008  \n",
       "75%       0.000008     0.000008     0.000008     0.000008     0.000008  \n",
       "max      18.406443    19.921083    17.896057    18.871094    15.686680  \n",
       "\n",
       "[8 rows x 1600 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_sub.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>px_1</th>\n",
       "      <th>px_2</th>\n",
       "      <th>px_3</th>\n",
       "      <th>px_4</th>\n",
       "      <th>px_5</th>\n",
       "      <th>px_6</th>\n",
       "      <th>px_7</th>\n",
       "      <th>px_8</th>\n",
       "      <th>px_9</th>\n",
       "      <th>...</th>\n",
       "      <th>px_1591</th>\n",
       "      <th>px_1592</th>\n",
       "      <th>px_1593</th>\n",
       "      <th>px_1594</th>\n",
       "      <th>px_1595</th>\n",
       "      <th>px_1596</th>\n",
       "      <th>px_1597</th>\n",
       "      <th>px_1598</th>\n",
       "      <th>px_1599</th>\n",
       "      <th>px_1600</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>029858_01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000432</td>\n",
       "      <td>0.000232</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002107</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>029858_02</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>029858_03</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000194</td>\n",
       "      <td>0.293054</td>\n",
       "      <td>0.001815</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>029858_05</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>029858_07</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>...</td>\n",
       "      <td>1.949110</td>\n",
       "      <td>2.725132</td>\n",
       "      <td>2.120112</td>\n",
       "      <td>0.847163</td>\n",
       "      <td>0.921058</td>\n",
       "      <td>2.304371</td>\n",
       "      <td>5.406007</td>\n",
       "      <td>5.375118</td>\n",
       "      <td>2.853923</td>\n",
       "      <td>1.846065</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1601 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id      px_1      px_2      px_3      px_4      px_5      px_6  \\\n",
       "0  029858_01  0.000000  0.000107  0.000000  0.000000  0.000432  0.000232   \n",
       "1  029858_02  0.000008  0.000008  0.000008  0.000008  0.000008  0.000008   \n",
       "2  029858_03  0.000008  0.000000  0.000194  0.293054  0.001815  0.000000   \n",
       "3  029858_05  0.000008  0.000008  0.000008  0.000008  0.000008  0.000008   \n",
       "4  029858_07  0.000008  0.000008  0.000008  0.000008  0.000008  0.000008   \n",
       "\n",
       "       px_7      px_8      px_9  ...   px_1591   px_1592   px_1593   px_1594  \\\n",
       "0  0.000000  0.000000  0.002107  ...  0.000008  0.000008  0.000008  0.000000   \n",
       "1  0.000008  0.000008  0.000008  ...  0.000008  0.000008  0.000008  0.000008   \n",
       "2  0.000100  0.000008  0.000008  ...  0.000008  0.000008  0.000008  0.000008   \n",
       "3  0.000008  0.000008  0.000008  ...  0.000008  0.000008  0.000008  0.000008   \n",
       "4  0.000008  0.000008  0.000008  ...  1.949110  2.725132  2.120112  0.847163   \n",
       "\n",
       "    px_1595   px_1596   px_1597   px_1598   px_1599   px_1600  \n",
       "0  0.000000  0.000000  0.000000  0.000008  0.000008  0.000008  \n",
       "1  0.000008  0.000008  0.000009  0.000000  0.000017  0.000017  \n",
       "2  0.000008  0.000008  0.000008  0.000008  0.000008  0.000008  \n",
       "3  0.000008  0.000008  0.000008  0.000008  0.000008  0.000008  \n",
       "4  0.921058  2.304371  5.406007  5.375118  2.853923  1.846065  \n",
       "\n",
       "[5 rows x 1601 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sub.to_csv('../D_WEATHER/sub/unet_ch9_shuffle_ho0.967_postpro.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit ('pytorch': conda)",
   "language": "python",
   "name": "python37564bitpytorchconda133dde54c45c40c2946593d30b593426"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
