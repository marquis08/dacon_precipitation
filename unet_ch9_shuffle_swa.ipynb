{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install torchcontrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchcontrib.optim import SWA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import tqdm\n",
    "import argparse\n",
    "import tqdm\n",
    "from collections import defaultdict, Counter\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import nn, cuda\n",
    "from torch.autograd import Variable \n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import CenterCrop\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR\n",
    "\n",
    "# from efficientnet_pytorch import EfficientNet\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim import Optimizer\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def mae(y_true, y_pred) :\n",
    "    y_true, y_pred = np.array(y_true.detach().numpy()), np.array(y_pred.detach().numpy())\n",
    "    y_true = y_true.reshape(1, -1)[0]\n",
    "    y_pred = y_pred.reshape(1, -1)[0]\n",
    "    over_threshold = y_true >= 0.1\n",
    "    return np.mean(np.abs(y_true[over_threshold] - y_pred[over_threshold]))\n",
    "\n",
    "def fscore(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true.detach().numpy()), np.array(y_pred.detach().numpy())\n",
    "    y_true = y_true.reshape(1, -1)[0]\n",
    "    y_pred = y_pred.reshape(1, -1)[0]\n",
    "    remove_NAs = y_true >= 0\n",
    "    y_true = np.where(y_true[remove_NAs] >= 0.1, 1, 0)\n",
    "    y_pred = np.where(y_pred[remove_NAs] >= 0.1, 1, 0)\n",
    "    return(f1_score(y_true, y_pred))\n",
    "\n",
    "def maeOverFscore(y_true, y_pred):\n",
    "    return mae(y_true, y_pred) / (fscore(y_true, y_pred) + 1e-07)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **File info**\n",
    "**ex. subset_010462_01**\n",
    "> **orbit 010462**\n",
    "\n",
    "> **subset 01**\n",
    "\n",
    "> **ortbit 별로 subset 개수는 다를 수 있고 연속적이지 않을 수도 있음**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>orbit</th>\n",
       "      <th>orbit_subset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../D_WEATHER//input/train/subset_010462_01.npy</td>\n",
       "      <td>10462</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../D_WEATHER//input/train/subset_010462_02.npy</td>\n",
       "      <td>10462</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../D_WEATHER//input/train/subset_010462_03.npy</td>\n",
       "      <td>10462</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../D_WEATHER//input/train/subset_010462_04.npy</td>\n",
       "      <td>10462</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../D_WEATHER//input/train/subset_010462_05.npy</td>\n",
       "      <td>10462</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             path  orbit  orbit_subset\n",
       "0  ../D_WEATHER//input/train/subset_010462_01.npy  10462             1\n",
       "1  ../D_WEATHER//input/train/subset_010462_02.npy  10462             2\n",
       "2  ../D_WEATHER//input/train/subset_010462_03.npy  10462             3\n",
       "3  ../D_WEATHER//input/train/subset_010462_04.npy  10462             4\n",
       "4  ../D_WEATHER//input/train/subset_010462_05.npy  10462             5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_df = pd.read_csv(\"../D_WEATHER//input/train_df.csv\")\n",
    "te_df = pd.read_csv(\"../D_WEATHER/input/test_df.csv\")\n",
    "tr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = tr_df[:int(len(tr_df)*0.8)]\n",
    "valid_df = tr_df[int(len(tr_df)*0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((61076, 3), (15269, 3))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, valid_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Weather_Dataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        \n",
    "        self.image_list = []\n",
    "        self.label_list = []\n",
    "\n",
    "        for file in self.df['path']:\n",
    "            data = np.load(file)\n",
    "            image = data[:,:,:9] # use 14 channels except target\n",
    "            image = np.transpose(image, (2,0,1))\n",
    "            image = image.astype(np.float32)\n",
    "            self.image_list.append(image)\n",
    "            \n",
    "            label = data[:,:,-1].reshape(40,40,1)\n",
    "            label = np.transpose(label, (2,0,1))\n",
    "            self.label_list.append(label)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        image = self.image_list[idx]\n",
    "        label = self.label_list[idx]\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def worker_init(worker_id):\n",
    "#     np.random.seed(SEED)\n",
    "\n",
    "def build_dataloader(df, batch_size, shuffle=False):\n",
    "    dataset = Weather_Dataset(df)\n",
    "    dataloader = DataLoader(\n",
    "                            dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=shuffle,\n",
    "                            num_workers=0,\n",
    "#                             worker_init_fn=worker_init\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "def build_te_dataloader(df, batch_size, shuffle=False):\n",
    "    dataset = Test_Dataset(df)\n",
    "    dataloader = DataLoader(\n",
    "                            dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=shuffle,\n",
    "                            num_workers=0,\n",
    "#                             worker_init_fn=worker_init\n",
    "                            )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=True):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels # \n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = Down(512, 1024 // factor)\n",
    "        self.up1 = Up(1024, 512, bilinear)\n",
    "        self.up2 = Up(512, 256, bilinear)\n",
    "        self.up3 = Up(256, 128, bilinear)\n",
    "        self.up4 = Up(128, 64 * factor, bilinear)\n",
    "        self.outc = OutConv(64, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels // 2, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels , in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        diffY = torch.tensor([x2.size()[2] - x1.size()[2]])\n",
    "        diffX = torch.tensor([x2.size()[3] - x1.size()[3]])\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        # if you have padding issues, see\n",
    "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
    "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "batch_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = build_dataloader(train_df, batch_size, shuffle=True)\n",
    "valid_loader = build_dataloader(valid_df, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enable gpu use\n",
      "start training\n",
      "================ ༼ つ ◕_◕ ༽つ BEST epoch : 1, MOF : 4.279534769857475 \n",
      "E 1/200 tr_loss: 42.49235 tr_mae: 1.95531 tr_fs: 0.40804 tr_mof: 6.38813 val_loss: 52.07499 val_mae: 1.90440 val_fs: 0.50094 val_mof: 4.27953 lr: 0.000500 elapsed: 101\n",
      "================ ༼ つ ◕_◕ ༽つ BEST epoch : 2, MOF : 3.389866356197008 \n",
      "E 2/200 tr_loss: 42.45600 tr_mae: 1.69906 tr_fs: 0.61487 tr_mof: 2.76603 val_loss: 52.06353 val_mae: 1.87237 val_fs: 0.55373 val_mof: 3.38987 lr: 0.000500 elapsed: 101\n",
      "================ ༼ つ ◕_◕ ༽つ BEST epoch : 3, MOF : 2.9061517672475397 \n",
      "E 3/200 tr_loss: 42.56369 tr_mae: 1.60234 tr_fs: 0.64948 tr_mof: 2.46972 val_loss: 52.06832 val_mae: 1.75945 val_fs: 0.60500 val_mof: 2.90615 lr: 0.000500 elapsed: 102\n",
      "E 4/200 tr_loss: 43.64538 tr_mae: 1.53956 tr_fs: 0.66513 tr_mof: 2.31626 val_loss: 52.05553 val_mae: 1.78505 val_fs: 0.60436 val_mof: 2.95344 lr: 0.000500 elapsed: 102\n",
      "================ ༼ つ ◕_◕ ༽つ BEST epoch : 5, MOF : 2.5747513004622005 \n",
      "E 5/200 tr_loss: 42.56572 tr_mae: 1.49669 tr_fs: 0.68068 tr_mof: 2.20022 val_loss: 52.06188 val_mae: 1.73181 val_fs: 0.67091 val_mof: 2.57475 lr: 0.000500 elapsed: 102\n",
      "================ ༼ つ ◕_◕ ༽つ BEST epoch : 6, MOF : 2.3196938739007233 \n",
      "E 6/200 tr_loss: 42.43699 tr_mae: 1.46345 tr_fs: 0.69161 tr_mof: 2.11648 val_loss: 52.04115 val_mae: 1.58214 val_fs: 0.68035 val_mof: 2.31969 lr: 0.000500 elapsed: 102\n",
      "E 7/200 tr_loss: 42.70547 tr_mae: 1.45023 tr_fs: 0.68960 tr_mof: 2.10987 val_loss: 52.64800 val_mae: 2.25181 val_fs: 0.14334 val_mof: 16.06061 lr: 0.000500 elapsed: 102\n",
      "E 8/200 tr_loss: 42.84107 tr_mae: 1.51354 tr_fs: 0.67106 tr_mof: 2.26224 val_loss: 52.04607 val_mae: 1.66779 val_fs: 0.62657 val_mof: 2.66025 lr: 0.000500 elapsed: 102\n",
      "E 9/200 tr_loss: 42.43610 tr_mae: 1.45083 tr_fs: 0.69271 tr_mof: 2.09478 val_loss: 52.13485 val_mae: 1.53993 val_fs: 0.27720 val_mof: 5.63581 lr: 0.000500 elapsed: 102\n",
      "================ ༼ つ ◕_◕ ༽つ BEST epoch : 10, MOF : 2.272833329942057 \n",
      "E 10/200 tr_loss: 42.83393 tr_mae: 1.42074 tr_fs: 0.70320 tr_mof: 2.02084 val_loss: 52.03868 val_mae: 1.51917 val_fs: 0.66764 val_mof: 2.27283 lr: 0.000500 elapsed: 101\n",
      "E 11/200 tr_loss: 42.83255 tr_mae: 1.40575 tr_fs: 0.70679 tr_mof: 1.99024 val_loss: 52.11849 val_mae: 1.54243 val_fs: 0.41332 val_mof: 3.85700 lr: 0.000500 elapsed: 102\n",
      "================ ༼ つ ◕_◕ ༽つ BEST epoch : 12, MOF : 2.064355896078073 \n",
      "E 12/200 tr_loss: 43.23239 tr_mae: 1.39516 tr_fs: 0.71336 tr_mof: 1.95551 val_loss: 52.03520 val_mae: 1.49126 val_fs: 0.71993 val_mof: 2.06436 lr: 0.000500 elapsed: 102\n",
      "E 13/200 tr_loss: 42.43132 tr_mae: 1.39262 tr_fs: 0.71286 tr_mof: 1.95330 val_loss: 52.04225 val_mae: 1.54338 val_fs: 0.63025 val_mof: 2.46098 lr: 0.000500 elapsed: 102\n",
      "E 14/200 tr_loss: 42.43048 tr_mae: 1.37995 tr_fs: 0.71848 tr_mof: 1.92182 val_loss: 52.24199 val_mae: 1.64140 val_fs: 0.38145 val_mof: 4.47657 lr: 0.000500 elapsed: 101\n",
      "E 15/200 tr_loss: 42.43013 tr_mae: 1.37263 tr_fs: 0.72084 tr_mof: 1.90520 val_loss: 52.05870 val_mae: 1.79022 val_fs: 0.54196 val_mof: 3.31346 lr: 0.000500 elapsed: 101\n",
      "E 16/200 tr_loss: 42.42978 tr_mae: 1.36804 tr_fs: 0.72371 tr_mof: 1.89041 val_loss: 52.57613 val_mae: 2.07649 val_fs: 0.31159 val_mof: 6.88782 lr: 0.000500 elapsed: 102\n",
      "E 17/200 tr_loss: 42.67959 tr_mae: 1.36260 tr_fs: 0.72564 tr_mof: 1.87779 val_loss: 52.04940 val_mae: 1.52115 val_fs: 0.57189 val_mof: 2.66372 lr: 0.000500 elapsed: 102\n",
      "E 18/200 tr_loss: 42.42875 tr_mae: 1.35452 tr_fs: 0.72921 tr_mof: 1.85751 val_loss: 52.09837 val_mae: 1.49608 val_fs: 0.40998 val_mof: 3.81559 lr: 0.000500 elapsed: 102\n",
      "E 19/200 tr_loss: 42.42877 tr_mae: 1.35466 tr_fs: 0.72974 tr_mof: 1.85655 val_loss: 52.09724 val_mae: 1.59103 val_fs: 0.42979 val_mof: 3.90101 lr: 0.000500 elapsed: 102\n",
      "E 20/200 tr_loss: 42.42795 tr_mae: 1.34640 tr_fs: 0.73226 tr_mof: 1.83836 val_loss: 52.04933 val_mae: 1.50599 val_fs: 0.54495 val_mof: 2.80075 lr: 0.000500 elapsed: 102\n",
      "E 21/200 tr_loss: 42.42703 tr_mae: 1.33131 tr_fs: 0.73840 tr_mof: 1.80330 val_loss: 52.04993 val_mae: 1.55742 val_fs: 0.63552 val_mof: 2.45073 lr: 0.000500 elapsed: 101\n",
      "E 22/200 tr_loss: 42.42771 tr_mae: 1.34059 tr_fs: 0.73465 tr_mof: 1.82554 val_loss: 52.12722 val_mae: 1.59043 val_fs: 0.45917 val_mof: 3.59862 lr: 0.000500 elapsed: 102\n",
      "E 23/200 tr_loss: 42.67120 tr_mae: 1.32420 tr_fs: 0.74279 tr_mof: 1.78381 val_loss: 52.03187 val_mae: 1.47814 val_fs: 0.67417 val_mof: 2.18584 lr: 0.000500 elapsed: 101\n",
      "E 24/200 tr_loss: 42.42626 tr_mae: 1.32731 tr_fs: 0.73972 tr_mof: 1.79534 val_loss: 52.03412 val_mae: 1.53235 val_fs: 0.68062 val_mof: 2.24490 lr: 0.000500 elapsed: 102\n",
      "================ ༼ つ ◕_◕ ༽つ BEST epoch : 25, MOF : 2.0551100205778026 \n",
      "E 25/200 tr_loss: 42.42701 tr_mae: 1.33247 tr_fs: 0.73842 tr_mof: 1.80498 val_loss: 52.02870 val_mae: 1.46744 val_fs: 0.71206 val_mof: 2.05511 lr: 0.000500 elapsed: 101\n",
      "E 26/200 tr_loss: 43.22587 tr_mae: 1.30978 tr_fs: 0.74656 tr_mof: 1.75560 val_loss: 52.05900 val_mae: 1.78127 val_fs: 0.47831 val_mof: 3.74586 lr: 0.000500 elapsed: 101\n",
      "E 27/200 tr_loss: 42.83092 tr_mae: 1.38768 tr_fs: 0.71629 tr_mof: 1.93861 val_loss: 52.04776 val_mae: 1.50463 val_fs: 0.60934 val_mof: 2.48966 lr: 0.000500 elapsed: 102\n",
      "E 28/200 tr_loss: 42.42680 tr_mae: 1.33577 tr_fs: 0.73677 tr_mof: 1.81332 val_loss: 52.04673 val_mae: 1.42761 val_fs: 0.59614 val_mof: 2.39050 lr: 0.000500 elapsed: 102\n",
      "E 29/200 tr_loss: 42.42538 tr_mae: 1.31561 tr_fs: 0.74663 tr_mof: 1.76211 val_loss: 52.05011 val_mae: 1.76629 val_fs: 0.59324 val_mof: 2.97529 lr: 0.000500 elapsed: 101\n",
      "E 30/200 tr_loss: 43.04633 tr_mae: 1.32068 tr_fs: 0.74328 tr_mof: 1.77752 val_loss: 52.05604 val_mae: 1.58194 val_fs: 0.49897 val_mof: 3.42312 lr: 0.000500 elapsed: 101\n",
      "E 31/200 tr_loss: 42.82669 tr_mae: 1.33004 tr_fs: 0.74268 tr_mof: 1.79252 val_loss: 52.19215 val_mae: 1.48390 val_fs: 0.29286 val_mof: 5.14114 lr: 0.000500 elapsed: 102\n",
      "================ ༼ つ ◕_◕ ༽つ BEST epoch : 32, MOF : 1.9418144900489116 \n",
      "E 32/200 tr_loss: 42.66515 tr_mae: 1.31646 tr_fs: 0.74789 tr_mof: 1.76032 val_loss: 52.03739 val_mae: 1.45184 val_fs: 0.74532 val_mof: 1.94181 lr: 0.000500 elapsed: 101\n",
      "E 33/200 tr_loss: 42.42489 tr_mae: 1.30545 tr_fs: 0.75102 tr_mof: 1.73807 val_loss: 52.03752 val_mae: 1.57670 val_fs: 0.70541 val_mof: 2.22874 lr: 0.000500 elapsed: 101\n",
      "E 34/200 tr_loss: 42.82492 tr_mae: 1.30356 tr_fs: 0.74978 tr_mof: 1.73837 val_loss: 53.60141 val_mae: 2.70589 val_fs: 0.31052 val_mof: 8.67968 lr: 0.000500 elapsed: 101\n",
      "E 35/200 tr_loss: 42.82577 tr_mae: 1.31271 tr_fs: 0.74611 tr_mof: 1.76032 val_loss: 52.03133 val_mae: 1.49040 val_fs: 0.69940 val_mof: 2.12377 lr: 0.000500 elapsed: 101\n",
      "E 36/200 tr_loss: 43.22467 tr_mae: 1.29921 tr_fs: 0.75323 tr_mof: 1.72511 val_loss: 52.26982 val_mae: 1.50687 val_fs: 0.29342 val_mof: 5.32945 lr: 0.000500 elapsed: 101\n",
      "E 37/200 tr_loss: 42.42413 tr_mae: 1.29876 tr_fs: 0.75345 tr_mof: 1.72385 val_loss: 52.17627 val_mae: 1.51730 val_fs: 0.30990 val_mof: 5.09479 lr: 0.000500 elapsed: 101\n",
      "E 38/200 tr_loss: 42.42341 tr_mae: 1.29096 tr_fs: 0.75616 tr_mof: 1.70751 val_loss: 52.03262 val_mae: 1.41270 val_fs: 0.68465 val_mof: 2.06005 lr: 0.000500 elapsed: 101\n",
      "E 39/200 tr_loss: 42.42591 tr_mae: 1.31964 tr_fs: 0.74293 tr_mof: 1.77778 val_loss: 52.03215 val_mae: 1.47350 val_fs: 0.69487 val_mof: 2.11376 lr: 0.000500 elapsed: 101\n",
      "E 40/200 tr_loss: 43.22502 tr_mae: 1.30184 tr_fs: 0.75149 tr_mof: 1.73266 val_loss: 52.43820 val_mae: 1.56665 val_fs: 0.21328 val_mof: 7.67603 lr: 0.000500 elapsed: 101\n",
      "E 41/200 tr_loss: 42.42381 tr_mae: 1.29236 tr_fs: 0.75409 tr_mof: 1.71406 val_loss: 52.03637 val_mae: 1.55390 val_fs: 0.69773 val_mof: 2.22037 lr: 0.000500 elapsed: 101\n",
      "E 42/200 tr_loss: 42.42374 tr_mae: 1.29413 tr_fs: 0.75551 tr_mof: 1.71319 val_loss: 52.03107 val_mae: 1.49424 val_fs: 0.69996 val_mof: 2.12916 lr: 0.000500 elapsed: 101\n",
      "E 43/200 tr_loss: 42.42344 tr_mae: 1.29166 tr_fs: 0.75539 tr_mof: 1.71044 val_loss: 52.04743 val_mae: 1.66352 val_fs: 0.63282 val_mof: 2.61509 lr: 0.000500 elapsed: 101\n",
      "E 44/200 tr_loss: 42.42321 tr_mae: 1.28850 tr_fs: 0.75751 tr_mof: 1.70124 val_loss: 52.03588 val_mae: 1.49827 val_fs: 0.69097 val_mof: 2.16706 lr: 0.000500 elapsed: 101\n",
      "E 45/200 tr_loss: 42.42373 tr_mae: 1.29067 tr_fs: 0.75336 tr_mof: 1.71393 val_loss: 52.04658 val_mae: 1.44080 val_fs: 0.66609 val_mof: 2.17878 lr: 0.000500 elapsed: 101\n",
      "E 46/200 tr_loss: 42.82327 tr_mae: 1.28240 tr_fs: 0.75872 tr_mof: 1.69034 val_loss: 52.11214 val_mae: 1.47169 val_fs: 0.43458 val_mof: 3.47883 lr: 0.000500 elapsed: 101\n",
      "E 47/200 tr_loss: 42.55994 tr_mae: 1.28045 tr_fs: 0.75894 tr_mof: 1.68713 val_loss: 52.29643 val_mae: 1.56393 val_fs: 0.32851 val_mof: 4.86870 lr: 0.000500 elapsed: 101\n",
      "E 48/200 tr_loss: 42.42259 tr_mae: 1.27744 tr_fs: 0.75794 tr_mof: 1.68546 val_loss: 52.02989 val_mae: 1.46633 val_fs: 0.72335 val_mof: 2.02026 lr: 0.000500 elapsed: 101\n",
      "E 49/200 tr_loss: 42.42265 tr_mae: 1.28012 tr_fs: 0.75830 tr_mof: 1.68869 val_loss: 52.10011 val_mae: 1.56855 val_fs: 0.55086 val_mof: 2.89342 lr: 0.000500 elapsed: 101\n",
      "E 50/200 tr_loss: 42.42274 tr_mae: 1.27906 tr_fs: 0.75746 tr_mof: 1.68827 val_loss: 52.11702 val_mae: 1.78294 val_fs: 0.31658 val_mof: 5.76082 lr: 0.000500 elapsed: 101\n",
      "E 51/200 tr_loss: 43.22744 tr_mae: 1.33511 tr_fs: 0.73561 tr_mof: 1.81800 val_loss: 52.03095 val_mae: 1.43020 val_fs: 0.73089 val_mof: 1.94999 lr: 0.000500 elapsed: 101\n",
      "E 52/200 tr_loss: 43.62505 tr_mae: 1.30071 tr_fs: 0.75098 tr_mof: 1.73279 val_loss: 52.30814 val_mae: 1.68007 val_fs: 0.38428 val_mof: 4.52330 lr: 0.000500 elapsed: 101\n",
      "E 53/200 tr_loss: 42.82451 tr_mae: 1.30069 tr_fs: 0.75223 tr_mof: 1.72924 val_loss: 52.04984 val_mae: 1.64332 val_fs: 0.58083 val_mof: 2.83961 lr: 0.000500 elapsed: 101\n",
      "E 54/200 tr_loss: 43.39624 tr_mae: 1.28430 tr_fs: 0.75650 tr_mof: 1.69768 val_loss: 52.03107 val_mae: 1.47297 val_fs: 0.72649 val_mof: 2.02146 lr: 0.000500 elapsed: 101\n",
      "E 55/200 tr_loss: 42.44985 tr_mae: 1.28607 tr_fs: 0.75820 tr_mof: 1.69652 val_loss: 52.04905 val_mae: 1.74583 val_fs: 0.59709 val_mof: 2.91441 lr: 0.000500 elapsed: 101\n",
      "E 56/200 tr_loss: 42.82274 tr_mae: 1.27747 tr_fs: 0.75937 tr_mof: 1.68267 val_loss: 52.04319 val_mae: 1.53379 val_fs: 0.73688 val_mof: 2.07452 lr: 0.000500 elapsed: 101\n",
      "E 57/200 tr_loss: 42.42271 tr_mae: 1.27962 tr_fs: 0.75945 tr_mof: 1.68491 val_loss: 52.08755 val_mae: 1.44947 val_fs: 0.44218 val_mof: 3.41499 lr: 0.000500 elapsed: 101\n",
      "E 58/200 tr_loss: 42.82277 tr_mae: 1.28064 tr_fs: 0.76048 tr_mof: 1.68451 val_loss: 52.16370 val_mae: 2.04032 val_fs: 0.34433 val_mof: 6.30257 lr: 0.000500 elapsed: 101\n",
      "E 59/200 tr_loss: 42.82244 tr_mae: 1.27555 tr_fs: 0.75886 tr_mof: 1.68128 val_loss: 52.07773 val_mae: 2.07512 val_fs: 0.67735 val_mof: 3.04289 lr: 0.000500 elapsed: 101\n",
      "E 60/200 tr_loss: 42.42192 tr_mae: 1.27290 tr_fs: 0.76248 tr_mof: 1.66917 val_loss: 52.02885 val_mae: 1.47484 val_fs: 0.72814 val_mof: 2.01997 lr: 0.000500 elapsed: 101\n",
      "================ ༼ つ ◕_◕ ༽つ BEST epoch : 61, MOF : 1.9274417745071954 \n",
      "E 61/200 tr_loss: 42.82212 tr_mae: 1.27059 tr_fs: 0.76282 tr_mof: 1.66555 val_loss: 52.02515 val_mae: 1.41773 val_fs: 0.73327 val_mof: 1.92744 lr: 0.000500 elapsed: 101\n",
      "E 62/200 tr_loss: 43.09612 tr_mae: 1.27021 tr_fs: 0.76391 tr_mof: 1.66270 val_loss: 52.03816 val_mae: 1.57655 val_fs: 0.66349 val_mof: 2.37143 lr: 0.000500 elapsed: 101\n",
      "E 63/200 tr_loss: 42.82198 tr_mae: 1.27009 tr_fs: 0.76355 tr_mof: 1.66366 val_loss: 52.03984 val_mae: 1.55138 val_fs: 0.58484 val_mof: 2.65456 lr: 0.000500 elapsed: 102\n",
      "E 64/200 tr_loss: 42.82473 tr_mae: 1.26947 tr_fs: 0.76319 tr_mof: 1.66402 val_loss: 52.13536 val_mae: 1.53318 val_fs: 0.32384 val_mof: 4.95238 lr: 0.000500 elapsed: 101\n",
      "E 65/200 tr_loss: 42.42160 tr_mae: 1.26677 tr_fs: 0.76394 tr_mof: 1.65825 val_loss: 52.02852 val_mae: 1.43087 val_fs: 0.73272 val_mof: 1.94637 lr: 0.000500 elapsed: 101\n",
      "E 66/200 tr_loss: 42.82201 tr_mae: 1.26876 tr_fs: 0.76337 tr_mof: 1.66205 val_loss: 52.93401 val_mae: 2.01867 val_fs: 0.35512 val_mof: 5.98188 lr: 0.000500 elapsed: 101\n",
      "E 67/200 tr_loss: 42.58802 tr_mae: 1.26651 tr_fs: 0.76411 tr_mof: 1.65763 val_loss: 52.05674 val_mae: 1.47630 val_fs: 0.60994 val_mof: 2.41607 lr: 0.000500 elapsed: 101\n",
      "E 68/200 tr_loss: 42.42148 tr_mae: 1.26491 tr_fs: 0.76427 tr_mof: 1.65504 val_loss: 52.03639 val_mae: 1.54957 val_fs: 0.65172 val_mof: 2.37424 lr: 0.000500 elapsed: 101\n",
      "E 69/200 tr_loss: 42.57024 tr_mae: 1.26694 tr_fs: 0.76285 tr_mof: 1.66092 val_loss: 52.03972 val_mae: 1.59766 val_fs: 0.66334 val_mof: 2.40058 lr: 0.000500 elapsed: 101\n",
      "E 70/200 tr_loss: 42.42164 tr_mae: 1.26778 tr_fs: 0.76512 tr_mof: 1.65742 val_loss: 52.07241 val_mae: 1.42735 val_fs: 0.58252 val_mof: 2.50398 lr: 0.000500 elapsed: 101\n",
      "E 71/200 tr_loss: 42.42185 tr_mae: 1.27079 tr_fs: 0.76110 tr_mof: 1.66979 val_loss: 52.03751 val_mae: 1.56622 val_fs: 0.59939 val_mof: 2.60545 lr: 0.000500 elapsed: 101\n",
      "E 72/200 tr_loss: 42.42175 tr_mae: 1.26661 tr_fs: 0.76366 tr_mof: 1.65872 val_loss: 52.14366 val_mae: 1.47863 val_fs: 0.47078 val_mof: 3.35952 lr: 0.000500 elapsed: 101\n",
      "E 73/200 tr_loss: 42.82168 tr_mae: 1.26529 tr_fs: 0.76419 tr_mof: 1.65625 val_loss: 52.03415 val_mae: 1.55276 val_fs: 0.64256 val_mof: 2.41222 lr: 0.000500 elapsed: 101\n",
      "E 74/200 tr_loss: 42.42164 tr_mae: 1.26743 tr_fs: 0.76391 tr_mof: 1.65908 val_loss: 52.05200 val_mae: 1.52955 val_fs: 0.68000 val_mof: 2.24938 lr: 0.000500 elapsed: 101\n",
      "E 75/200 tr_loss: 43.22295 tr_mae: 1.27324 tr_fs: 0.76131 tr_mof: 1.67267 val_loss: 52.11092 val_mae: 1.36070 val_fs: 0.30719 val_mof: 4.56105 lr: 0.000500 elapsed: 101\n",
      "E 76/200 tr_loss: 42.82203 tr_mae: 1.26759 tr_fs: 0.76210 tr_mof: 1.66329 val_loss: 52.06474 val_mae: 1.75964 val_fs: 0.65261 val_mof: 2.69122 lr: 0.000500 elapsed: 101\n",
      "E 77/200 tr_loss: 42.82147 tr_mae: 1.25985 tr_fs: 0.76613 tr_mof: 1.64410 val_loss: 52.03405 val_mae: 1.51985 val_fs: 0.68227 val_mof: 2.22130 lr: 0.000500 elapsed: 101\n",
      "E 78/200 tr_loss: 42.82180 tr_mae: 1.26492 tr_fs: 0.76527 tr_mof: 1.65275 val_loss: 52.02840 val_mae: 1.46154 val_fs: 0.71187 val_mof: 2.04777 lr: 0.000500 elapsed: 101\n",
      "E 79/200 tr_loss: 42.42087 tr_mae: 1.25711 tr_fs: 0.76739 tr_mof: 1.63825 val_loss: 52.18988 val_mae: 1.52601 val_fs: 0.36448 val_mof: 4.32486 lr: 0.000500 elapsed: 101\n",
      "E 80/200 tr_loss: 42.42143 tr_mae: 1.26481 tr_fs: 0.76587 tr_mof: 1.65164 val_loss: 52.08808 val_mae: 1.36291 val_fs: 0.42133 val_mof: 3.34408 lr: 0.000500 elapsed: 102\n",
      "E 81/200 tr_loss: 42.82163 tr_mae: 1.26283 tr_fs: 0.76703 tr_mof: 1.64646 val_loss: 52.03385 val_mae: 1.54031 val_fs: 0.69833 val_mof: 2.20238 lr: 0.000500 elapsed: 101\n",
      "E 82/200 tr_loss: 42.82186 tr_mae: 1.26359 tr_fs: 0.76503 tr_mof: 1.65185 val_loss: 52.05955 val_mae: 1.60041 val_fs: 0.67589 val_mof: 2.35388 lr: 0.000500 elapsed: 101\n",
      "E 83/200 tr_loss: 42.42118 tr_mae: 1.26199 tr_fs: 0.76585 tr_mof: 1.64809 val_loss: 53.35495 val_mae: 2.77633 val_fs: 0.35574 val_mof: 8.05216 lr: 0.000500 elapsed: 101\n",
      "E 84/200 tr_loss: 42.62015 tr_mae: 1.25782 tr_fs: 0.76685 tr_mof: 1.64024 val_loss: 52.03589 val_mae: 1.52545 val_fs: 0.68404 val_mof: 2.22470 lr: 0.000500 elapsed: 101\n",
      "E 85/200 tr_loss: 42.82109 tr_mae: 1.25694 tr_fs: 0.76703 tr_mof: 1.63913 val_loss: 52.03834 val_mae: 1.59021 val_fs: 0.65519 val_mof: 2.42477 lr: 0.000500 elapsed: 101\n",
      "E 86/200 tr_loss: 42.82125 tr_mae: 1.25879 tr_fs: 0.76833 tr_mof: 1.63837 val_loss: 52.05299 val_mae: 1.53385 val_fs: 0.56968 val_mof: 2.84495 lr: 0.000500 elapsed: 101\n",
      "================ ༼ つ ◕_◕ ༽つ BEST epoch : 87, MOF : 1.901441181649444 \n",
      "E 87/200 tr_loss: 42.82119 tr_mae: 1.25837 tr_fs: 0.76698 tr_mof: 1.64099 val_loss: 52.02613 val_mae: 1.39308 val_fs: 0.73072 val_mof: 1.90144 lr: 0.000500 elapsed: 101\n",
      "E 88/200 tr_loss: 42.82093 tr_mae: 1.25188 tr_fs: 0.76944 tr_mof: 1.62681 val_loss: 52.24111 val_mae: 1.68993 val_fs: 0.58397 val_mof: 2.95214 lr: 0.000500 elapsed: 101\n",
      "E 89/200 tr_loss: 42.42102 tr_mae: 1.26024 tr_fs: 0.76704 tr_mof: 1.64335 val_loss: 52.02825 val_mae: 1.45040 val_fs: 0.73372 val_mof: 1.97049 lr: 0.000500 elapsed: 101\n",
      "E 90/200 tr_loss: 42.42087 tr_mae: 1.25786 tr_fs: 0.76789 tr_mof: 1.63817 val_loss: 52.07794 val_mae: 1.46739 val_fs: 0.45113 val_mof: 3.32877 lr: 0.000500 elapsed: 101\n",
      "E 91/200 tr_loss: 42.82232 tr_mae: 1.27311 tr_fs: 0.76120 tr_mof: 1.67273 val_loss: 52.03322 val_mae: 1.41460 val_fs: 0.69831 val_mof: 2.02035 lr: 0.000500 elapsed: 101\n",
      "E 92/200 tr_loss: 42.42094 tr_mae: 1.25799 tr_fs: 0.76863 tr_mof: 1.63680 val_loss: 52.02883 val_mae: 1.46684 val_fs: 0.74744 val_mof: 1.95499 lr: 0.000500 elapsed: 101\n",
      "E 93/200 tr_loss: 43.33659 tr_mae: 1.25969 tr_fs: 0.76847 tr_mof: 1.63910 val_loss: 52.12940 val_mae: 1.42307 val_fs: 0.38241 val_mof: 3.84493 lr: 0.000500 elapsed: 101\n",
      "E 94/200 tr_loss: 42.82102 tr_mae: 1.25617 tr_fs: 0.76954 tr_mof: 1.63256 val_loss: 52.05565 val_mae: 1.69345 val_fs: 0.68039 val_mof: 2.47711 lr: 0.000500 elapsed: 101\n",
      "E 95/200 tr_loss: 42.82073 tr_mae: 1.25099 tr_fs: 0.76906 tr_mof: 1.62641 val_loss: 52.02806 val_mae: 1.46600 val_fs: 0.70200 val_mof: 2.08162 lr: 0.000500 elapsed: 101\n",
      "E 96/200 tr_loss: 42.42088 tr_mae: 1.25463 tr_fs: 0.76815 tr_mof: 1.63349 val_loss: 52.03321 val_mae: 1.55653 val_fs: 0.59595 val_mof: 2.63573 lr: 0.000500 elapsed: 101\n",
      "E 97/200 tr_loss: 42.82106 tr_mae: 1.25395 tr_fs: 0.76907 tr_mof: 1.63040 val_loss: 52.08582 val_mae: 2.11039 val_fs: 0.71004 val_mof: 2.94952 lr: 0.000500 elapsed: 101\n",
      "E 98/200 tr_loss: 42.82093 tr_mae: 1.25528 tr_fs: 0.76829 tr_mof: 1.63415 val_loss: 52.05628 val_mae: 1.81201 val_fs: 0.65006 val_mof: 2.77656 lr: 0.000500 elapsed: 101\n",
      "E 99/200 tr_loss: 42.65558 tr_mae: 1.25818 tr_fs: 0.76742 tr_mof: 1.63945 val_loss: 52.03688 val_mae: 1.35365 val_fs: 0.64858 val_mof: 2.10104 lr: 0.000500 elapsed: 101\n",
      "E 100/200 tr_loss: 42.42060 tr_mae: 1.25275 tr_fs: 0.76945 tr_mof: 1.62840 val_loss: 52.10271 val_mae: 2.32616 val_fs: 0.74150 val_mof: 3.11208 lr: 0.000500 elapsed: 101\n",
      "E 101/200 tr_loss: 42.82107 tr_mae: 1.25549 tr_fs: 0.76840 tr_mof: 1.63409 val_loss: 52.02766 val_mae: 1.42217 val_fs: 0.73135 val_mof: 1.93775 lr: 0.000500 elapsed: 101\n",
      "E 102/200 tr_loss: 42.82048 tr_mae: 1.24707 tr_fs: 0.77131 tr_mof: 1.61672 val_loss: 52.03712 val_mae: 1.52016 val_fs: 0.68656 val_mof: 2.20694 lr: 0.000500 elapsed: 101\n",
      "E 103/200 tr_loss: 42.42111 tr_mae: 1.26190 tr_fs: 0.76607 tr_mof: 1.64721 val_loss: 52.03265 val_mae: 1.52553 val_fs: 0.67664 val_mof: 2.24818 lr: 0.000500 elapsed: 101\n",
      "E 104/200 tr_loss: 43.04027 tr_mae: 1.25146 tr_fs: 0.76985 tr_mof: 1.62589 val_loss: 52.02943 val_mae: 1.51186 val_fs: 0.60914 val_mof: 2.48217 lr: 0.000500 elapsed: 101\n",
      "E 105/200 tr_loss: 43.22112 tr_mae: 1.25256 tr_fs: 0.76907 tr_mof: 1.62887 val_loss: 52.10829 val_mae: 1.64841 val_fs: 0.49705 val_mof: 3.41389 lr: 0.000500 elapsed: 101\n",
      "E 106/200 tr_loss: 42.42052 tr_mae: 1.25344 tr_fs: 0.76921 tr_mof: 1.62965 val_loss: 52.03060 val_mae: 1.52372 val_fs: 0.63637 val_mof: 2.39177 lr: 0.000500 elapsed: 101\n",
      "E 107/200 tr_loss: 42.42062 tr_mae: 1.25231 tr_fs: 0.76999 tr_mof: 1.62644 val_loss: 52.07678 val_mae: 1.81258 val_fs: 0.71338 val_mof: 2.52610 lr: 0.000500 elapsed: 101\n",
      "E 108/200 tr_loss: 42.82047 tr_mae: 1.24841 tr_fs: 0.77035 tr_mof: 1.62047 val_loss: 52.60086 val_mae: 1.70994 val_fs: 0.31931 val_mof: 5.56342 lr: 0.000500 elapsed: 101\n",
      "E 109/200 tr_loss: 42.82147 tr_mae: 1.26152 tr_fs: 0.76607 tr_mof: 1.64660 val_loss: 52.03125 val_mae: 1.51625 val_fs: 0.66173 val_mof: 2.29293 lr: 0.000500 elapsed: 101\n",
      "E 110/200 tr_loss: 42.82057 tr_mae: 1.24897 tr_fs: 0.77117 tr_mof: 1.61989 val_loss: 52.03601 val_mae: 1.59254 val_fs: 0.58914 val_mof: 2.69691 lr: 0.000500 elapsed: 101\n",
      "E 111/200 tr_loss: 43.81919 tr_mae: 1.24491 tr_fs: 0.77206 tr_mof: 1.61245 val_loss: 52.03914 val_mae: 1.56724 val_fs: 0.66787 val_mof: 2.34014 lr: 0.000500 elapsed: 101\n",
      "E 112/200 tr_loss: 42.42040 tr_mae: 1.24833 tr_fs: 0.77126 tr_mof: 1.61846 val_loss: 52.18850 val_mae: 1.52461 val_fs: 0.38096 val_mof: 4.12766 lr: 0.000500 elapsed: 101\n",
      "E 113/200 tr_loss: 42.42044 tr_mae: 1.25123 tr_fs: 0.76928 tr_mof: 1.62688 val_loss: 52.03008 val_mae: 1.49607 val_fs: 0.66458 val_mof: 2.25424 lr: 0.000500 elapsed: 101\n",
      "E 114/200 tr_loss: 43.22075 tr_mae: 1.24564 tr_fs: 0.77071 tr_mof: 1.61600 val_loss: 52.03119 val_mae: 1.51717 val_fs: 0.70336 val_mof: 2.15275 lr: 0.000500 elapsed: 101\n",
      "E 115/200 tr_loss: 42.42016 tr_mae: 1.24813 tr_fs: 0.77116 tr_mof: 1.61879 val_loss: 52.22838 val_mae: 1.50503 val_fs: 0.39028 val_mof: 4.17355 lr: 0.000500 elapsed: 101\n",
      "E 116/200 tr_loss: 42.42012 tr_mae: 1.24600 tr_fs: 0.77176 tr_mof: 1.61414 val_loss: 52.03205 val_mae: 1.52364 val_fs: 0.64224 val_mof: 2.37072 lr: 0.000500 elapsed: 101\n",
      "E 117/200 tr_loss: 42.82070 tr_mae: 1.24764 tr_fs: 0.77032 tr_mof: 1.61978 val_loss: 52.05238 val_mae: 1.67890 val_fs: 0.64648 val_mof: 2.58793 lr: 0.000500 elapsed: 101\n",
      "E 118/200 tr_loss: 43.20533 tr_mae: 1.24424 tr_fs: 0.77228 tr_mof: 1.61105 val_loss: 52.04511 val_mae: 1.59595 val_fs: 0.64472 val_mof: 2.49907 lr: 0.000500 elapsed: 101\n",
      "E 119/200 tr_loss: 43.62134 tr_mae: 1.24976 tr_fs: 0.76869 tr_mof: 1.62624 val_loss: 52.03714 val_mae: 1.61428 val_fs: 0.56266 val_mof: 2.87903 lr: 0.000500 elapsed: 101\n",
      "E 120/200 tr_loss: 42.82041 tr_mae: 1.24799 tr_fs: 0.77060 tr_mof: 1.61970 val_loss: 52.03112 val_mae: 1.50629 val_fs: 0.65401 val_mof: 2.30014 lr: 0.000500 elapsed: 101\n",
      "E 121/200 tr_loss: 42.42151 tr_mae: 1.25966 tr_fs: 0.76396 tr_mof: 1.64942 val_loss: 52.04975 val_mae: 1.75288 val_fs: 0.57929 val_mof: 3.01694 lr: 0.000500 elapsed: 101\n",
      "E 122/200 tr_loss: 42.41991 tr_mae: 1.24606 tr_fs: 0.77121 tr_mof: 1.61641 val_loss: 52.33988 val_mae: 1.59534 val_fs: 0.34775 val_mof: 4.70986 lr: 0.000500 elapsed: 101\n",
      "E 123/200 tr_loss: 42.82072 tr_mae: 1.24761 tr_fs: 0.77048 tr_mof: 1.61935 val_loss: 52.10197 val_mae: 1.40874 val_fs: 0.41424 val_mof: 3.48661 lr: 0.000500 elapsed: 101\n",
      "E 124/200 tr_loss: 42.82011 tr_mae: 1.24134 tr_fs: 0.77191 tr_mof: 1.60810 val_loss: 52.09314 val_mae: 1.78904 val_fs: 0.46417 val_mof: 3.90730 lr: 0.000500 elapsed: 101\n",
      "E 125/200 tr_loss: 42.41987 tr_mae: 1.24380 tr_fs: 0.77168 tr_mof: 1.61194 val_loss: 52.04870 val_mae: 1.40537 val_fs: 0.59247 val_mof: 2.63348 lr: 0.000500 elapsed: 101\n",
      "E 126/200 tr_loss: 42.42000 tr_mae: 1.24253 tr_fs: 0.77211 tr_mof: 1.60926 val_loss: 52.04501 val_mae: 1.62898 val_fs: 0.74273 val_mof: 2.18457 lr: 0.000500 elapsed: 101\n",
      "E 127/200 tr_loss: 42.82028 tr_mae: 1.24160 tr_fs: 0.77293 tr_mof: 1.60650 val_loss: 52.14840 val_mae: 1.46373 val_fs: 0.49936 val_mof: 3.03162 lr: 0.000500 elapsed: 101\n",
      "E 128/200 tr_loss: 42.67938 tr_mae: 1.24117 tr_fs: 0.77152 tr_mof: 1.60832 val_loss: 52.03481 val_mae: 1.54231 val_fs: 0.64768 val_mof: 2.37871 lr: 0.000500 elapsed: 101\n",
      "E 129/200 tr_loss: 42.41971 tr_mae: 1.24228 tr_fs: 0.77114 tr_mof: 1.61054 val_loss: 52.02843 val_mae: 1.44396 val_fs: 0.71446 val_mof: 2.01619 lr: 0.000500 elapsed: 101\n",
      "E 130/200 tr_loss: 42.82027 tr_mae: 1.24427 tr_fs: 0.77309 tr_mof: 1.60923 val_loss: 52.02915 val_mae: 1.42704 val_fs: 0.71958 val_mof: 1.97765 lr: 0.000500 elapsed: 101\n",
      "E 131/200 tr_loss: 42.82001 tr_mae: 1.24116 tr_fs: 0.77229 tr_mof: 1.60740 val_loss: 52.03887 val_mae: 1.54024 val_fs: 0.68188 val_mof: 2.25117 lr: 0.000500 elapsed: 101\n",
      "E 132/200 tr_loss: 42.41971 tr_mae: 1.24075 tr_fs: 0.77236 tr_mof: 1.60662 val_loss: 52.03306 val_mae: 1.54565 val_fs: 0.65186 val_mof: 2.36682 lr: 0.000500 elapsed: 101\n",
      "================ ༼ つ ◕_◕ ༽つ BEST epoch : 133, MOF : 1.8660543404858678 \n",
      "E 133/200 tr_loss: 42.81988 tr_mae: 1.23980 tr_fs: 0.77197 tr_mof: 1.60616 val_loss: 52.02630 val_mae: 1.36509 val_fs: 0.72927 val_mof: 1.86605 lr: 0.000500 elapsed: 101\n",
      "E 134/200 tr_loss: 42.41946 tr_mae: 1.23702 tr_fs: 0.77323 tr_mof: 1.59995 val_loss: 52.03626 val_mae: 1.38852 val_fs: 0.65147 val_mof: 2.12787 lr: 0.000500 elapsed: 102\n",
      "E 135/200 tr_loss: 43.36132 tr_mae: 1.23931 tr_fs: 0.77279 tr_mof: 1.60374 val_loss: 52.03314 val_mae: 1.51977 val_fs: 0.62410 val_mof: 2.42978 lr: 0.000500 elapsed: 101\n",
      "E 136/200 tr_loss: 42.41953 tr_mae: 1.23657 tr_fs: 0.77385 tr_mof: 1.59782 val_loss: 52.03155 val_mae: 1.43789 val_fs: 0.70224 val_mof: 2.03990 lr: 0.000500 elapsed: 101\n",
      "E 137/200 tr_loss: 42.41976 tr_mae: 1.24082 tr_fs: 0.77206 tr_mof: 1.60728 val_loss: 52.05570 val_mae: 1.72214 val_fs: 0.68888 val_mof: 2.49316 lr: 0.000500 elapsed: 101\n",
      "E 138/200 tr_loss: 42.41952 tr_mae: 1.23782 tr_fs: 0.77383 tr_mof: 1.59948 val_loss: 52.09765 val_mae: 1.41388 val_fs: 0.50834 val_mof: 3.07254 lr: 0.000500 elapsed: 101\n",
      "E 139/200 tr_loss: 42.81999 tr_mae: 1.23979 tr_fs: 0.77311 tr_mof: 1.60400 val_loss: 52.28067 val_mae: 1.70519 val_fs: 0.44909 val_mof: 3.93075 lr: 0.000500 elapsed: 101\n",
      "E 140/200 tr_loss: 42.41959 tr_mae: 1.23776 tr_fs: 0.77229 tr_mof: 1.60281 val_loss: 52.03972 val_mae: 1.63313 val_fs: 0.60550 val_mof: 2.69461 lr: 0.000500 elapsed: 101\n",
      "E 141/200 tr_loss: 42.82047 tr_mae: 1.24565 tr_fs: 0.77018 tr_mof: 1.61733 val_loss: 52.12505 val_mae: 1.37616 val_fs: 0.29405 val_mof: 4.72804 lr: 0.000500 elapsed: 101\n",
      "E 142/200 tr_loss: 42.41966 tr_mae: 1.23964 tr_fs: 0.77297 tr_mof: 1.60384 val_loss: 52.05297 val_mae: 1.44342 val_fs: 0.57342 val_mof: 2.56615 lr: 0.000500 elapsed: 101\n",
      "E 143/200 tr_loss: 42.41956 tr_mae: 1.23799 tr_fs: 0.77282 tr_mof: 1.60216 val_loss: 52.03161 val_mae: 1.45839 val_fs: 0.70253 val_mof: 2.07753 lr: 0.000500 elapsed: 101\n",
      "E 144/200 tr_loss: 42.42030 tr_mae: 1.24691 tr_fs: 0.76950 tr_mof: 1.62086 val_loss: 52.33028 val_mae: 1.61613 val_fs: 0.36127 val_mof: 4.59371 lr: 0.000500 elapsed: 101\n",
      "E 145/200 tr_loss: 43.16004 tr_mae: 1.23954 tr_fs: 0.77168 tr_mof: 1.60622 val_loss: 52.03232 val_mae: 1.40838 val_fs: 0.73878 val_mof: 1.89876 lr: 0.000500 elapsed: 101\n",
      "================ ༼ つ ◕_◕ ༽つ BEST epoch : 146, MOF : 1.8339169996093196 \n",
      "E 146/200 tr_loss: 42.81940 tr_mae: 1.23135 tr_fs: 0.77483 tr_mof: 1.58951 val_loss: 52.02627 val_mae: 1.37059 val_fs: 0.74615 val_mof: 1.83392 lr: 0.000500 elapsed: 101\n",
      "E 147/200 tr_loss: 42.42223 tr_mae: 1.23773 tr_fs: 0.77410 tr_mof: 1.59860 val_loss: 52.11499 val_mae: 1.45871 val_fs: 0.35841 val_mof: 4.19563 lr: 0.000500 elapsed: 101\n",
      "E 148/200 tr_loss: 42.81980 tr_mae: 1.23635 tr_fs: 0.77380 tr_mof: 1.59782 val_loss: 52.13489 val_mae: 1.42051 val_fs: 0.34760 val_mof: 4.26605 lr: 0.000500 elapsed: 101\n",
      "E 149/200 tr_loss: 42.82039 tr_mae: 1.24604 tr_fs: 0.77006 tr_mof: 1.61787 val_loss: 52.08915 val_mae: 1.47714 val_fs: 0.54934 val_mof: 2.74704 lr: 0.000500 elapsed: 101\n",
      "E 150/200 tr_loss: 42.81995 tr_mae: 1.23863 tr_fs: 0.77336 tr_mof: 1.60158 val_loss: 52.03398 val_mae: 1.54733 val_fs: 0.66246 val_mof: 2.33106 lr: 0.000500 elapsed: 101\n",
      "E 151/200 tr_loss: 42.42061 tr_mae: 1.24910 tr_fs: 0.76986 tr_mof: 1.62263 val_loss: 52.06895 val_mae: 1.52267 val_fs: 0.49610 val_mof: 3.09285 lr: 0.000500 elapsed: 101\n",
      "E 152/200 tr_loss: 43.02635 tr_mae: 1.23579 tr_fs: 0.77426 tr_mof: 1.59609 val_loss: 52.05271 val_mae: 1.50348 val_fs: 0.66617 val_mof: 2.26589 lr: 0.000500 elapsed: 101\n",
      "E 153/200 tr_loss: 42.41927 tr_mae: 1.23298 tr_fs: 0.77382 tr_mof: 1.59348 val_loss: 52.16885 val_mae: 1.66697 val_fs: 0.46239 val_mof: 3.72614 lr: 0.000500 elapsed: 101\n",
      "E 154/200 tr_loss: 42.41955 tr_mae: 1.23611 tr_fs: 0.77326 tr_mof: 1.59858 val_loss: 52.09897 val_mae: 1.59233 val_fs: 0.45380 val_mof: 3.76331 lr: 0.000500 elapsed: 101\n",
      "E 155/200 tr_loss: 42.81965 tr_mae: 1.23427 tr_fs: 0.77338 tr_mof: 1.59585 val_loss: 52.03259 val_mae: 1.46345 val_fs: 0.73898 val_mof: 1.97291 lr: 0.000500 elapsed: 101\n",
      "E 156/200 tr_loss: 42.67874 tr_mae: 1.23273 tr_fs: 0.77407 tr_mof: 1.59221 val_loss: 52.14622 val_mae: 1.43229 val_fs: 0.46081 val_mof: 3.21158 lr: 0.000500 elapsed: 101\n",
      "E 157/200 tr_loss: 43.21977 tr_mae: 1.23287 tr_fs: 0.77364 tr_mof: 1.59379 val_loss: 52.09283 val_mae: 1.49600 val_fs: 0.35931 val_mof: 4.27220 lr: 0.000500 elapsed: 101\n",
      "E 158/200 tr_loss: 43.22005 tr_mae: 1.23672 tr_fs: 0.77447 tr_mof: 1.59718 val_loss: 52.03065 val_mae: 1.49059 val_fs: 0.66551 val_mof: 2.23378 lr: 0.000500 elapsed: 101\n",
      "E 159/200 tr_loss: 42.41915 tr_mae: 1.23327 tr_fs: 0.77455 tr_mof: 1.59208 val_loss: 52.06096 val_mae: 1.63855 val_fs: 0.49474 val_mof: 3.35927 lr: 0.000500 elapsed: 101\n",
      "E 160/200 tr_loss: 42.74056 tr_mae: 1.23511 tr_fs: 0.77345 tr_mof: 1.59704 val_loss: 52.06525 val_mae: 1.54756 val_fs: 0.62172 val_mof: 2.51539 lr: 0.000500 elapsed: 101\n",
      "E 161/200 tr_loss: 42.41975 tr_mae: 1.24168 tr_fs: 0.77257 tr_mof: 1.60750 val_loss: 52.04213 val_mae: 1.37609 val_fs: 0.58934 val_mof: 2.33468 lr: 0.000500 elapsed: 101\n",
      "E 162/200 tr_loss: 42.81973 tr_mae: 1.23485 tr_fs: 0.77445 tr_mof: 1.59424 val_loss: 52.11754 val_mae: 1.48675 val_fs: 0.34451 val_mof: 4.41208 lr: 0.000500 elapsed: 102\n",
      "E 163/200 tr_loss: 42.57164 tr_mae: 1.23344 tr_fs: 0.77311 tr_mof: 1.59544 val_loss: 52.04616 val_mae: 1.66639 val_fs: 0.69638 val_mof: 2.38185 lr: 0.000500 elapsed: 101\n",
      "E 164/200 tr_loss: 42.42226 tr_mae: 1.23592 tr_fs: 0.77371 tr_mof: 1.59735 val_loss: 52.03775 val_mae: 1.51361 val_fs: 0.71136 val_mof: 2.12319 lr: 0.000500 elapsed: 102\n",
      "E 165/200 tr_loss: 42.81992 tr_mae: 1.23634 tr_fs: 0.77347 tr_mof: 1.59846 val_loss: 52.05357 val_mae: 1.39069 val_fs: 0.51105 val_mof: 2.87046 lr: 0.000500 elapsed: 101\n",
      "E 166/200 tr_loss: 42.41920 tr_mae: 1.23296 tr_fs: 0.77379 tr_mof: 1.59346 val_loss: 52.03645 val_mae: 1.59791 val_fs: 0.53956 val_mof: 2.97853 lr: 0.000500 elapsed: 101\n",
      "E 167/200 tr_loss: 42.81956 tr_mae: 1.23319 tr_fs: 0.77425 tr_mof: 1.59256 val_loss: 52.24719 val_mae: 1.45177 val_fs: 0.36293 val_mof: 4.10648 lr: 0.000500 elapsed: 101\n",
      "E 168/200 tr_loss: 42.41935 tr_mae: 1.23294 tr_fs: 0.77350 tr_mof: 1.59412 val_loss: 52.03822 val_mae: 1.59506 val_fs: 0.59544 val_mof: 2.66973 lr: 0.000500 elapsed: 101\n",
      "E 169/200 tr_loss: 42.81974 tr_mae: 1.23456 tr_fs: 0.77407 tr_mof: 1.59504 val_loss: 52.05576 val_mae: 1.84755 val_fs: 0.58410 val_mof: 3.15403 lr: 0.000500 elapsed: 100\n",
      "E 170/200 tr_loss: 43.46484 tr_mae: 1.23424 tr_fs: 0.77283 tr_mof: 1.59679 val_loss: 52.09099 val_mae: 1.33382 val_fs: 0.32171 val_mof: 4.23566 lr: 0.000500 elapsed: 101\n",
      "E 171/200 tr_loss: 43.22001 tr_mae: 1.23634 tr_fs: 0.77196 tr_mof: 1.60143 val_loss: 52.03444 val_mae: 1.41928 val_fs: 0.72193 val_mof: 1.96451 lr: 0.000500 elapsed: 101\n",
      "E 172/200 tr_loss: 42.41908 tr_mae: 1.23107 tr_fs: 0.77509 tr_mof: 1.58839 val_loss: 52.05642 val_mae: 1.46848 val_fs: 0.55767 val_mof: 2.66022 lr: 0.000500 elapsed: 102\n",
      "E 173/200 tr_loss: 42.81954 tr_mae: 1.23297 tr_fs: 0.77390 tr_mof: 1.59316 val_loss: 52.03408 val_mae: 1.54166 val_fs: 0.61016 val_mof: 2.52208 lr: 0.000500 elapsed: 104\n",
      "E 174/200 tr_loss: 43.04918 tr_mae: 1.23270 tr_fs: 0.77371 tr_mof: 1.59332 val_loss: 52.04650 val_mae: 1.73490 val_fs: 0.59021 val_mof: 2.93542 lr: 0.000500 elapsed: 102\n",
      "E 175/200 tr_loss: 42.41891 tr_mae: 1.22913 tr_fs: 0.77617 tr_mof: 1.58371 val_loss: 52.04999 val_mae: 1.40979 val_fs: 0.60506 val_mof: 2.51851 lr: 0.000500 elapsed: 101\n",
      "E 176/200 tr_loss: 42.41894 tr_mae: 1.22708 tr_fs: 0.77601 tr_mof: 1.58118 val_loss: 52.03511 val_mae: 1.57767 val_fs: 0.63530 val_mof: 2.47859 lr: 0.000500 elapsed: 101\n",
      "E 177/200 tr_loss: 42.41919 tr_mae: 1.23233 tr_fs: 0.77400 tr_mof: 1.59238 val_loss: 52.06919 val_mae: 1.41343 val_fs: 0.57811 val_mof: 2.49231 lr: 0.000500 elapsed: 101\n",
      "E 178/200 tr_loss: 42.41918 tr_mae: 1.23114 tr_fs: 0.77444 tr_mof: 1.58920 val_loss: 52.02809 val_mae: 1.44992 val_fs: 0.70593 val_mof: 2.04788 lr: 0.000500 elapsed: 101\n",
      "E 179/200 tr_loss: 42.41915 tr_mae: 1.23014 tr_fs: 0.77480 tr_mof: 1.58772 val_loss: 52.13563 val_mae: 1.50382 val_fs: 0.45178 val_mof: 3.37507 lr: 0.000500 elapsed: 101\n",
      "E 180/200 tr_loss: 42.81943 tr_mae: 1.23162 tr_fs: 0.77419 tr_mof: 1.59092 val_loss: 52.07696 val_mae: 1.92622 val_fs: 0.70744 val_mof: 2.70745 lr: 0.000500 elapsed: 101\n",
      "E 181/200 tr_loss: 42.41920 tr_mae: 1.23279 tr_fs: 0.77582 tr_mof: 1.58903 val_loss: 52.08756 val_mae: 1.36367 val_fs: 0.42918 val_mof: 3.20134 lr: 0.000500 elapsed: 101\n",
      "E 182/200 tr_loss: 42.81952 tr_mae: 1.23286 tr_fs: 0.77243 tr_mof: 1.59601 val_loss: 52.03036 val_mae: 1.47851 val_fs: 0.71561 val_mof: 2.05921 lr: 0.000500 elapsed: 101\n",
      "E 183/200 tr_loss: 42.81912 tr_mae: 1.22746 tr_fs: 0.77525 tr_mof: 1.58360 val_loss: 52.03353 val_mae: 1.46239 val_fs: 0.67627 val_mof: 2.21275 lr: 0.000500 elapsed: 101\n",
      "E 184/200 tr_loss: 42.62199 tr_mae: 1.23032 tr_fs: 0.77407 tr_mof: 1.58936 val_loss: 52.11251 val_mae: 1.41601 val_fs: 0.49061 val_mof: 3.25418 lr: 0.000500 elapsed: 101\n",
      "E 185/200 tr_loss: 42.41901 tr_mae: 1.23027 tr_fs: 0.77344 tr_mof: 1.59088 val_loss: 52.03173 val_mae: 1.49534 val_fs: 0.66504 val_mof: 2.24089 lr: 0.000500 elapsed: 101\n",
      "E 186/200 tr_loss: 42.57160 tr_mae: 1.23356 tr_fs: 0.77401 tr_mof: 1.59381 val_loss: 52.10161 val_mae: 1.63706 val_fs: 0.32396 val_mof: 5.14854 lr: 0.000500 elapsed: 101\n",
      "E 187/200 tr_loss: 42.41929 tr_mae: 1.23283 tr_fs: 0.77365 tr_mof: 1.59394 val_loss: 52.03524 val_mae: 1.55211 val_fs: 0.71247 val_mof: 2.17285 lr: 0.000500 elapsed: 101\n",
      "E 188/200 tr_loss: 42.41912 tr_mae: 1.23031 tr_fs: 0.77451 tr_mof: 1.58842 val_loss: 52.02897 val_mae: 1.43253 val_fs: 0.68171 val_mof: 2.10922 lr: 0.000500 elapsed: 101\n",
      "E 189/200 tr_loss: 42.81925 tr_mae: 1.22826 tr_fs: 0.77464 tr_mof: 1.58570 val_loss: 52.02730 val_mae: 1.43275 val_fs: 0.70354 val_mof: 2.03536 lr: 0.000500 elapsed: 101\n",
      "E 190/200 tr_loss: 43.61979 tr_mae: 1.22662 tr_fs: 0.77637 tr_mof: 1.57967 val_loss: 52.06141 val_mae: 1.74575 val_fs: 0.72157 val_mof: 2.40577 lr: 0.000500 elapsed: 101\n",
      "E 191/200 tr_loss: 42.41904 tr_mae: 1.23047 tr_fs: 0.77411 tr_mof: 1.58967 val_loss: 52.08834 val_mae: 1.47821 val_fs: 0.46058 val_mof: 3.29401 lr: 0.000500 elapsed: 101\n",
      "E 192/200 tr_loss: 42.41903 tr_mae: 1.23009 tr_fs: 0.77518 tr_mof: 1.58677 val_loss: 52.10082 val_mae: 1.46141 val_fs: 0.44996 val_mof: 3.34769 lr: 0.000500 elapsed: 101\n",
      "E 193/200 tr_loss: 42.41875 tr_mae: 1.22723 tr_fs: 0.77474 tr_mof: 1.58402 val_loss: 52.03074 val_mae: 1.45502 val_fs: 0.75191 val_mof: 1.92761 lr: 0.000500 elapsed: 102\n",
      "E 194/200 tr_loss: 42.41907 tr_mae: 1.23113 tr_fs: 0.77477 tr_mof: 1.58887 val_loss: 52.03773 val_mae: 1.60639 val_fs: 0.66375 val_mof: 2.41661 lr: 0.000500 elapsed: 101\n",
      "E 195/200 tr_loss: 42.41880 tr_mae: 1.22844 tr_fs: 0.77490 tr_mof: 1.58529 val_loss: 52.02551 val_mae: 1.40552 val_fs: 0.74411 val_mof: 1.88295 lr: 0.000500 elapsed: 101\n",
      "E 196/200 tr_loss: 43.21955 tr_mae: 1.22757 tr_fs: 0.77521 tr_mof: 1.58373 val_loss: 52.03449 val_mae: 1.55692 val_fs: 0.67259 val_mof: 2.30558 lr: 0.000500 elapsed: 101\n",
      "E 197/200 tr_loss: 42.81908 tr_mae: 1.22724 tr_fs: 0.77496 tr_mof: 1.58365 val_loss: 52.03324 val_mae: 1.52392 val_fs: 0.62332 val_mof: 2.43866 lr: 0.000500 elapsed: 101\n",
      "E 198/200 tr_loss: 42.41892 tr_mae: 1.22931 tr_fs: 0.77526 tr_mof: 1.58556 val_loss: 52.02388 val_mae: 1.39424 val_fs: 0.75266 val_mof: 1.84617 lr: 0.000500 elapsed: 101\n",
      "E 199/200 tr_loss: 42.67656 tr_mae: 1.22679 tr_fs: 0.77591 tr_mof: 1.58124 val_loss: 52.03760 val_mae: 1.51086 val_fs: 0.73683 val_mof: 2.04298 lr: 0.000500 elapsed: 101\n",
      "E 200/200 tr_loss: 42.41868 tr_mae: 1.22568 tr_fs: 0.77547 tr_mof: 1.58058 val_loss: 52.03160 val_mae: 1.51789 val_fs: 0.67683 val_mof: 2.23665 lr: 0.000500 elapsed: 101\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "seed_everything(SEED)\n",
    "\n",
    "device = 'cuda:0'\n",
    "use_gpu = cuda.is_available()\n",
    "if use_gpu:\n",
    "    print(\"enable gpu use\")\n",
    "else:\n",
    "    print(\"enable cpu for debugging\")\n",
    "\n",
    "model = UNet(n_channels=9, n_classes=1, bilinear=False) # if bilinear = True -> non deterministic : not recommended\n",
    "model = model.to(device)\n",
    "\n",
    "base_opt = optim.Adam(model.parameters(), lr = 0.001, weight_decay=0.00025)\n",
    "optimizer = SWA(base_opt, swa_start=10, swa_freq=2, swa_lr=0.0005)\n",
    "# optimizer = AdamW(model.parameters(), 2.5e-4, weight_decay=0.000025)\n",
    "#optimizer = optim.SGD(model.parameters(), args.lr, momentum=0.9, weight_decay=0.025)\n",
    "\n",
    "###### SCHEDULER #######\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n",
    "\n",
    "#eta_min = 0.00001\n",
    "#T_max = 10\n",
    "#T_mult = 1\n",
    "#restart_decay = 0.97\n",
    "#scheduler = CosineAnnealingWithRestartsLR(optimizer, T_max=T_max, eta_min=eta_min, T_mult=T_mult, restart_decay=restart_decay)\n",
    "\n",
    "#scheduler = StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss() \n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "def to_numpy(t):\n",
    "    return t.cpu().detach().numpy()\n",
    "\n",
    "best_mae_score = 999\n",
    "best_f_score = 999\n",
    "best_mof_score = 999\n",
    "grad_clip_step = 100\n",
    "grad_clip = 100\n",
    "step = 0\n",
    "# accumulation_step = 2\n",
    "EPOCH = 200\n",
    "\n",
    "model_fname = '../D_WEATHER/weight/unet_ch9_shuffle_swa.pt'\n",
    "# log file\n",
    "log_df = pd.DataFrame(columns=['epoch_idx', 'train_loss', 'train_mae', 'train_fs', 'train_mof', 'valid_loss', 'valid_mae', 'valid_fs', 'valid_mof'])\n",
    "\n",
    "print(\"start training\")\n",
    "\n",
    "for epoch_idx in range(1, EPOCH + 1):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss = 0\n",
    "    train_mae = 0\n",
    "    train_fs = 0\n",
    "    train_mof = 0 \n",
    "#     train_total_correct = 0\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for batch_idx, (image, labels) in enumerate(train_loader):\n",
    "        if use_gpu:\n",
    "            image = image.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "        output = model(image)\n",
    "        loss = criterion(output, labels)\n",
    "        mae_score = mae(labels.cpu(), output.cpu())\n",
    "        f_score = fscore(labels.cpu(), output.cpu())\n",
    "        mof_score = maeOverFscore(labels.cpu(), output.cpu())\n",
    "\n",
    "        # gradient explosion prevention\n",
    "        if step > grad_clip_step:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "\n",
    "        step += 1\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        train_loss += loss.item() / len(train_loader)\n",
    "        train_mae += mae_score.item() / len(train_loader)\n",
    "        train_fs += f_score.item() / len(train_loader)\n",
    "        train_mof += mof_score.item() / len(train_loader)\n",
    "\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    valid_mae = 0\n",
    "    valid_fs = 0\n",
    "    valid_mof = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (image, labels) in enumerate(valid_loader):\n",
    "            if use_gpu:\n",
    "                image = image.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "            output = model(image)\n",
    "            loss = criterion(output, labels)\n",
    "            mae_score = mae(labels.cpu(), output.cpu())\n",
    "            f_score = fscore(labels.cpu(), output.cpu())\n",
    "            mof_score = maeOverFscore(labels.cpu(), output.cpu())\n",
    "\n",
    "#             output_prob = F.sigmoid(output)\n",
    "\n",
    "            predict_vector = to_numpy(output)\n",
    "\n",
    "            valid_loss += loss.item() / len(valid_loader)\n",
    "            valid_mae += mae_score.item() / len(valid_loader)\n",
    "            valid_fs += f_score.item() / len(valid_loader)\n",
    "            valid_mof += mof_score.item() / len(valid_loader)\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "    # checkpoint\n",
    "    if valid_mof < best_mof_score:\n",
    "        best_mof_score = valid_mof\n",
    "#         print(\"Improved !! \")\n",
    "        torch.save(model.state_dict(), model_fname)\n",
    "        print(\"================ ༼ つ ◕_◕ ༽つ BEST epoch : {}, MOF : {} \".format(epoch_idx, best_mof_score))\n",
    "        #file_save_name = 'best_acc' + '_' + str(num_fold)\n",
    "        #print(file_save_name)\n",
    "#     else:\n",
    "#         print(\"val acc has not improved\")\n",
    "\n",
    "    lr = [_['lr'] for _ in optimizer.param_groups]\n",
    "\n",
    "    #if args.scheduler == 'plateau':\n",
    "    scheduler.step(valid_mof)\n",
    "    #else:\n",
    "    #    scheduler.step()\n",
    "\n",
    "    # nsml.save(epoch_idx)\n",
    "\n",
    "    print(\"E {}/{} tr_loss: {:.5f} tr_mae: {:.5f} tr_fs: {:.5f} tr_mof: {:.5f} val_loss: {:.5f} val_mae: {:.5f} val_fs: {:.5f} val_mof: {:.5f} lr: {:.6f} elapsed: {:.0f}\".format(\n",
    "           epoch_idx, EPOCH, train_loss, train_mae, train_fs, train_mof, valid_loss, valid_mae, valid_fs, valid_mof, lr[0], elapsed))\n",
    "            #epoch_idx, args.epochs, train_loss, valid_loss, val_acc, lr[0], elapsed\n",
    "    # log file element\n",
    "#     log = []\n",
    "    log_data = [epoch_idx, train_loss, train_mae, train_fs, train_mof, valid_loss, valid_mae, valid_fs, valid_mof]\n",
    "#     log.append(log_data)\n",
    "    log_df.loc[epoch_idx] = log_data\n",
    "\n",
    "optimizer.swap_swa_sgd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df.to_csv(\"../D_WEATHER/log/unet_ch9_shuffle_swa.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test_Dataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        \n",
    "        self.image_list = []\n",
    "#         self.label_list = []\n",
    "\n",
    "        for file in self.df['path']:\n",
    "            data = np.load(file)\n",
    "#             image = data[:,:,:]\n",
    "            image = data[:,:,:9]#.reshape(40,40,-1)\n",
    "            image = np.transpose(image, (2,0,1))\n",
    "            image = image.astype(np.float32)\n",
    "            self.image_list.append(image)\n",
    "            \n",
    "#             label = data[:,:,-1].reshape(-1)\n",
    "#             self.label_list.append(label)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        image = self.image_list[idx]\n",
    "#         label = self.label_list[idx]\n",
    "        \n",
    "        return image#, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = build_te_dataloader(te_df, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2416, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loader.dataset.df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 40, 40)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 40, 40)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loader.dataset[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict values check :  [8.93034958e-05 8.93034958e-05 9.14128905e-05 ... 8.93034958e-05\n",
      " 8.93034958e-05 8.93034958e-05]\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(model_fname))\n",
    "model.eval()\n",
    "predictions = np.zeros((len(test_loader.dataset), 1600))\n",
    "with torch.no_grad():\n",
    "    for i, image in enumerate(test_loader):\n",
    "        image = image.to(device)\n",
    "        output = model(image)\n",
    "        \n",
    "        predictions[i*batch_size: (i+1)*batch_size] = output.detach().cpu().numpy().reshape(-1, 1600)\n",
    "print(\"predict values check : \",predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2416, 1600)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8.93034958e-05, 8.93034958e-05, 9.14128905e-05, ...,\n",
       "       8.93034958e-05, 8.93034958e-05, 8.93034958e-05])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv(\"../D_WEATHER/input/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>px_1</th>\n",
       "      <th>px_2</th>\n",
       "      <th>px_3</th>\n",
       "      <th>px_4</th>\n",
       "      <th>px_5</th>\n",
       "      <th>px_6</th>\n",
       "      <th>px_7</th>\n",
       "      <th>px_8</th>\n",
       "      <th>px_9</th>\n",
       "      <th>...</th>\n",
       "      <th>px_1591</th>\n",
       "      <th>px_1592</th>\n",
       "      <th>px_1593</th>\n",
       "      <th>px_1594</th>\n",
       "      <th>px_1595</th>\n",
       "      <th>px_1596</th>\n",
       "      <th>px_1597</th>\n",
       "      <th>px_1598</th>\n",
       "      <th>px_1599</th>\n",
       "      <th>px_1600</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>029858_01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>029858_02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>029858_03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>029858_05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>029858_07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1601 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  px_1  px_2  px_3  px_4  px_5  px_6  px_7  px_8  px_9  ...  \\\n",
       "0  029858_01   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "1  029858_02   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "2  029858_03   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "3  029858_05   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "4  029858_07   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "\n",
       "   px_1591  px_1592  px_1593  px_1594  px_1595  px_1596  px_1597  px_1598  \\\n",
       "0      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "1      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "2      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "3      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "4      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "   px_1599  px_1600  \n",
       "0      0.0      0.0  \n",
       "1      0.0      0.0  \n",
       "2      0.0      0.0  \n",
       "3      0.0      0.0  \n",
       "4      0.0      0.0  \n",
       "\n",
       "[5 rows x 1601 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.iloc[:,1:] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>px_1</th>\n",
       "      <th>px_2</th>\n",
       "      <th>px_3</th>\n",
       "      <th>px_4</th>\n",
       "      <th>px_5</th>\n",
       "      <th>px_6</th>\n",
       "      <th>px_7</th>\n",
       "      <th>px_8</th>\n",
       "      <th>px_9</th>\n",
       "      <th>...</th>\n",
       "      <th>px_1591</th>\n",
       "      <th>px_1592</th>\n",
       "      <th>px_1593</th>\n",
       "      <th>px_1594</th>\n",
       "      <th>px_1595</th>\n",
       "      <th>px_1596</th>\n",
       "      <th>px_1597</th>\n",
       "      <th>px_1598</th>\n",
       "      <th>px_1599</th>\n",
       "      <th>px_1600</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>029858_01</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>-0.000970</td>\n",
       "      <td>-0.000721</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>029858_02</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>029858_03</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.022229</td>\n",
       "      <td>0.170700</td>\n",
       "      <td>0.000540</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>029858_05</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>029858_07</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>...</td>\n",
       "      <td>1.712617</td>\n",
       "      <td>2.498391</td>\n",
       "      <td>1.441851</td>\n",
       "      <td>0.665086</td>\n",
       "      <td>0.458185</td>\n",
       "      <td>1.311074</td>\n",
       "      <td>3.439888</td>\n",
       "      <td>3.354876</td>\n",
       "      <td>1.726757</td>\n",
       "      <td>0.960530</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1601 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id      px_1      px_2      px_3      px_4      px_5      px_6  \\\n",
       "0  029858_01  0.000089  0.000089  0.000091  0.000089 -0.000007 -0.000035   \n",
       "1  029858_02  0.000089  0.000089  0.000089  0.000089  0.000089  0.000089   \n",
       "2  029858_03  0.000089  0.000089  0.000110  0.022229  0.170700  0.000540   \n",
       "3  029858_05  0.000089  0.000089  0.000089  0.000089  0.000089  0.000089   \n",
       "4  029858_07  0.000089  0.000089  0.000089  0.000089  0.000089  0.000089   \n",
       "\n",
       "       px_7      px_8      px_9  ...   px_1591   px_1592   px_1593   px_1594  \\\n",
       "0 -0.000970 -0.000721 -0.000018  ...  0.000089  0.000089  0.000089  0.000089   \n",
       "1  0.000089  0.000089  0.000089  ...  0.000089  0.000089  0.000089  0.000089   \n",
       "2  0.000089  0.000089  0.000089  ...  0.000089  0.000089  0.000089  0.000089   \n",
       "3  0.000089  0.000089  0.000089  ...  0.000089  0.000089  0.000089  0.000089   \n",
       "4  0.000089  0.000089  0.000089  ...  1.712617  2.498391  1.441851  0.665086   \n",
       "\n",
       "    px_1595   px_1596   px_1597   px_1598   px_1599   px_1600  \n",
       "0  0.000089  0.000089  0.000089  0.000089  0.000089  0.000089  \n",
       "1  0.000089  0.000089  0.000089  0.000089  0.000089  0.000089  \n",
       "2  0.000089  0.000089  0.000089  0.000089  0.000089  0.000089  \n",
       "3  0.000089  0.000089  0.000089  0.000089  0.000089  0.000089  \n",
       "4  0.458185  1.311074  3.439888  3.354876  1.726757  0.960530  \n",
       "\n",
       "[5 rows x 1601 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('../D_WEATHER/sub/unet_ch9_shuffle_swa.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sub = sub.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1600/1600 [00:01<00:00, 1308.10it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm.tqdm(range(1,1601)):\n",
    "    new_sub.loc[new_sub[new_sub.columns[i]]<0, new_sub.columns[i]] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>px_1</th>\n",
       "      <th>px_2</th>\n",
       "      <th>px_3</th>\n",
       "      <th>px_4</th>\n",
       "      <th>px_5</th>\n",
       "      <th>px_6</th>\n",
       "      <th>px_7</th>\n",
       "      <th>px_8</th>\n",
       "      <th>px_9</th>\n",
       "      <th>px_10</th>\n",
       "      <th>...</th>\n",
       "      <th>px_1591</th>\n",
       "      <th>px_1592</th>\n",
       "      <th>px_1593</th>\n",
       "      <th>px_1594</th>\n",
       "      <th>px_1595</th>\n",
       "      <th>px_1596</th>\n",
       "      <th>px_1597</th>\n",
       "      <th>px_1598</th>\n",
       "      <th>px_1599</th>\n",
       "      <th>px_1600</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.071400</td>\n",
       "      <td>0.090231</td>\n",
       "      <td>0.093307</td>\n",
       "      <td>0.095865</td>\n",
       "      <td>0.107072</td>\n",
       "      <td>0.111689</td>\n",
       "      <td>0.108108</td>\n",
       "      <td>0.109842</td>\n",
       "      <td>0.103929</td>\n",
       "      <td>0.098552</td>\n",
       "      <td>...</td>\n",
       "      <td>0.128523</td>\n",
       "      <td>0.114735</td>\n",
       "      <td>0.122111</td>\n",
       "      <td>0.113278</td>\n",
       "      <td>0.100124</td>\n",
       "      <td>0.094830</td>\n",
       "      <td>0.097382</td>\n",
       "      <td>0.094948</td>\n",
       "      <td>0.094106</td>\n",
       "      <td>0.074604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.452158</td>\n",
       "      <td>0.548384</td>\n",
       "      <td>0.510658</td>\n",
       "      <td>0.574311</td>\n",
       "      <td>0.698199</td>\n",
       "      <td>0.763710</td>\n",
       "      <td>0.722834</td>\n",
       "      <td>0.796999</td>\n",
       "      <td>0.771164</td>\n",
       "      <td>0.779447</td>\n",
       "      <td>...</td>\n",
       "      <td>0.900962</td>\n",
       "      <td>0.755377</td>\n",
       "      <td>0.877043</td>\n",
       "      <td>0.933304</td>\n",
       "      <td>0.850779</td>\n",
       "      <td>0.678522</td>\n",
       "      <td>0.621669</td>\n",
       "      <td>0.587681</td>\n",
       "      <td>0.759864</td>\n",
       "      <td>0.611493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.000666</td>\n",
       "      <td>-0.000533</td>\n",
       "      <td>-0.001202</td>\n",
       "      <td>-0.000779</td>\n",
       "      <td>-0.000747</td>\n",
       "      <td>-0.001128</td>\n",
       "      <td>-0.001154</td>\n",
       "      <td>-0.000738</td>\n",
       "      <td>-0.000918</td>\n",
       "      <td>-0.001023</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000081</td>\n",
       "      <td>-0.000135</td>\n",
       "      <td>-0.000106</td>\n",
       "      <td>-0.000179</td>\n",
       "      <td>-0.000240</td>\n",
       "      <td>-0.000171</td>\n",
       "      <td>-0.000167</td>\n",
       "      <td>-0.000076</td>\n",
       "      <td>-0.000286</td>\n",
       "      <td>-0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.713820</td>\n",
       "      <td>10.008502</td>\n",
       "      <td>8.380287</td>\n",
       "      <td>10.922000</td>\n",
       "      <td>19.736729</td>\n",
       "      <td>24.683893</td>\n",
       "      <td>19.461266</td>\n",
       "      <td>25.841715</td>\n",
       "      <td>26.699366</td>\n",
       "      <td>21.176273</td>\n",
       "      <td>...</td>\n",
       "      <td>28.599171</td>\n",
       "      <td>24.566702</td>\n",
       "      <td>23.116318</td>\n",
       "      <td>31.893583</td>\n",
       "      <td>31.201900</td>\n",
       "      <td>17.390079</td>\n",
       "      <td>16.938623</td>\n",
       "      <td>14.239760</td>\n",
       "      <td>24.658699</td>\n",
       "      <td>18.735142</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 1600 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              px_1         px_2         px_3         px_4         px_5  \\\n",
       "count  2416.000000  2416.000000  2416.000000  2416.000000  2416.000000   \n",
       "mean      0.071400     0.090231     0.093307     0.095865     0.107072   \n",
       "std       0.452158     0.548384     0.510658     0.574311     0.698199   \n",
       "min      -0.000666    -0.000533    -0.001202    -0.000779    -0.000747   \n",
       "25%       0.000089     0.000089     0.000089     0.000089     0.000089   \n",
       "50%       0.000089     0.000089     0.000089     0.000089     0.000089   \n",
       "75%       0.000089     0.000089     0.000089     0.000089     0.000089   \n",
       "max       7.713820    10.008502     8.380287    10.922000    19.736729   \n",
       "\n",
       "              px_6         px_7         px_8         px_9        px_10  ...  \\\n",
       "count  2416.000000  2416.000000  2416.000000  2416.000000  2416.000000  ...   \n",
       "mean      0.111689     0.108108     0.109842     0.103929     0.098552  ...   \n",
       "std       0.763710     0.722834     0.796999     0.771164     0.779447  ...   \n",
       "min      -0.001128    -0.001154    -0.000738    -0.000918    -0.001023  ...   \n",
       "25%       0.000089     0.000089     0.000089     0.000089     0.000089  ...   \n",
       "50%       0.000089     0.000089     0.000089     0.000089     0.000089  ...   \n",
       "75%       0.000089     0.000089     0.000089     0.000089     0.000089  ...   \n",
       "max      24.683893    19.461266    25.841715    26.699366    21.176273  ...   \n",
       "\n",
       "           px_1591      px_1592      px_1593      px_1594      px_1595  \\\n",
       "count  2416.000000  2416.000000  2416.000000  2416.000000  2416.000000   \n",
       "mean      0.128523     0.114735     0.122111     0.113278     0.100124   \n",
       "std       0.900962     0.755377     0.877043     0.933304     0.850779   \n",
       "min      -0.000081    -0.000135    -0.000106    -0.000179    -0.000240   \n",
       "25%       0.000089     0.000089     0.000089     0.000089     0.000089   \n",
       "50%       0.000089     0.000089     0.000089     0.000089     0.000089   \n",
       "75%       0.000089     0.000089     0.000089     0.000089     0.000089   \n",
       "max      28.599171    24.566702    23.116318    31.893583    31.201900   \n",
       "\n",
       "           px_1596      px_1597      px_1598      px_1599      px_1600  \n",
       "count  2416.000000  2416.000000  2416.000000  2416.000000  2416.000000  \n",
       "mean      0.094830     0.097382     0.094948     0.094106     0.074604  \n",
       "std       0.678522     0.621669     0.587681     0.759864     0.611493  \n",
       "min      -0.000171    -0.000167    -0.000076    -0.000286    -0.000298  \n",
       "25%       0.000089     0.000089     0.000089     0.000089     0.000089  \n",
       "50%       0.000089     0.000089     0.000089     0.000089     0.000089  \n",
       "75%       0.000089     0.000089     0.000089     0.000089     0.000089  \n",
       "max      17.390079    16.938623    14.239760    24.658699    18.735142  \n",
       "\n",
       "[8 rows x 1600 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>px_1</th>\n",
       "      <th>px_2</th>\n",
       "      <th>px_3</th>\n",
       "      <th>px_4</th>\n",
       "      <th>px_5</th>\n",
       "      <th>px_6</th>\n",
       "      <th>px_7</th>\n",
       "      <th>px_8</th>\n",
       "      <th>px_9</th>\n",
       "      <th>px_10</th>\n",
       "      <th>...</th>\n",
       "      <th>px_1591</th>\n",
       "      <th>px_1592</th>\n",
       "      <th>px_1593</th>\n",
       "      <th>px_1594</th>\n",
       "      <th>px_1595</th>\n",
       "      <th>px_1596</th>\n",
       "      <th>px_1597</th>\n",
       "      <th>px_1598</th>\n",
       "      <th>px_1599</th>\n",
       "      <th>px_1600</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.071401</td>\n",
       "      <td>0.090232</td>\n",
       "      <td>0.093309</td>\n",
       "      <td>0.095867</td>\n",
       "      <td>0.107074</td>\n",
       "      <td>0.111691</td>\n",
       "      <td>0.108111</td>\n",
       "      <td>0.109844</td>\n",
       "      <td>0.103932</td>\n",
       "      <td>0.098556</td>\n",
       "      <td>...</td>\n",
       "      <td>0.128523</td>\n",
       "      <td>0.114735</td>\n",
       "      <td>0.122111</td>\n",
       "      <td>0.113279</td>\n",
       "      <td>0.100125</td>\n",
       "      <td>0.094830</td>\n",
       "      <td>0.097382</td>\n",
       "      <td>0.094948</td>\n",
       "      <td>0.094106</td>\n",
       "      <td>0.074605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.452158</td>\n",
       "      <td>0.548384</td>\n",
       "      <td>0.510657</td>\n",
       "      <td>0.574311</td>\n",
       "      <td>0.698198</td>\n",
       "      <td>0.763710</td>\n",
       "      <td>0.722834</td>\n",
       "      <td>0.796999</td>\n",
       "      <td>0.771163</td>\n",
       "      <td>0.779447</td>\n",
       "      <td>...</td>\n",
       "      <td>0.900962</td>\n",
       "      <td>0.755377</td>\n",
       "      <td>0.877043</td>\n",
       "      <td>0.933304</td>\n",
       "      <td>0.850779</td>\n",
       "      <td>0.678522</td>\n",
       "      <td>0.621669</td>\n",
       "      <td>0.587681</td>\n",
       "      <td>0.759864</td>\n",
       "      <td>0.611493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.713820</td>\n",
       "      <td>10.008502</td>\n",
       "      <td>8.380287</td>\n",
       "      <td>10.922000</td>\n",
       "      <td>19.736729</td>\n",
       "      <td>24.683893</td>\n",
       "      <td>19.461266</td>\n",
       "      <td>25.841715</td>\n",
       "      <td>26.699366</td>\n",
       "      <td>21.176273</td>\n",
       "      <td>...</td>\n",
       "      <td>28.599171</td>\n",
       "      <td>24.566702</td>\n",
       "      <td>23.116318</td>\n",
       "      <td>31.893583</td>\n",
       "      <td>31.201900</td>\n",
       "      <td>17.390079</td>\n",
       "      <td>16.938623</td>\n",
       "      <td>14.239760</td>\n",
       "      <td>24.658699</td>\n",
       "      <td>18.735142</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 1600 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              px_1         px_2         px_3         px_4         px_5  \\\n",
       "count  2416.000000  2416.000000  2416.000000  2416.000000  2416.000000   \n",
       "mean      0.071401     0.090232     0.093309     0.095867     0.107074   \n",
       "std       0.452158     0.548384     0.510657     0.574311     0.698198   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000089     0.000089     0.000089     0.000089     0.000089   \n",
       "50%       0.000089     0.000089     0.000089     0.000089     0.000089   \n",
       "75%       0.000089     0.000089     0.000089     0.000089     0.000089   \n",
       "max       7.713820    10.008502     8.380287    10.922000    19.736729   \n",
       "\n",
       "              px_6         px_7         px_8         px_9        px_10  ...  \\\n",
       "count  2416.000000  2416.000000  2416.000000  2416.000000  2416.000000  ...   \n",
       "mean      0.111691     0.108111     0.109844     0.103932     0.098556  ...   \n",
       "std       0.763710     0.722834     0.796999     0.771163     0.779447  ...   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "25%       0.000089     0.000089     0.000089     0.000089     0.000089  ...   \n",
       "50%       0.000089     0.000089     0.000089     0.000089     0.000089  ...   \n",
       "75%       0.000089     0.000089     0.000089     0.000089     0.000089  ...   \n",
       "max      24.683893    19.461266    25.841715    26.699366    21.176273  ...   \n",
       "\n",
       "           px_1591      px_1592      px_1593      px_1594      px_1595  \\\n",
       "count  2416.000000  2416.000000  2416.000000  2416.000000  2416.000000   \n",
       "mean      0.128523     0.114735     0.122111     0.113279     0.100125   \n",
       "std       0.900962     0.755377     0.877043     0.933304     0.850779   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000089     0.000089     0.000089     0.000089     0.000089   \n",
       "50%       0.000089     0.000089     0.000089     0.000089     0.000089   \n",
       "75%       0.000089     0.000089     0.000089     0.000089     0.000089   \n",
       "max      28.599171    24.566702    23.116318    31.893583    31.201900   \n",
       "\n",
       "           px_1596      px_1597      px_1598      px_1599      px_1600  \n",
       "count  2416.000000  2416.000000  2416.000000  2416.000000  2416.000000  \n",
       "mean      0.094830     0.097382     0.094948     0.094106     0.074605  \n",
       "std       0.678522     0.621669     0.587681     0.759864     0.611493  \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "25%       0.000089     0.000089     0.000089     0.000089     0.000089  \n",
       "50%       0.000089     0.000089     0.000089     0.000089     0.000089  \n",
       "75%       0.000089     0.000089     0.000089     0.000089     0.000089  \n",
       "max      17.390079    16.938623    14.239760    24.658699    18.735142  \n",
       "\n",
       "[8 rows x 1600 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_sub.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>px_1</th>\n",
       "      <th>px_2</th>\n",
       "      <th>px_3</th>\n",
       "      <th>px_4</th>\n",
       "      <th>px_5</th>\n",
       "      <th>px_6</th>\n",
       "      <th>px_7</th>\n",
       "      <th>px_8</th>\n",
       "      <th>px_9</th>\n",
       "      <th>...</th>\n",
       "      <th>px_1591</th>\n",
       "      <th>px_1592</th>\n",
       "      <th>px_1593</th>\n",
       "      <th>px_1594</th>\n",
       "      <th>px_1595</th>\n",
       "      <th>px_1596</th>\n",
       "      <th>px_1597</th>\n",
       "      <th>px_1598</th>\n",
       "      <th>px_1599</th>\n",
       "      <th>px_1600</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>029858_01</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>029858_02</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>029858_03</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.022229</td>\n",
       "      <td>0.170700</td>\n",
       "      <td>0.000540</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>029858_05</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>029858_07</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>...</td>\n",
       "      <td>1.712617</td>\n",
       "      <td>2.498391</td>\n",
       "      <td>1.441851</td>\n",
       "      <td>0.665086</td>\n",
       "      <td>0.458185</td>\n",
       "      <td>1.311074</td>\n",
       "      <td>3.439888</td>\n",
       "      <td>3.354876</td>\n",
       "      <td>1.726757</td>\n",
       "      <td>0.960530</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1601 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id      px_1      px_2      px_3      px_4      px_5      px_6  \\\n",
       "0  029858_01  0.000089  0.000089  0.000091  0.000089  0.000000  0.000000   \n",
       "1  029858_02  0.000089  0.000089  0.000089  0.000089  0.000089  0.000089   \n",
       "2  029858_03  0.000089  0.000089  0.000110  0.022229  0.170700  0.000540   \n",
       "3  029858_05  0.000089  0.000089  0.000089  0.000089  0.000089  0.000089   \n",
       "4  029858_07  0.000089  0.000089  0.000089  0.000089  0.000089  0.000089   \n",
       "\n",
       "       px_7      px_8      px_9  ...   px_1591   px_1592   px_1593   px_1594  \\\n",
       "0  0.000000  0.000000  0.000000  ...  0.000089  0.000089  0.000089  0.000089   \n",
       "1  0.000089  0.000089  0.000089  ...  0.000089  0.000089  0.000089  0.000089   \n",
       "2  0.000089  0.000089  0.000089  ...  0.000089  0.000089  0.000089  0.000089   \n",
       "3  0.000089  0.000089  0.000089  ...  0.000089  0.000089  0.000089  0.000089   \n",
       "4  0.000089  0.000089  0.000089  ...  1.712617  2.498391  1.441851  0.665086   \n",
       "\n",
       "    px_1595   px_1596   px_1597   px_1598   px_1599   px_1600  \n",
       "0  0.000089  0.000089  0.000089  0.000089  0.000089  0.000089  \n",
       "1  0.000089  0.000089  0.000089  0.000089  0.000089  0.000089  \n",
       "2  0.000089  0.000089  0.000089  0.000089  0.000089  0.000089  \n",
       "3  0.000089  0.000089  0.000089  0.000089  0.000089  0.000089  \n",
       "4  0.458185  1.311074  3.439888  3.354876  1.726757  0.960530  \n",
       "\n",
       "[5 rows x 1601 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sub.to_csv('../D_WEATHER/sub/unet_ch9_shuffle_swa_postpro.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit ('pytorch': conda)",
   "language": "python",
   "name": "python37564bitpytorchconda133dde54c45c40c2946593d30b593426"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
