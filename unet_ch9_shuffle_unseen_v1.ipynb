{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import tqdm\n",
    "import argparse\n",
    "import tqdm\n",
    "from collections import defaultdict, Counter\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import nn, cuda\n",
    "from torch.autograd import Variable \n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import CenterCrop\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR\n",
    "\n",
    "# from efficientnet_pytorch import EfficientNet\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim import Optimizer\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def mae(y_true, y_pred) :\n",
    "    y_true, y_pred = np.array(y_true.detach().numpy()), np.array(y_pred.detach().numpy())\n",
    "    y_true = y_true.reshape(1, -1)[0]\n",
    "    y_pred = y_pred.reshape(1, -1)[0]\n",
    "    over_threshold = y_true >= 0.1\n",
    "    return np.mean(np.abs(y_true[over_threshold] - y_pred[over_threshold]))\n",
    "\n",
    "def fscore(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true.detach().numpy()), np.array(y_pred.detach().numpy())\n",
    "    y_true = y_true.reshape(1, -1)[0]\n",
    "    y_pred = y_pred.reshape(1, -1)[0]\n",
    "    remove_NAs = y_true >= 0\n",
    "    y_true = np.where(y_true[remove_NAs] >= 0.1, 1, 0)\n",
    "    y_pred = np.where(y_pred[remove_NAs] >= 0.1, 1, 0)\n",
    "    return(f1_score(y_true, y_pred))\n",
    "\n",
    "def maeOverFscore(y_true, y_pred):\n",
    "    return mae(y_true, y_pred) / (fscore(y_true, y_pred) + 1e-07)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **File info**\n",
    "**ex. subset_010462_01**\n",
    "> **orbit 010462**\n",
    "\n",
    "> **subset 01**\n",
    "\n",
    "> **ortbit 별로 subset 개수는 다를 수 있고 연속적이지 않을 수도 있음**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>orbit</th>\n",
       "      <th>orbit_subset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../D_WEATHER//input/train/subset_010462_01.npy</td>\n",
       "      <td>10462</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../D_WEATHER//input/train/subset_010462_02.npy</td>\n",
       "      <td>10462</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../D_WEATHER//input/train/subset_010462_03.npy</td>\n",
       "      <td>10462</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../D_WEATHER//input/train/subset_010462_04.npy</td>\n",
       "      <td>10462</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../D_WEATHER//input/train/subset_010462_05.npy</td>\n",
       "      <td>10462</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             path  orbit  orbit_subset\n",
       "0  ../D_WEATHER//input/train/subset_010462_01.npy  10462             1\n",
       "1  ../D_WEATHER//input/train/subset_010462_02.npy  10462             2\n",
       "2  ../D_WEATHER//input/train/subset_010462_03.npy  10462             3\n",
       "3  ../D_WEATHER//input/train/subset_010462_04.npy  10462             4\n",
       "4  ../D_WEATHER//input/train/subset_010462_05.npy  10462             5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_df = pd.read_csv(\"../D_WEATHER//input/train_df.csv\")\n",
    "te_df = pd.read_csv(\"../D_WEATHER/input/test_df.csv\")\n",
    "tr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((73113, 3), (3232, 3))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = tr_df['orbit'].value_counts()\n",
    "unseen = list(ids[ids<4].index)\n",
    "\n",
    "train_df = tr_df[~tr_df['orbit'].isin(unseen)]\n",
    "valid_df = tr_df[tr_df['orbit'].isin(unseen)]\n",
    "\n",
    "# train_df = tr_df[:int(len(tr_df)*0.8)]\n",
    "# valid_df = tr_df[int(len(tr_df)*0.8):]\n",
    "\n",
    "train_df.shape, valid_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Weather_Dataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        \n",
    "        self.image_list = []\n",
    "        self.label_list = []\n",
    "\n",
    "        for file in self.df['path']:\n",
    "            data = np.load(file)\n",
    "            image = data[:,:,:9] # use 14 channels except target\n",
    "            image = np.transpose(image, (2,0,1))\n",
    "            image = image.astype(np.float32)\n",
    "            self.image_list.append(image)\n",
    "            \n",
    "            label = data[:,:,-1].reshape(40,40,1)\n",
    "            label = np.transpose(label, (2,0,1))\n",
    "            self.label_list.append(label)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        image = self.image_list[idx]\n",
    "        label = self.label_list[idx]\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def worker_init(worker_id):\n",
    "#     np.random.seed(SEED)\n",
    "\n",
    "def build_dataloader(df, batch_size, shuffle=False):\n",
    "    dataset = Weather_Dataset(df)\n",
    "    dataloader = DataLoader(\n",
    "                            dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=shuffle,\n",
    "                            num_workers=0,\n",
    "#                             worker_init_fn=worker_init\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "def build_te_dataloader(df, batch_size, shuffle=False):\n",
    "    dataset = Test_Dataset(df)\n",
    "    dataloader = DataLoader(\n",
    "                            dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=shuffle,\n",
    "                            num_workers=0,\n",
    "#                             worker_init_fn=worker_init\n",
    "                            )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=True):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels # \n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = Down(512, 1024 // factor)\n",
    "        self.up1 = Up(1024, 512, bilinear)\n",
    "        self.up2 = Up(512, 256, bilinear)\n",
    "        self.up3 = Up(256, 128, bilinear)\n",
    "        self.up4 = Up(128, 64 * factor, bilinear)\n",
    "        self.outc = OutConv(64, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels // 2, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels , in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        diffY = torch.tensor([x2.size()[2] - x1.size()[2]])\n",
    "        diffX = torch.tensor([x2.size()[3] - x1.size()[3]])\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        # if you have padding issues, see\n",
    "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
    "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "batch_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = build_dataloader(train_df, batch_size, shuffle=True)\n",
    "valid_loader = build_dataloader(valid_df, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enable gpu use\n",
      "start training\n",
      "================ ༼ つ ◕_◕ ༽つ BEST epoch : 1, MOF : 3.610022018068707 \n",
      "E 1/200 tr_loss: 45.12921 tr_mae: 1.87091 tr_fs: 0.51683 tr_mof: 4.41580 val_loss: 30.29279 val_mae: 1.45559 val_fs: 0.43190 val_mof: 3.61002 lr: 0.001000 elapsed: 107\n",
      "================ ༼ つ ◕_◕ ༽つ BEST epoch : 2, MOF : 3.248134874044071 \n",
      "E 2/200 tr_loss: 45.05886 tr_mae: 1.64468 tr_fs: 0.64756 tr_mof: 2.54328 val_loss: 30.29034 val_mae: 1.40138 val_fs: 0.44739 val_mof: 3.24813 lr: 0.001000 elapsed: 107\n",
      "E 3/200 tr_loss: 45.12081 tr_mae: 1.55962 tr_fs: 0.67502 tr_mof: 2.31370 val_loss: 30.32665 val_mae: 1.55309 val_fs: 0.33596 val_mof: 4.93530 lr: 0.001000 elapsed: 107\n",
      "================ ༼ つ ◕_◕ ༽つ BEST epoch : 4, MOF : 2.5791409687768128 \n",
      "E 4/200 tr_loss: 45.18470 tr_mae: 1.50610 tr_fs: 0.69420 tr_mof: 2.17106 val_loss: 30.28037 val_mae: 1.31290 val_fs: 0.53114 val_mof: 2.57914 lr: 0.001000 elapsed: 107\n",
      "E 5/200 tr_loss: 45.14270 tr_mae: 1.48812 tr_fs: 0.69576 tr_mof: 2.14053 val_loss: 32.05016 val_mae: 2.25307 val_fs: 0.15553 val_mof: 16.20737 lr: 0.001000 elapsed: 107\n",
      "E 6/200 tr_loss: 45.07951 tr_mae: 1.48402 tr_fs: 0.69521 tr_mof: 2.13691 val_loss: 30.28964 val_mae: 1.44256 val_fs: 0.44078 val_mof: 3.47702 lr: 0.001000 elapsed: 107\n",
      "================ ༼ つ ◕_◕ ༽つ BEST epoch : 7, MOF : 2.1629873512209588 \n",
      "E 7/200 tr_loss: 45.04306 tr_mae: 1.46384 tr_fs: 0.70496 tr_mof: 2.07732 val_loss: 30.27983 val_mae: 1.27473 val_fs: 0.60111 val_mof: 2.16299 lr: 0.001000 elapsed: 107\n",
      "E 8/200 tr_loss: 45.15801 tr_mae: 1.45904 tr_fs: 0.70307 tr_mof: 2.07712 val_loss: 30.29528 val_mae: 1.41516 val_fs: 0.38156 val_mof: 3.89473 lr: 0.001000 elapsed: 107\n",
      "E 9/200 tr_loss: 45.12885 tr_mae: 1.43019 tr_fs: 0.71554 tr_mof: 1.99913 val_loss: 30.28857 val_mae: 1.48111 val_fs: 0.37210 val_mof: 4.31257 lr: 0.001000 elapsed: 107\n",
      "================ ༼ つ ◕_◕ ༽つ BEST epoch : 10, MOF : 1.901233923427785 \n",
      "E 10/200 tr_loss: 45.07501 tr_mae: 1.42092 tr_fs: 0.72082 tr_mof: 1.97264 val_loss: 30.28398 val_mae: 1.19222 val_fs: 0.63380 val_mof: 1.90123 lr: 0.001000 elapsed: 107\n",
      "E 11/200 tr_loss: 45.07544 tr_mae: 1.43114 tr_fs: 0.71501 tr_mof: 2.00297 val_loss: 30.52660 val_mae: 1.35058 val_fs: 0.11484 val_mof: 14.31086 lr: 0.001000 elapsed: 107\n",
      "E 12/200 tr_loss: 45.09640 tr_mae: 1.42032 tr_fs: 0.72012 tr_mof: 1.97345 val_loss: 30.71985 val_mae: 1.32007 val_fs: 0.18577 val_mof: 7.42870 lr: 0.001000 elapsed: 107\n",
      "E 13/200 tr_loss: 45.07414 tr_mae: 1.42071 tr_fs: 0.71991 tr_mof: 1.97361 val_loss: 30.29155 val_mae: 1.15387 val_fs: 0.54238 val_mof: 2.22243 lr: 0.001000 elapsed: 107\n",
      "E 14/200 tr_loss: 45.13525 tr_mae: 1.42733 tr_fs: 0.71634 tr_mof: 1.99564 val_loss: 30.42325 val_mae: 1.35943 val_fs: 0.20868 val_mof: 7.33839 lr: 0.001000 elapsed: 106\n",
      "E 15/200 tr_loss: 45.16532 tr_mae: 1.45328 tr_fs: 0.70041 tr_mof: 2.08288 val_loss: 30.28463 val_mae: 1.42326 val_fs: 0.39766 val_mof: 3.79732 lr: 0.001000 elapsed: 106\n",
      "E 16/200 tr_loss: 45.11962 tr_mae: 1.40352 tr_fs: 0.72671 tr_mof: 1.93201 val_loss: 30.31061 val_mae: 1.40258 val_fs: 0.53115 val_mof: 2.69150 lr: 0.001000 elapsed: 106\n",
      "================ ༼ つ ◕_◕ ༽つ BEST epoch : 17, MOF : 1.7299831393467264 \n",
      "E 17/200 tr_loss: 45.04587 tr_mae: 1.37912 tr_fs: 0.73893 tr_mof: 1.86711 val_loss: 30.27291 val_mae: 1.12543 val_fs: 0.65980 val_mof: 1.72998 lr: 0.000500 elapsed: 106\n",
      "================ ༼ つ ◕_◕ ༽つ BEST epoch : 18, MOF : 1.6135772704354097 \n",
      "E 18/200 tr_loss: 45.21894 tr_mae: 1.36718 tr_fs: 0.74477 tr_mof: 1.83639 val_loss: 30.27077 val_mae: 1.10457 val_fs: 0.69072 val_mof: 1.61358 lr: 0.000500 elapsed: 106\n",
      "E 19/200 tr_loss: 45.06045 tr_mae: 1.36580 tr_fs: 0.74383 tr_mof: 1.83682 val_loss: 30.32852 val_mae: 1.10722 val_fs: 0.41389 val_mof: 2.85264 lr: 0.000500 elapsed: 107\n",
      "E 20/200 tr_loss: 45.17259 tr_mae: 1.35946 tr_fs: 0.74714 tr_mof: 1.81983 val_loss: 30.28253 val_mae: 1.26595 val_fs: 0.64555 val_mof: 1.98396 lr: 0.000500 elapsed: 106\n",
      "E 21/200 tr_loss: 45.09845 tr_mae: 1.36850 tr_fs: 0.74188 tr_mof: 1.84529 val_loss: 30.27633 val_mae: 1.28886 val_fs: 0.49222 val_mof: 2.73472 lr: 0.000500 elapsed: 106\n",
      "E 22/200 tr_loss: 45.04484 tr_mae: 1.36044 tr_fs: 0.74481 tr_mof: 1.82694 val_loss: 30.27086 val_mae: 1.17119 val_fs: 0.64800 val_mof: 1.83440 lr: 0.000500 elapsed: 106\n",
      "E 23/200 tr_loss: 45.08208 tr_mae: 1.35643 tr_fs: 0.74672 tr_mof: 1.81707 val_loss: 30.28324 val_mae: 1.15654 val_fs: 0.50242 val_mof: 2.49099 lr: 0.000500 elapsed: 106\n",
      "E 24/200 tr_loss: 45.03444 tr_mae: 1.35159 tr_fs: 0.74996 tr_mof: 1.80245 val_loss: 30.27291 val_mae: 1.19053 val_fs: 0.64715 val_mof: 1.85924 lr: 0.000500 elapsed: 106\n",
      "E 25/200 tr_loss: 45.10218 tr_mae: 1.33923 tr_fs: 0.75495 tr_mof: 1.77419 val_loss: 30.27443 val_mae: 1.21189 val_fs: 0.62814 val_mof: 1.97378 lr: 0.000250 elapsed: 106\n",
      "E 26/200 tr_loss: 45.13154 tr_mae: 1.33600 tr_fs: 0.75553 tr_mof: 1.76806 val_loss: 30.27559 val_mae: 1.26776 val_fs: 0.58571 val_mof: 2.19816 lr: 0.000250 elapsed: 106\n",
      "E 27/200 tr_loss: 45.17042 tr_mae: 1.32979 tr_fs: 0.75694 tr_mof: 1.75673 val_loss: 30.27440 val_mae: 1.24611 val_fs: 0.61872 val_mof: 2.03875 lr: 0.000250 elapsed: 106\n",
      "E 28/200 tr_loss: 45.13612 tr_mae: 1.33210 tr_fs: 0.75620 tr_mof: 1.76168 val_loss: 30.27347 val_mae: 1.23130 val_fs: 0.60480 val_mof: 2.06910 lr: 0.000250 elapsed: 106\n",
      "E 29/200 tr_loss: 45.10151 tr_mae: 1.32958 tr_fs: 0.75727 tr_mof: 1.75572 val_loss: 30.26998 val_mae: 1.14244 val_fs: 0.69590 val_mof: 1.65349 lr: 0.000250 elapsed: 106\n",
      "E 30/200 tr_loss: 45.06422 tr_mae: 1.32602 tr_fs: 0.75842 tr_mof: 1.74892 val_loss: 30.27254 val_mae: 1.08518 val_fs: 0.64142 val_mof: 1.73271 lr: 0.000250 elapsed: 106\n",
      "E 31/200 tr_loss: 45.17047 tr_mae: 1.32583 tr_fs: 0.75874 tr_mof: 1.74740 val_loss: 30.27237 val_mae: 1.21326 val_fs: 0.63817 val_mof: 1.92522 lr: 0.000125 elapsed: 106\n",
      "================ ༼ つ ◕_◕ ༽つ BEST epoch : 32, MOF : 1.5868934267417225 \n",
      "E 32/200 tr_loss: 45.05788 tr_mae: 1.31996 tr_fs: 0.76214 tr_mof: 1.73217 val_loss: 30.26837 val_mae: 1.09853 val_fs: 0.69799 val_mof: 1.58689 lr: 0.000125 elapsed: 106\n",
      "E 33/200 tr_loss: 45.16944 tr_mae: 1.31604 tr_fs: 0.76189 tr_mof: 1.72690 val_loss: 30.28804 val_mae: 1.28447 val_fs: 0.65544 val_mof: 1.98583 lr: 0.000125 elapsed: 106\n",
      "E 34/200 tr_loss: 45.03155 tr_mae: 1.31355 tr_fs: 0.76340 tr_mof: 1.72048 val_loss: 30.26816 val_mae: 1.14179 val_fs: 0.65921 val_mof: 1.74685 lr: 0.000125 elapsed: 106\n",
      "================ ༼ つ ◕_◕ ༽つ BEST epoch : 35, MOF : 1.5608975461709582 \n",
      "E 35/200 tr_loss: 45.06585 tr_mae: 1.31216 tr_fs: 0.76368 tr_mof: 1.71860 val_loss: 30.26923 val_mae: 1.08861 val_fs: 0.70253 val_mof: 1.56090 lr: 0.000125 elapsed: 106\n",
      "E 36/200 tr_loss: 45.10038 tr_mae: 1.31451 tr_fs: 0.76257 tr_mof: 1.72405 val_loss: 30.27491 val_mae: 1.04733 val_fs: 0.63752 val_mof: 1.66189 lr: 0.000125 elapsed: 106\n",
      "E 37/200 tr_loss: 45.08379 tr_mae: 1.31033 tr_fs: 0.76455 tr_mof: 1.71401 val_loss: 30.27106 val_mae: 1.12563 val_fs: 0.69602 val_mof: 1.62505 lr: 0.000125 elapsed: 106\n",
      "================ ༼ つ ◕_◕ ༽つ BEST epoch : 38, MOF : 1.504783645262587 \n",
      "E 38/200 tr_loss: 45.03133 tr_mae: 1.30998 tr_fs: 0.76515 tr_mof: 1.71201 val_loss: 30.26715 val_mae: 1.07829 val_fs: 0.71878 val_mof: 1.50478 lr: 0.000125 elapsed: 106\n",
      "E 39/200 tr_loss: 45.08578 tr_mae: 1.30854 tr_fs: 0.76475 tr_mof: 1.71131 val_loss: 30.26672 val_mae: 1.07751 val_fs: 0.71637 val_mof: 1.51065 lr: 0.000125 elapsed: 106\n",
      "E 40/200 tr_loss: 45.18681 tr_mae: 1.30693 tr_fs: 0.76633 tr_mof: 1.70497 val_loss: 30.26752 val_mae: 1.11941 val_fs: 0.68724 val_mof: 1.63865 lr: 0.000125 elapsed: 106\n",
      "================ ༼ つ ◕_◕ ༽つ BEST epoch : 41, MOF : 1.4790607018475659 \n",
      "E 41/200 tr_loss: 45.06519 tr_mae: 1.30320 tr_fs: 0.76691 tr_mof: 1.69932 val_loss: 30.26722 val_mae: 1.06375 val_fs: 0.72130 val_mof: 1.47906 lr: 0.000125 elapsed: 106\n",
      "E 42/200 tr_loss: 45.06534 tr_mae: 1.30469 tr_fs: 0.76664 tr_mof: 1.70171 val_loss: 30.26657 val_mae: 1.08247 val_fs: 0.72032 val_mof: 1.50858 lr: 0.000125 elapsed: 106\n",
      "E 43/200 tr_loss: 45.07620 tr_mae: 1.30275 tr_fs: 0.76848 tr_mof: 1.69532 val_loss: 30.26806 val_mae: 1.11291 val_fs: 0.70797 val_mof: 1.58067 lr: 0.000125 elapsed: 106\n",
      "E 44/200 tr_loss: 45.03057 tr_mae: 1.30045 tr_fs: 0.76810 tr_mof: 1.69318 val_loss: 30.26625 val_mae: 1.08680 val_fs: 0.70830 val_mof: 1.54391 lr: 0.000125 elapsed: 106\n",
      "================ ༼ つ ◕_◕ ༽つ BEST epoch : 45, MOF : 1.4774092716792808 \n",
      "E 45/200 tr_loss: 45.03077 tr_mae: 1.30277 tr_fs: 0.76779 tr_mof: 1.69658 val_loss: 30.26728 val_mae: 1.05348 val_fs: 0.71471 val_mof: 1.47741 lr: 0.000125 elapsed: 106\n",
      "E 46/200 tr_loss: 45.13909 tr_mae: 1.30056 tr_fs: 0.76911 tr_mof: 1.69108 val_loss: 30.29428 val_mae: 1.17705 val_fs: 0.56332 val_mof: 2.13844 lr: 0.000125 elapsed: 106\n",
      "E 47/200 tr_loss: 45.05570 tr_mae: 1.30017 tr_fs: 0.76800 tr_mof: 1.69282 val_loss: 30.27598 val_mae: 1.09503 val_fs: 0.64526 val_mof: 1.72718 lr: 0.000125 elapsed: 106\n",
      "E 48/200 tr_loss: 45.11801 tr_mae: 1.29624 tr_fs: 0.76911 tr_mof: 1.68530 val_loss: 30.31683 val_mae: 1.43565 val_fs: 0.56455 val_mof: 2.61935 lr: 0.000125 elapsed: 106\n",
      "E 49/200 tr_loss: 45.03037 tr_mae: 1.29681 tr_fs: 0.76951 tr_mof: 1.68510 val_loss: 30.27210 val_mae: 1.05043 val_fs: 0.65075 val_mof: 1.64671 lr: 0.000125 elapsed: 106\n",
      "E 50/200 tr_loss: 45.06446 tr_mae: 1.29177 tr_fs: 0.77086 tr_mof: 1.67574 val_loss: 30.27087 val_mae: 1.13196 val_fs: 0.69916 val_mof: 1.63186 lr: 0.000125 elapsed: 106\n",
      "E 51/200 tr_loss: 45.18475 tr_mae: 1.29282 tr_fs: 0.77014 tr_mof: 1.67867 val_loss: 30.35648 val_mae: 1.49766 val_fs: 0.46717 val_mof: 3.41219 lr: 0.000125 elapsed: 106\n",
      "================ ༼ つ ◕_◕ ༽つ BEST epoch : 52, MOF : 1.445906074133114 \n",
      "E 52/200 tr_loss: 45.19106 tr_mae: 1.28597 tr_fs: 0.77257 tr_mof: 1.66434 val_loss: 30.26671 val_mae: 1.04793 val_fs: 0.72642 val_mof: 1.44591 lr: 0.000063 elapsed: 106\n",
      "E 53/200 tr_loss: 45.10559 tr_mae: 1.28452 tr_fs: 0.77388 tr_mof: 1.65952 val_loss: 30.26928 val_mae: 1.03294 val_fs: 0.67734 val_mof: 1.54538 lr: 0.000063 elapsed: 106\n",
      "E 54/200 tr_loss: 45.08866 tr_mae: 1.28448 tr_fs: 0.77349 tr_mof: 1.66038 val_loss: 30.26896 val_mae: 1.15526 val_fs: 0.64745 val_mof: 1.80157 lr: 0.000063 elapsed: 106\n",
      "E 55/200 tr_loss: 45.06389 tr_mae: 1.28698 tr_fs: 0.77379 tr_mof: 1.66327 val_loss: 30.26840 val_mae: 1.11034 val_fs: 0.72222 val_mof: 1.54390 lr: 0.000063 elapsed: 106\n",
      "E 56/200 tr_loss: 45.16715 tr_mae: 1.28535 tr_fs: 0.77381 tr_mof: 1.66110 val_loss: 30.26609 val_mae: 1.04652 val_fs: 0.71779 val_mof: 1.46735 lr: 0.000063 elapsed: 106\n",
      "E 57/200 tr_loss: 45.12672 tr_mae: 1.28226 tr_fs: 0.77461 tr_mof: 1.65528 val_loss: 30.26679 val_mae: 1.09421 val_fs: 0.71263 val_mof: 1.54169 lr: 0.000063 elapsed: 106\n",
      "E 58/200 tr_loss: 45.22178 tr_mae: 1.28185 tr_fs: 0.77481 tr_mof: 1.65425 val_loss: 30.28537 val_mae: 1.10093 val_fs: 0.58724 val_mof: 1.93906 lr: 0.000063 elapsed: 106\n",
      "E 59/200 tr_loss: 45.02897 tr_mae: 1.27670 tr_fs: 0.77621 tr_mof: 1.64456 val_loss: 30.26479 val_mae: 1.05439 val_fs: 0.72241 val_mof: 1.46612 lr: 0.000031 elapsed: 106\n",
      "================ ༼ つ ◕_◕ ༽つ BEST epoch : 60, MOF : 1.4090320663519367 \n",
      "E 60/200 tr_loss: 45.12651 tr_mae: 1.27585 tr_fs: 0.77667 tr_mof: 1.64258 val_loss: 30.26456 val_mae: 1.03445 val_fs: 0.73582 val_mof: 1.40903 lr: 0.000031 elapsed: 106\n",
      "E 61/200 tr_loss: 45.06329 tr_mae: 1.27660 tr_fs: 0.77650 tr_mof: 1.64411 val_loss: 30.26489 val_mae: 1.05530 val_fs: 0.72190 val_mof: 1.46945 lr: 0.000031 elapsed: 106\n",
      "E 62/200 tr_loss: 45.10507 tr_mae: 1.27744 tr_fs: 0.77624 tr_mof: 1.64542 val_loss: 30.26427 val_mae: 1.06077 val_fs: 0.72368 val_mof: 1.47125 lr: 0.000031 elapsed: 106\n",
      "E 63/200 tr_loss: 45.04135 tr_mae: 1.27524 tr_fs: 0.77687 tr_mof: 1.64118 val_loss: 30.26487 val_mae: 1.04874 val_fs: 0.73018 val_mof: 1.44169 lr: 0.000031 elapsed: 106\n",
      "E 64/200 tr_loss: 45.09755 tr_mae: 1.27528 tr_fs: 0.77698 tr_mof: 1.64115 val_loss: 30.26413 val_mae: 1.05095 val_fs: 0.73151 val_mof: 1.44118 lr: 0.000031 elapsed: 106\n",
      "E 65/200 tr_loss: 45.06313 tr_mae: 1.27395 tr_fs: 0.77717 tr_mof: 1.63892 val_loss: 30.26482 val_mae: 1.03850 val_fs: 0.72985 val_mof: 1.42605 lr: 0.000031 elapsed: 106\n",
      "E 66/200 tr_loss: 45.05738 tr_mae: 1.27567 tr_fs: 0.77702 tr_mof: 1.64167 val_loss: 30.27075 val_mae: 1.13102 val_fs: 0.71879 val_mof: 1.58128 lr: 0.000031 elapsed: 106\n",
      "E 67/200 tr_loss: 45.14619 tr_mae: 1.27165 tr_fs: 0.77770 tr_mof: 1.63481 val_loss: 30.26517 val_mae: 1.04799 val_fs: 0.72236 val_mof: 1.45854 lr: 0.000016 elapsed: 106\n",
      "E 68/200 tr_loss: 45.16009 tr_mae: 1.27157 tr_fs: 0.77826 tr_mof: 1.63350 val_loss: 30.26482 val_mae: 1.04015 val_fs: 0.72263 val_mof: 1.44680 lr: 0.000016 elapsed: 106\n",
      "E 69/200 tr_loss: 45.11381 tr_mae: 1.27152 tr_fs: 0.77785 tr_mof: 1.63467 val_loss: 30.26423 val_mae: 1.03934 val_fs: 0.73351 val_mof: 1.42154 lr: 0.000016 elapsed: 107\n",
      "E 70/200 tr_loss: 45.10021 tr_mae: 1.27246 tr_fs: 0.77843 tr_mof: 1.63457 val_loss: 30.26475 val_mae: 1.05887 val_fs: 0.72796 val_mof: 1.45914 lr: 0.000016 elapsed: 106\n",
      "E 71/200 tr_loss: 45.06290 tr_mae: 1.26995 tr_fs: 0.77842 tr_mof: 1.63109 val_loss: 30.26399 val_mae: 1.04361 val_fs: 0.73378 val_mof: 1.42628 lr: 0.000016 elapsed: 106\n",
      "E 72/200 tr_loss: 45.14153 tr_mae: 1.27167 tr_fs: 0.77811 tr_mof: 1.63422 val_loss: 30.26517 val_mae: 1.03960 val_fs: 0.72566 val_mof: 1.43976 lr: 0.000016 elapsed: 106\n",
      "E 73/200 tr_loss: 45.09716 tr_mae: 1.27016 tr_fs: 0.77844 tr_mof: 1.63176 val_loss: 30.26381 val_mae: 1.04616 val_fs: 0.73069 val_mof: 1.43631 lr: 0.000008 elapsed: 106\n",
      "E 74/200 tr_loss: 45.09709 tr_mae: 1.26904 tr_fs: 0.77897 tr_mof: 1.62898 val_loss: 30.26420 val_mae: 1.05681 val_fs: 0.72757 val_mof: 1.45734 lr: 0.000008 elapsed: 106\n",
      "E 75/200 tr_loss: 45.05693 tr_mae: 1.26915 tr_fs: 0.77897 tr_mof: 1.62913 val_loss: 30.26396 val_mae: 1.03940 val_fs: 0.73340 val_mof: 1.42200 lr: 0.000008 elapsed: 106\n",
      "E 76/200 tr_loss: 45.09706 tr_mae: 1.26793 tr_fs: 0.77898 tr_mof: 1.62722 val_loss: 30.26409 val_mae: 1.04315 val_fs: 0.72970 val_mof: 1.43509 lr: 0.000008 elapsed: 106\n",
      "E 77/200 tr_loss: 45.16288 tr_mae: 1.26799 tr_fs: 0.77862 tr_mof: 1.62846 val_loss: 30.26404 val_mae: 1.03942 val_fs: 0.73375 val_mof: 1.42157 lr: 0.000008 elapsed: 106\n",
      "E 78/200 tr_loss: 45.04255 tr_mae: 1.26818 tr_fs: 0.77928 tr_mof: 1.62731 val_loss: 30.26374 val_mae: 1.04504 val_fs: 0.73193 val_mof: 1.43238 lr: 0.000008 elapsed: 106\n",
      "E 79/200 tr_loss: 45.12464 tr_mae: 1.26928 tr_fs: 0.77921 tr_mof: 1.62895 val_loss: 30.26393 val_mae: 1.04469 val_fs: 0.73285 val_mof: 1.43034 lr: 0.000004 elapsed: 106\n",
      "E 80/200 tr_loss: 45.02822 tr_mae: 1.26658 tr_fs: 0.77919 tr_mof: 1.62549 val_loss: 30.26401 val_mae: 1.03940 val_fs: 0.73417 val_mof: 1.42041 lr: 0.000004 elapsed: 107\n",
      "E 81/200 tr_loss: 45.09703 tr_mae: 1.26810 tr_fs: 0.77931 tr_mof: 1.62705 val_loss: 30.26372 val_mae: 1.04400 val_fs: 0.73234 val_mof: 1.43008 lr: 0.000004 elapsed: 106\n",
      "E 82/200 tr_loss: 45.06271 tr_mae: 1.26797 tr_fs: 0.77930 tr_mof: 1.62671 val_loss: 30.26381 val_mae: 1.04419 val_fs: 0.73131 val_mof: 1.43272 lr: 0.000004 elapsed: 106\n",
      "E 83/200 tr_loss: 45.06264 tr_mae: 1.26851 tr_fs: 0.77971 tr_mof: 1.62688 val_loss: 30.26386 val_mae: 1.03698 val_fs: 0.73460 val_mof: 1.41614 lr: 0.000004 elapsed: 106\n",
      "E 84/200 tr_loss: 45.06261 tr_mae: 1.26671 tr_fs: 0.77896 tr_mof: 1.62586 val_loss: 30.26372 val_mae: 1.04092 val_fs: 0.73508 val_mof: 1.42032 lr: 0.000004 elapsed: 106\n",
      "E 85/200 tr_loss: 45.09946 tr_mae: 1.26717 tr_fs: 0.77911 tr_mof: 1.62630 val_loss: 30.26371 val_mae: 1.04073 val_fs: 0.73406 val_mof: 1.42226 lr: 0.000002 elapsed: 106\n",
      "E 86/200 tr_loss: 45.05047 tr_mae: 1.26763 tr_fs: 0.77956 tr_mof: 1.62610 val_loss: 30.26373 val_mae: 1.04021 val_fs: 0.73410 val_mof: 1.42147 lr: 0.000002 elapsed: 106\n",
      "E 87/200 tr_loss: 45.09817 tr_mae: 1.26615 tr_fs: 0.77951 tr_mof: 1.62420 val_loss: 30.26373 val_mae: 1.04129 val_fs: 0.73347 val_mof: 1.42426 lr: 0.000002 elapsed: 106\n",
      "E 88/200 tr_loss: 45.19741 tr_mae: 1.26778 tr_fs: 0.77955 tr_mof: 1.62611 val_loss: 30.26365 val_mae: 1.04340 val_fs: 0.73293 val_mof: 1.42785 lr: 0.000002 elapsed: 106\n",
      "E 89/200 tr_loss: 45.09698 tr_mae: 1.26650 tr_fs: 0.77948 tr_mof: 1.62462 val_loss: 30.26373 val_mae: 1.04608 val_fs: 0.73065 val_mof: 1.43663 lr: 0.000002 elapsed: 106\n",
      "E 90/200 tr_loss: 45.11756 tr_mae: 1.26710 tr_fs: 0.77931 tr_mof: 1.62558 val_loss: 30.26375 val_mae: 1.03647 val_fs: 0.73665 val_mof: 1.41105 lr: 0.000002 elapsed: 106\n",
      "E 91/200 tr_loss: 45.02814 tr_mae: 1.26768 tr_fs: 0.77909 tr_mof: 1.62724 val_loss: 30.26376 val_mae: 1.04238 val_fs: 0.73213 val_mof: 1.42847 lr: 0.000001 elapsed: 106\n",
      "E 92/200 tr_loss: 45.19088 tr_mae: 1.26631 tr_fs: 0.77991 tr_mof: 1.62361 val_loss: 30.26375 val_mae: 1.04008 val_fs: 0.73456 val_mof: 1.42037 lr: 0.000001 elapsed: 106\n",
      "E 93/200 tr_loss: 45.13322 tr_mae: 1.26699 tr_fs: 0.77974 tr_mof: 1.62485 val_loss: 30.26370 val_mae: 1.04412 val_fs: 0.73272 val_mof: 1.42956 lr: 0.000001 elapsed: 106\n",
      "E 94/200 tr_loss: 45.06259 tr_mae: 1.26654 tr_fs: 0.77970 tr_mof: 1.62410 val_loss: 30.26371 val_mae: 1.04201 val_fs: 0.73349 val_mof: 1.42502 lr: 0.000001 elapsed: 106\n",
      "E 95/200 tr_loss: 45.09693 tr_mae: 1.26705 tr_fs: 0.78001 tr_mof: 1.62425 val_loss: 30.26369 val_mae: 1.04337 val_fs: 0.73249 val_mof: 1.42896 lr: 0.000001 elapsed: 106\n",
      "E 96/200 tr_loss: 45.15725 tr_mae: 1.26710 tr_fs: 0.77957 tr_mof: 1.62545 val_loss: 30.26379 val_mae: 1.04338 val_fs: 0.73317 val_mof: 1.42769 lr: 0.000001 elapsed: 107\n",
      "E 97/200 tr_loss: 45.09693 tr_mae: 1.26646 tr_fs: 0.77911 tr_mof: 1.62547 val_loss: 30.26374 val_mae: 1.04251 val_fs: 0.73375 val_mof: 1.42531 lr: 0.000000 elapsed: 106\n",
      "E 98/200 tr_loss: 45.06258 tr_mae: 1.26647 tr_fs: 0.77946 tr_mof: 1.62469 val_loss: 30.26371 val_mae: 1.04261 val_fs: 0.73258 val_mof: 1.42785 lr: 0.000000 elapsed: 106\n",
      "E 99/200 tr_loss: 45.13071 tr_mae: 1.26600 tr_fs: 0.77984 tr_mof: 1.62352 val_loss: 30.26371 val_mae: 1.04204 val_fs: 0.73340 val_mof: 1.42529 lr: 0.000000 elapsed: 106\n",
      "E 100/200 tr_loss: 45.06254 tr_mae: 1.26635 tr_fs: 0.77961 tr_mof: 1.62423 val_loss: 30.26377 val_mae: 1.04181 val_fs: 0.73283 val_mof: 1.42628 lr: 0.000000 elapsed: 106\n",
      "E 101/200 tr_loss: 45.02816 tr_mae: 1.26767 tr_fs: 0.77951 tr_mof: 1.62616 val_loss: 30.26371 val_mae: 1.03867 val_fs: 0.73562 val_mof: 1.41618 lr: 0.000000 elapsed: 106\n",
      "E 102/200 tr_loss: 45.09355 tr_mae: 1.26619 tr_fs: 0.77969 tr_mof: 1.62397 val_loss: 30.26375 val_mae: 1.03938 val_fs: 0.73537 val_mof: 1.41768 lr: 0.000000 elapsed: 106\n",
      "E 103/200 tr_loss: 45.09697 tr_mae: 1.26682 tr_fs: 0.77980 tr_mof: 1.62451 val_loss: 30.26364 val_mae: 1.04355 val_fs: 0.73276 val_mof: 1.42859 lr: 0.000000 elapsed: 106\n",
      "E 104/200 tr_loss: 45.10634 tr_mae: 1.26715 tr_fs: 0.77972 tr_mof: 1.62513 val_loss: 30.26375 val_mae: 1.04196 val_fs: 0.73287 val_mof: 1.42642 lr: 0.000000 elapsed: 106\n",
      "E 105/200 tr_loss: 45.09688 tr_mae: 1.26618 tr_fs: 0.77990 tr_mof: 1.62346 val_loss: 30.26366 val_mae: 1.04240 val_fs: 0.73342 val_mof: 1.42566 lr: 0.000000 elapsed: 106\n",
      "E 106/200 tr_loss: 45.06255 tr_mae: 1.26646 tr_fs: 0.77956 tr_mof: 1.62434 val_loss: 30.26369 val_mae: 1.04369 val_fs: 0.73197 val_mof: 1.43060 lr: 0.000000 elapsed: 106\n",
      "E 107/200 tr_loss: 45.04525 tr_mae: 1.26630 tr_fs: 0.77991 tr_mof: 1.62350 val_loss: 30.26368 val_mae: 1.04186 val_fs: 0.73372 val_mof: 1.42448 lr: 0.000000 elapsed: 106\n",
      "E 108/200 tr_loss: 45.06013 tr_mae: 1.26640 tr_fs: 0.77978 tr_mof: 1.62408 val_loss: 30.26373 val_mae: 1.04117 val_fs: 0.73365 val_mof: 1.42365 lr: 0.000000 elapsed: 106\n",
      "E 109/200 tr_loss: 45.20049 tr_mae: 1.26463 tr_fs: 0.77971 tr_mof: 1.62180 val_loss: 30.26365 val_mae: 1.04253 val_fs: 0.73307 val_mof: 1.42672 lr: 0.000000 elapsed: 107\n",
      "E 110/200 tr_loss: 45.13132 tr_mae: 1.26696 tr_fs: 0.77955 tr_mof: 1.62504 val_loss: 30.26370 val_mae: 1.04137 val_fs: 0.73393 val_mof: 1.42332 lr: 0.000000 elapsed: 106\n",
      "E 111/200 tr_loss: 45.09692 tr_mae: 1.26574 tr_fs: 0.77960 tr_mof: 1.62352 val_loss: 30.26376 val_mae: 1.04278 val_fs: 0.73258 val_mof: 1.42808 lr: 0.000000 elapsed: 106\n",
      "E 112/200 tr_loss: 45.02813 tr_mae: 1.26643 tr_fs: 0.77964 tr_mof: 1.62446 val_loss: 30.26367 val_mae: 1.04188 val_fs: 0.73363 val_mof: 1.42464 lr: 0.000000 elapsed: 107\n",
      "E 113/200 tr_loss: 45.09693 tr_mae: 1.26623 tr_fs: 0.77946 tr_mof: 1.62423 val_loss: 30.26376 val_mae: 1.04112 val_fs: 0.73275 val_mof: 1.42547 lr: 0.000000 elapsed: 106\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-49eca5a5a601>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mmae_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mf_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0mmof_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaeOverFscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-65d04cbf7b3d>\u001b[0m in \u001b[0;36mfscore\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mremove_NAs\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mremove_NAs\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmaeOverFscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mf1_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1097\u001b[0m                        \u001b[0mpos_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpos_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m                        \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m                        zero_division=zero_division)\n\u001b[0m\u001b[1;32m   1100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mfbeta_score\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1224\u001b[0m                                                  \u001b[0mwarn_for\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'f-score'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m                                                  \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1226\u001b[0;31m                                                  zero_division=zero_division)\n\u001b[0m\u001b[1;32m   1227\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1488\u001b[0m     MCM = multilabel_confusion_matrix(y_true, y_pred,\n\u001b[1;32m   1489\u001b[0m                                       \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1490\u001b[0;31m                                       labels=labels, samplewise=samplewise)\n\u001b[0m\u001b[1;32m   1491\u001b[0m     \u001b[0mtp_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMCM\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1492\u001b[0m     \u001b[0mpred_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtp_sum\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mMCM\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mmultilabel_confusion_matrix\u001b[0;34m(y_true, y_pred, sample_weight, labels, samplewise)\u001b[0m\n\u001b[1;32m    446\u001b[0m         \u001b[0mle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m         \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m         \u001b[0msorted_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/sklearn/preprocessing/_label.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    271\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muniques\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/sklearn/preprocessing/_label.py\u001b[0m in \u001b[0;36m_encode\u001b[0;34m(values, uniques, encode, check_unknown)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         return _encode_numpy(values, uniques, encode,\n\u001b[0;32m--> 118\u001b[0;31m                              check_unknown=check_unknown)\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/sklearn/preprocessing/_label.py\u001b[0m in \u001b[0;36m_encode_numpy\u001b[0;34m(values, uniques, encode, check_unknown)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck_unknown\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_encode_check_unknown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdiff\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                 raise ValueError(\"y contains previously unseen labels: %s\"\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/sklearn/preprocessing/_label.py\u001b[0m in \u001b[0;36m_encode_check_unknown\u001b[0;34m(values, uniques, return_mask)\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdiff\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m         \u001b[0munique_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m         \u001b[0mdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdiff1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munique_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muniques\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0massume_unique\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_mask\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36munique\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/numpy/lib/arraysetops.py\u001b[0m in \u001b[0;36munique\u001b[0;34m(ar, return_index, return_inverse, return_counts, axis)\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0mar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unique1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_counts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_unpack_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/numpy/lib/arraysetops.py\u001b[0m in \u001b[0;36m_unique1d\u001b[0;34m(ar, return_index, return_inverse, return_counts)\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0maux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mar\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mperm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m         \u001b[0mar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m         \u001b[0maux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maux\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "seed_everything(SEED)\n",
    "\n",
    "device = 'cuda:0'\n",
    "use_gpu = cuda.is_available()\n",
    "if use_gpu:\n",
    "    print(\"enable gpu use\")\n",
    "else:\n",
    "    print(\"enable cpu for debugging\")\n",
    "\n",
    "model = UNet(n_channels=9, n_classes=1, bilinear=False) # if bilinear = True -> non deterministic : not recommended\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr, weight_decay=0.00025)\n",
    "# optimizer = AdamW(model.parameters(), 2.5e-4, weight_decay=0.000025)\n",
    "#optimizer = optim.SGD(model.parameters(), args.lr, momentum=0.9, weight_decay=0.025)\n",
    "\n",
    "###### SCHEDULER #######\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n",
    "\n",
    "#eta_min = 0.00001\n",
    "#T_max = 10\n",
    "#T_mult = 1\n",
    "#restart_decay = 0.97\n",
    "#scheduler = CosineAnnealingWithRestartsLR(optimizer, T_max=T_max, eta_min=eta_min, T_mult=T_mult, restart_decay=restart_decay)\n",
    "\n",
    "#scheduler = StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss() \n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "def to_numpy(t):\n",
    "    return t.cpu().detach().numpy()\n",
    "\n",
    "best_mae_score = 999\n",
    "best_f_score = 999\n",
    "best_mof_score = 999\n",
    "grad_clip_step = 100\n",
    "grad_clip = 100\n",
    "step = 0\n",
    "# accumulation_step = 2\n",
    "EPOCH = 200\n",
    "\n",
    "model_fname = '../D_WEATHER/weight/unet_ch9_shuffle_unseen_v1.pt'\n",
    "# log file\n",
    "log_df = pd.DataFrame(columns=['epoch_idx', 'train_loss', 'train_mae', 'train_fs', 'train_mof', 'valid_loss', 'valid_mae', 'valid_fs', 'valid_mof'])\n",
    "\n",
    "print(\"start training\")\n",
    "\n",
    "for epoch_idx in range(1, EPOCH + 1):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss = 0\n",
    "    train_mae = 0\n",
    "    train_fs = 0\n",
    "    train_mof = 0 \n",
    "#     train_total_correct = 0\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for batch_idx, (image, labels) in enumerate(train_loader):\n",
    "        if use_gpu:\n",
    "            image = image.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "        output = model(image)\n",
    "        loss = criterion(output, labels)\n",
    "        mae_score = mae(labels.cpu(), output.cpu())\n",
    "        f_score = fscore(labels.cpu(), output.cpu())\n",
    "        mof_score = maeOverFscore(labels.cpu(), output.cpu())\n",
    "\n",
    "        # gradient explosion prevention\n",
    "        if step > grad_clip_step:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "\n",
    "        step += 1\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        train_loss += loss.item() / len(train_loader)\n",
    "        train_mae += mae_score.item() / len(train_loader)\n",
    "        train_fs += f_score.item() / len(train_loader)\n",
    "        train_mof += mof_score.item() / len(train_loader)\n",
    "\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    valid_mae = 0\n",
    "    valid_fs = 0\n",
    "    valid_mof = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (image, labels) in enumerate(valid_loader):\n",
    "            if use_gpu:\n",
    "                image = image.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "            output = model(image)\n",
    "            loss = criterion(output, labels)\n",
    "            mae_score = mae(labels.cpu(), output.cpu())\n",
    "            f_score = fscore(labels.cpu(), output.cpu())\n",
    "            mof_score = maeOverFscore(labels.cpu(), output.cpu())\n",
    "\n",
    "#             output_prob = F.sigmoid(output)\n",
    "\n",
    "            predict_vector = to_numpy(output)\n",
    "\n",
    "            valid_loss += loss.item() / len(valid_loader)\n",
    "            valid_mae += mae_score.item() / len(valid_loader)\n",
    "            valid_fs += f_score.item() / len(valid_loader)\n",
    "            valid_mof += mof_score.item() / len(valid_loader)\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "    # checkpoint\n",
    "    if valid_mof < best_mof_score:\n",
    "        best_mof_score = valid_mof\n",
    "#         print(\"Improved !! \")\n",
    "        torch.save(model.state_dict(), model_fname)\n",
    "        print(\"================ ༼ つ ◕_◕ ༽つ BEST epoch : {}, MOF : {} \".format(epoch_idx, best_mof_score))\n",
    "        #file_save_name = 'best_acc' + '_' + str(num_fold)\n",
    "        #print(file_save_name)\n",
    "#     else:\n",
    "#         print(\"val acc has not improved\")\n",
    "\n",
    "    lr = [_['lr'] for _ in optimizer.param_groups]\n",
    "\n",
    "    #if args.scheduler == 'plateau':\n",
    "    scheduler.step(valid_mof)\n",
    "    #else:\n",
    "    #    scheduler.step()\n",
    "\n",
    "    # nsml.save(epoch_idx)\n",
    "\n",
    "    print(\"E {}/{} tr_loss: {:.5f} tr_mae: {:.5f} tr_fs: {:.5f} tr_mof: {:.5f} val_loss: {:.5f} val_mae: {:.5f} val_fs: {:.5f} val_mof: {:.5f} lr: {:.6f} elapsed: {:.0f}\".format(\n",
    "           epoch_idx, EPOCH, train_loss, train_mae, train_fs, train_mof, valid_loss, valid_mae, valid_fs, valid_mof, lr[0], elapsed))\n",
    "            #epoch_idx, args.epochs, train_loss, valid_loss, val_acc, lr[0], elapsed\n",
    "    # log file element\n",
    "#     log = []\n",
    "    log_data = [epoch_idx, train_loss, train_mae, train_fs, train_mof, valid_loss, valid_mae, valid_fs, valid_mof]\n",
    "#     log.append(log_data)\n",
    "    log_df.loc[epoch_idx] = log_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch_idx</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_mae</th>\n",
       "      <th>train_fs</th>\n",
       "      <th>train_mof</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>valid_mae</th>\n",
       "      <th>valid_fs</th>\n",
       "      <th>valid_mof</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>109.0</td>\n",
       "      <td>45.200485</td>\n",
       "      <td>1.264625</td>\n",
       "      <td>0.779711</td>\n",
       "      <td>1.621797</td>\n",
       "      <td>30.263653</td>\n",
       "      <td>1.042530</td>\n",
       "      <td>0.733071</td>\n",
       "      <td>1.426718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>110.0</td>\n",
       "      <td>45.131319</td>\n",
       "      <td>1.266962</td>\n",
       "      <td>0.779548</td>\n",
       "      <td>1.625037</td>\n",
       "      <td>30.263702</td>\n",
       "      <td>1.041367</td>\n",
       "      <td>0.733929</td>\n",
       "      <td>1.423325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>111.0</td>\n",
       "      <td>45.096920</td>\n",
       "      <td>1.265740</td>\n",
       "      <td>0.779596</td>\n",
       "      <td>1.623522</td>\n",
       "      <td>30.263761</td>\n",
       "      <td>1.042779</td>\n",
       "      <td>0.732582</td>\n",
       "      <td>1.428084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>112.0</td>\n",
       "      <td>45.028127</td>\n",
       "      <td>1.266425</td>\n",
       "      <td>0.779643</td>\n",
       "      <td>1.624460</td>\n",
       "      <td>30.263668</td>\n",
       "      <td>1.041878</td>\n",
       "      <td>0.733625</td>\n",
       "      <td>1.424640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>113.0</td>\n",
       "      <td>45.096929</td>\n",
       "      <td>1.266230</td>\n",
       "      <td>0.779458</td>\n",
       "      <td>1.624235</td>\n",
       "      <td>30.263756</td>\n",
       "      <td>1.041120</td>\n",
       "      <td>0.732753</td>\n",
       "      <td>1.425474</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     epoch_idx  train_loss  train_mae  train_fs  train_mof  valid_loss  \\\n",
       "109      109.0   45.200485   1.264625  0.779711   1.621797   30.263653   \n",
       "110      110.0   45.131319   1.266962  0.779548   1.625037   30.263702   \n",
       "111      111.0   45.096920   1.265740  0.779596   1.623522   30.263761   \n",
       "112      112.0   45.028127   1.266425  0.779643   1.624460   30.263668   \n",
       "113      113.0   45.096929   1.266230  0.779458   1.624235   30.263756   \n",
       "\n",
       "     valid_mae  valid_fs  valid_mof  \n",
       "109   1.042530  0.733071   1.426718  \n",
       "110   1.041367  0.733929   1.423325  \n",
       "111   1.042779  0.732582   1.428084  \n",
       "112   1.041878  0.733625   1.424640  \n",
       "113   1.041120  0.732753   1.425474  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df.to_csv(\"../D_WEATHER/log/unet_ch9_shuffle_unseen_v1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test_Dataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        \n",
    "        self.image_list = []\n",
    "#         self.label_list = []\n",
    "\n",
    "        for file in self.df['path']:\n",
    "            data = np.load(file)\n",
    "#             image = data[:,:,:]\n",
    "            image = data[:,:,:9]#.reshape(40,40,-1)\n",
    "            image = np.transpose(image, (2,0,1))\n",
    "            image = image.astype(np.float32)\n",
    "            self.image_list.append(image)\n",
    "            \n",
    "#             label = data[:,:,-1].reshape(-1)\n",
    "#             self.label_list.append(label)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        image = self.image_list[idx]\n",
    "#         label = self.label_list[idx]\n",
    "        \n",
    "        return image#, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = build_te_dataloader(te_df, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2416, 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loader.dataset.df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 40, 40)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 40, 40)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loader.dataset[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict values check :  [ 6.95499068e-04  4.37678816e-03  5.14530344e-03 ... -8.45150589e-06\n",
      " -8.45150589e-06 -8.45150589e-06]\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(model_fname))\n",
    "model.eval()\n",
    "predictions = np.zeros((len(test_loader.dataset), 1600))\n",
    "with torch.no_grad():\n",
    "    for i, image in enumerate(test_loader):\n",
    "        image = image.to(device)\n",
    "        output = model(image)\n",
    "        \n",
    "        predictions[i*batch_size: (i+1)*batch_size] = output.detach().cpu().numpy().reshape(-1, 1600)\n",
    "print(\"predict values check : \",predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2416, 1600)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.95499068e-04,  4.37678816e-03,  5.14530344e-03, ...,\n",
       "       -8.45150589e-06, -8.45150589e-06, -8.45150589e-06])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv(\"../D_WEATHER/input/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>px_1</th>\n",
       "      <th>px_2</th>\n",
       "      <th>px_3</th>\n",
       "      <th>px_4</th>\n",
       "      <th>px_5</th>\n",
       "      <th>px_6</th>\n",
       "      <th>px_7</th>\n",
       "      <th>px_8</th>\n",
       "      <th>px_9</th>\n",
       "      <th>...</th>\n",
       "      <th>px_1591</th>\n",
       "      <th>px_1592</th>\n",
       "      <th>px_1593</th>\n",
       "      <th>px_1594</th>\n",
       "      <th>px_1595</th>\n",
       "      <th>px_1596</th>\n",
       "      <th>px_1597</th>\n",
       "      <th>px_1598</th>\n",
       "      <th>px_1599</th>\n",
       "      <th>px_1600</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>029858_01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>029858_02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>029858_03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>029858_05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>029858_07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1601 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  px_1  px_2  px_3  px_4  px_5  px_6  px_7  px_8  px_9  ...  \\\n",
       "0  029858_01   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "1  029858_02   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "2  029858_03   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "3  029858_05   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "4  029858_07   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "\n",
       "   px_1591  px_1592  px_1593  px_1594  px_1595  px_1596  px_1597  px_1598  \\\n",
       "0      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "1      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "2      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "3      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "4      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "   px_1599  px_1600  \n",
       "0      0.0      0.0  \n",
       "1      0.0      0.0  \n",
       "2      0.0      0.0  \n",
       "3      0.0      0.0  \n",
       "4      0.0      0.0  \n",
       "\n",
       "[5 rows x 1601 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.iloc[:,1:] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>px_1</th>\n",
       "      <th>px_2</th>\n",
       "      <th>px_3</th>\n",
       "      <th>px_4</th>\n",
       "      <th>px_5</th>\n",
       "      <th>px_6</th>\n",
       "      <th>px_7</th>\n",
       "      <th>px_8</th>\n",
       "      <th>px_9</th>\n",
       "      <th>...</th>\n",
       "      <th>px_1591</th>\n",
       "      <th>px_1592</th>\n",
       "      <th>px_1593</th>\n",
       "      <th>px_1594</th>\n",
       "      <th>px_1595</th>\n",
       "      <th>px_1596</th>\n",
       "      <th>px_1597</th>\n",
       "      <th>px_1598</th>\n",
       "      <th>px_1599</th>\n",
       "      <th>px_1600</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>029858_01</td>\n",
       "      <td>0.000695</td>\n",
       "      <td>0.004377</td>\n",
       "      <td>0.005145</td>\n",
       "      <td>-0.000061</td>\n",
       "      <td>0.000293</td>\n",
       "      <td>0.001819</td>\n",
       "      <td>0.025605</td>\n",
       "      <td>0.023118</td>\n",
       "      <td>0.004287</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>0.000431</td>\n",
       "      <td>0.000636</td>\n",
       "      <td>0.000571</td>\n",
       "      <td>0.000727</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>029858_02</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>0.000490</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>029858_03</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000329</td>\n",
       "      <td>0.010504</td>\n",
       "      <td>0.159920</td>\n",
       "      <td>0.056207</td>\n",
       "      <td>-0.001732</td>\n",
       "      <td>-0.000076</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>029858_05</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>029858_07</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>...</td>\n",
       "      <td>1.718630</td>\n",
       "      <td>2.531649</td>\n",
       "      <td>1.663102</td>\n",
       "      <td>0.815817</td>\n",
       "      <td>0.862955</td>\n",
       "      <td>1.987553</td>\n",
       "      <td>5.634937</td>\n",
       "      <td>6.003900</td>\n",
       "      <td>2.731457</td>\n",
       "      <td>2.417581</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1601 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id      px_1      px_2      px_3      px_4      px_5      px_6  \\\n",
       "0  029858_01  0.000695  0.004377  0.005145 -0.000061  0.000293  0.001819   \n",
       "1  029858_02 -0.000008 -0.000008  0.000166  0.000490  0.000136 -0.000008   \n",
       "2  029858_03 -0.000008 -0.000329  0.010504  0.159920  0.056207 -0.001732   \n",
       "3  029858_05 -0.000008 -0.000008 -0.000008 -0.000008 -0.000008 -0.000008   \n",
       "4  029858_07 -0.000008 -0.000008 -0.000008 -0.000008 -0.000008 -0.000008   \n",
       "\n",
       "       px_7      px_8      px_9  ...   px_1591   px_1592   px_1593   px_1594  \\\n",
       "0  0.025605  0.023118  0.004287  ... -0.000008 -0.000008  0.000431  0.000636   \n",
       "1 -0.000008 -0.000008 -0.000008  ... -0.000008 -0.000008 -0.000008 -0.000008   \n",
       "2 -0.000076 -0.000008 -0.000008  ... -0.000008 -0.000008 -0.000008 -0.000008   \n",
       "3 -0.000008 -0.000008 -0.000008  ... -0.000008 -0.000008 -0.000008 -0.000008   \n",
       "4 -0.000008 -0.000008 -0.000008  ...  1.718630  2.531649  1.663102  0.815817   \n",
       "\n",
       "    px_1595   px_1596   px_1597   px_1598   px_1599   px_1600  \n",
       "0  0.000571  0.000727  0.000125 -0.000008 -0.000008 -0.000008  \n",
       "1 -0.000008 -0.000008 -0.000008 -0.000008 -0.000008 -0.000008  \n",
       "2 -0.000008 -0.000008 -0.000008 -0.000008 -0.000008 -0.000008  \n",
       "3 -0.000008 -0.000008 -0.000008 -0.000008 -0.000008 -0.000008  \n",
       "4  0.862955  1.987553  5.634937  6.003900  2.731457  2.417581  \n",
       "\n",
       "[5 rows x 1601 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('../D_WEATHER/sub/unet_ch9_shuffle_unseen_v1.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sub = sub.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1600/1600 [00:01<00:00, 1351.26it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm.tqdm(range(1,1601)):\n",
    "    new_sub.loc[new_sub[new_sub.columns[i]]<0, new_sub.columns[i]] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>px_1</th>\n",
       "      <th>px_2</th>\n",
       "      <th>px_3</th>\n",
       "      <th>px_4</th>\n",
       "      <th>px_5</th>\n",
       "      <th>px_6</th>\n",
       "      <th>px_7</th>\n",
       "      <th>px_8</th>\n",
       "      <th>px_9</th>\n",
       "      <th>px_10</th>\n",
       "      <th>...</th>\n",
       "      <th>px_1591</th>\n",
       "      <th>px_1592</th>\n",
       "      <th>px_1593</th>\n",
       "      <th>px_1594</th>\n",
       "      <th>px_1595</th>\n",
       "      <th>px_1596</th>\n",
       "      <th>px_1597</th>\n",
       "      <th>px_1598</th>\n",
       "      <th>px_1599</th>\n",
       "      <th>px_1600</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.086599</td>\n",
       "      <td>0.110633</td>\n",
       "      <td>0.111881</td>\n",
       "      <td>0.132918</td>\n",
       "      <td>0.150624</td>\n",
       "      <td>0.158935</td>\n",
       "      <td>0.151734</td>\n",
       "      <td>0.154366</td>\n",
       "      <td>0.145773</td>\n",
       "      <td>0.134241</td>\n",
       "      <td>...</td>\n",
       "      <td>0.134724</td>\n",
       "      <td>0.125238</td>\n",
       "      <td>0.125947</td>\n",
       "      <td>0.120677</td>\n",
       "      <td>0.113612</td>\n",
       "      <td>0.119149</td>\n",
       "      <td>0.121848</td>\n",
       "      <td>0.120337</td>\n",
       "      <td>0.116552</td>\n",
       "      <td>0.098733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.597793</td>\n",
       "      <td>0.716600</td>\n",
       "      <td>0.699595</td>\n",
       "      <td>0.825594</td>\n",
       "      <td>1.046199</td>\n",
       "      <td>1.189576</td>\n",
       "      <td>1.165583</td>\n",
       "      <td>1.180978</td>\n",
       "      <td>1.115131</td>\n",
       "      <td>1.077176</td>\n",
       "      <td>...</td>\n",
       "      <td>0.764317</td>\n",
       "      <td>0.653997</td>\n",
       "      <td>0.691180</td>\n",
       "      <td>0.766282</td>\n",
       "      <td>0.794808</td>\n",
       "      <td>0.835127</td>\n",
       "      <td>0.780684</td>\n",
       "      <td>0.728848</td>\n",
       "      <td>0.775481</td>\n",
       "      <td>0.604362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.026584</td>\n",
       "      <td>-0.011954</td>\n",
       "      <td>-0.042810</td>\n",
       "      <td>-0.077522</td>\n",
       "      <td>-0.135630</td>\n",
       "      <td>-0.165187</td>\n",
       "      <td>-0.191611</td>\n",
       "      <td>-0.145327</td>\n",
       "      <td>-0.172685</td>\n",
       "      <td>-0.194533</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018680</td>\n",
       "      <td>-0.019657</td>\n",
       "      <td>-0.020539</td>\n",
       "      <td>-0.010358</td>\n",
       "      <td>-0.033393</td>\n",
       "      <td>-0.014808</td>\n",
       "      <td>-0.013639</td>\n",
       "      <td>-0.021965</td>\n",
       "      <td>-0.016827</td>\n",
       "      <td>-0.010658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>12.445879</td>\n",
       "      <td>13.825877</td>\n",
       "      <td>14.452060</td>\n",
       "      <td>16.526499</td>\n",
       "      <td>30.715355</td>\n",
       "      <td>38.280350</td>\n",
       "      <td>31.871758</td>\n",
       "      <td>35.959183</td>\n",
       "      <td>37.322262</td>\n",
       "      <td>31.481152</td>\n",
       "      <td>...</td>\n",
       "      <td>17.993885</td>\n",
       "      <td>11.759321</td>\n",
       "      <td>13.366447</td>\n",
       "      <td>16.802965</td>\n",
       "      <td>20.870436</td>\n",
       "      <td>19.221441</td>\n",
       "      <td>20.625252</td>\n",
       "      <td>17.121483</td>\n",
       "      <td>18.810265</td>\n",
       "      <td>13.138242</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 1600 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              px_1         px_2         px_3         px_4         px_5  \\\n",
       "count  2416.000000  2416.000000  2416.000000  2416.000000  2416.000000   \n",
       "mean      0.086599     0.110633     0.111881     0.132918     0.150624   \n",
       "std       0.597793     0.716600     0.699595     0.825594     1.046199   \n",
       "min      -0.026584    -0.011954    -0.042810    -0.077522    -0.135630   \n",
       "25%      -0.000008    -0.000008    -0.000008    -0.000008    -0.000008   \n",
       "50%      -0.000008    -0.000008    -0.000008    -0.000008    -0.000008   \n",
       "75%      -0.000008    -0.000008    -0.000008    -0.000008    -0.000008   \n",
       "max      12.445879    13.825877    14.452060    16.526499    30.715355   \n",
       "\n",
       "              px_6         px_7         px_8         px_9        px_10  ...  \\\n",
       "count  2416.000000  2416.000000  2416.000000  2416.000000  2416.000000  ...   \n",
       "mean      0.158935     0.151734     0.154366     0.145773     0.134241  ...   \n",
       "std       1.189576     1.165583     1.180978     1.115131     1.077176  ...   \n",
       "min      -0.165187    -0.191611    -0.145327    -0.172685    -0.194533  ...   \n",
       "25%      -0.000008    -0.000008    -0.000008    -0.000008    -0.000008  ...   \n",
       "50%      -0.000008    -0.000008    -0.000008    -0.000008    -0.000008  ...   \n",
       "75%      -0.000008    -0.000008    -0.000008    -0.000008    -0.000008  ...   \n",
       "max      38.280350    31.871758    35.959183    37.322262    31.481152  ...   \n",
       "\n",
       "           px_1591      px_1592      px_1593      px_1594      px_1595  \\\n",
       "count  2416.000000  2416.000000  2416.000000  2416.000000  2416.000000   \n",
       "mean      0.134724     0.125238     0.125947     0.120677     0.113612   \n",
       "std       0.764317     0.653997     0.691180     0.766282     0.794808   \n",
       "min      -0.018680    -0.019657    -0.020539    -0.010358    -0.033393   \n",
       "25%      -0.000008    -0.000008    -0.000008    -0.000008    -0.000008   \n",
       "50%      -0.000008    -0.000008    -0.000008    -0.000008    -0.000008   \n",
       "75%      -0.000008    -0.000008    -0.000008    -0.000008    -0.000008   \n",
       "max      17.993885    11.759321    13.366447    16.802965    20.870436   \n",
       "\n",
       "           px_1596      px_1597      px_1598      px_1599      px_1600  \n",
       "count  2416.000000  2416.000000  2416.000000  2416.000000  2416.000000  \n",
       "mean      0.119149     0.121848     0.120337     0.116552     0.098733  \n",
       "std       0.835127     0.780684     0.728848     0.775481     0.604362  \n",
       "min      -0.014808    -0.013639    -0.021965    -0.016827    -0.010658  \n",
       "25%      -0.000008    -0.000008    -0.000008    -0.000008    -0.000008  \n",
       "50%      -0.000008    -0.000008    -0.000008    -0.000008    -0.000008  \n",
       "75%      -0.000008    -0.000008    -0.000008    -0.000008    -0.000008  \n",
       "max      19.221441    20.625252    17.121483    18.810265    13.138242  \n",
       "\n",
       "[8 rows x 1600 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>px_1</th>\n",
       "      <th>px_2</th>\n",
       "      <th>px_3</th>\n",
       "      <th>px_4</th>\n",
       "      <th>px_5</th>\n",
       "      <th>px_6</th>\n",
       "      <th>px_7</th>\n",
       "      <th>px_8</th>\n",
       "      <th>px_9</th>\n",
       "      <th>px_10</th>\n",
       "      <th>...</th>\n",
       "      <th>px_1591</th>\n",
       "      <th>px_1592</th>\n",
       "      <th>px_1593</th>\n",
       "      <th>px_1594</th>\n",
       "      <th>px_1595</th>\n",
       "      <th>px_1596</th>\n",
       "      <th>px_1597</th>\n",
       "      <th>px_1598</th>\n",
       "      <th>px_1599</th>\n",
       "      <th>px_1600</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.086669</td>\n",
       "      <td>0.110686</td>\n",
       "      <td>0.111978</td>\n",
       "      <td>0.133046</td>\n",
       "      <td>0.150787</td>\n",
       "      <td>0.159073</td>\n",
       "      <td>0.151891</td>\n",
       "      <td>0.154512</td>\n",
       "      <td>0.145905</td>\n",
       "      <td>0.134417</td>\n",
       "      <td>...</td>\n",
       "      <td>0.134779</td>\n",
       "      <td>0.125327</td>\n",
       "      <td>0.126021</td>\n",
       "      <td>0.120734</td>\n",
       "      <td>0.113719</td>\n",
       "      <td>0.119225</td>\n",
       "      <td>0.121947</td>\n",
       "      <td>0.120419</td>\n",
       "      <td>0.116623</td>\n",
       "      <td>0.098780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.597783</td>\n",
       "      <td>0.716591</td>\n",
       "      <td>0.699579</td>\n",
       "      <td>0.825572</td>\n",
       "      <td>1.046172</td>\n",
       "      <td>1.189552</td>\n",
       "      <td>1.165556</td>\n",
       "      <td>1.180955</td>\n",
       "      <td>1.115108</td>\n",
       "      <td>1.077147</td>\n",
       "      <td>...</td>\n",
       "      <td>0.764307</td>\n",
       "      <td>0.653979</td>\n",
       "      <td>0.691166</td>\n",
       "      <td>0.766273</td>\n",
       "      <td>0.794792</td>\n",
       "      <td>0.835116</td>\n",
       "      <td>0.780668</td>\n",
       "      <td>0.728834</td>\n",
       "      <td>0.775471</td>\n",
       "      <td>0.604354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>12.445879</td>\n",
       "      <td>13.825877</td>\n",
       "      <td>14.452060</td>\n",
       "      <td>16.526499</td>\n",
       "      <td>30.715355</td>\n",
       "      <td>38.280350</td>\n",
       "      <td>31.871758</td>\n",
       "      <td>35.959183</td>\n",
       "      <td>37.322262</td>\n",
       "      <td>31.481152</td>\n",
       "      <td>...</td>\n",
       "      <td>17.993885</td>\n",
       "      <td>11.759321</td>\n",
       "      <td>13.366447</td>\n",
       "      <td>16.802965</td>\n",
       "      <td>20.870436</td>\n",
       "      <td>19.221441</td>\n",
       "      <td>20.625252</td>\n",
       "      <td>17.121483</td>\n",
       "      <td>18.810265</td>\n",
       "      <td>13.138242</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 1600 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              px_1         px_2         px_3         px_4         px_5  \\\n",
       "count  2416.000000  2416.000000  2416.000000  2416.000000  2416.000000   \n",
       "mean      0.086669     0.110686     0.111978     0.133046     0.150787   \n",
       "std       0.597783     0.716591     0.699579     0.825572     1.046172   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "max      12.445879    13.825877    14.452060    16.526499    30.715355   \n",
       "\n",
       "              px_6         px_7         px_8         px_9        px_10  ...  \\\n",
       "count  2416.000000  2416.000000  2416.000000  2416.000000  2416.000000  ...   \n",
       "mean      0.159073     0.151891     0.154512     0.145905     0.134417  ...   \n",
       "std       1.189552     1.165556     1.180955     1.115108     1.077147  ...   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "max      38.280350    31.871758    35.959183    37.322262    31.481152  ...   \n",
       "\n",
       "           px_1591      px_1592      px_1593      px_1594      px_1595  \\\n",
       "count  2416.000000  2416.000000  2416.000000  2416.000000  2416.000000   \n",
       "mean      0.134779     0.125327     0.126021     0.120734     0.113719   \n",
       "std       0.764307     0.653979     0.691166     0.766273     0.794792   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "max      17.993885    11.759321    13.366447    16.802965    20.870436   \n",
       "\n",
       "           px_1596      px_1597      px_1598      px_1599      px_1600  \n",
       "count  2416.000000  2416.000000  2416.000000  2416.000000  2416.000000  \n",
       "mean      0.119225     0.121947     0.120419     0.116623     0.098780  \n",
       "std       0.835116     0.780668     0.728834     0.775471     0.604354  \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "max      19.221441    20.625252    17.121483    18.810265    13.138242  \n",
       "\n",
       "[8 rows x 1600 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_sub.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>px_1</th>\n",
       "      <th>px_2</th>\n",
       "      <th>px_3</th>\n",
       "      <th>px_4</th>\n",
       "      <th>px_5</th>\n",
       "      <th>px_6</th>\n",
       "      <th>px_7</th>\n",
       "      <th>px_8</th>\n",
       "      <th>px_9</th>\n",
       "      <th>...</th>\n",
       "      <th>px_1591</th>\n",
       "      <th>px_1592</th>\n",
       "      <th>px_1593</th>\n",
       "      <th>px_1594</th>\n",
       "      <th>px_1595</th>\n",
       "      <th>px_1596</th>\n",
       "      <th>px_1597</th>\n",
       "      <th>px_1598</th>\n",
       "      <th>px_1599</th>\n",
       "      <th>px_1600</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>029858_01</td>\n",
       "      <td>0.000695</td>\n",
       "      <td>0.004377</td>\n",
       "      <td>0.005145</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000293</td>\n",
       "      <td>0.001819</td>\n",
       "      <td>0.025605</td>\n",
       "      <td>0.023118</td>\n",
       "      <td>0.004287</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000431</td>\n",
       "      <td>0.000636</td>\n",
       "      <td>0.000571</td>\n",
       "      <td>0.000727</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>029858_02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>0.00049</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>029858_03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010504</td>\n",
       "      <td>0.15992</td>\n",
       "      <td>0.056207</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>029858_05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>029858_07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.71863</td>\n",
       "      <td>2.531649</td>\n",
       "      <td>1.663102</td>\n",
       "      <td>0.815817</td>\n",
       "      <td>0.862955</td>\n",
       "      <td>1.987553</td>\n",
       "      <td>5.634937</td>\n",
       "      <td>6.0039</td>\n",
       "      <td>2.731457</td>\n",
       "      <td>2.417581</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1601 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id      px_1      px_2      px_3     px_4      px_5      px_6  \\\n",
       "0  029858_01  0.000695  0.004377  0.005145  0.00000  0.000293  0.001819   \n",
       "1  029858_02  0.000000  0.000000  0.000166  0.00049  0.000136  0.000000   \n",
       "2  029858_03  0.000000  0.000000  0.010504  0.15992  0.056207  0.000000   \n",
       "3  029858_05  0.000000  0.000000  0.000000  0.00000  0.000000  0.000000   \n",
       "4  029858_07  0.000000  0.000000  0.000000  0.00000  0.000000  0.000000   \n",
       "\n",
       "       px_7      px_8      px_9  ...  px_1591   px_1592   px_1593   px_1594  \\\n",
       "0  0.025605  0.023118  0.004287  ...  0.00000  0.000000  0.000431  0.000636   \n",
       "1  0.000000  0.000000  0.000000  ...  0.00000  0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.000000  0.000000  ...  0.00000  0.000000  0.000000  0.000000   \n",
       "3  0.000000  0.000000  0.000000  ...  0.00000  0.000000  0.000000  0.000000   \n",
       "4  0.000000  0.000000  0.000000  ...  1.71863  2.531649  1.663102  0.815817   \n",
       "\n",
       "    px_1595   px_1596   px_1597  px_1598   px_1599   px_1600  \n",
       "0  0.000571  0.000727  0.000125   0.0000  0.000000  0.000000  \n",
       "1  0.000000  0.000000  0.000000   0.0000  0.000000  0.000000  \n",
       "2  0.000000  0.000000  0.000000   0.0000  0.000000  0.000000  \n",
       "3  0.000000  0.000000  0.000000   0.0000  0.000000  0.000000  \n",
       "4  0.862955  1.987553  5.634937   6.0039  2.731457  2.417581  \n",
       "\n",
       "[5 rows x 1601 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sub.to_csv('../D_WEATHER/sub/unet_ch9_shuffle_unseen_v1_postpro.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit ('pytorch': conda)",
   "language": "python",
   "name": "python37564bitpytorchconda133dde54c45c40c2946593d30b593426"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
