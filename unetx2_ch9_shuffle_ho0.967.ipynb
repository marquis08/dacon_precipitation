{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import tqdm\n",
    "import argparse\n",
    "import tqdm\n",
    "from collections import defaultdict, Counter\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import nn, cuda\n",
    "from torch.autograd import Variable \n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import CenterCrop\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR\n",
    "\n",
    "# from efficientnet_pytorch import EfficientNet\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim import Optimizer\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def mae(y_true, y_pred) :\n",
    "    y_true, y_pred = np.array(y_true.detach().numpy()), np.array(y_pred.detach().numpy())\n",
    "    y_true = y_true.reshape(1, -1)[0]\n",
    "    y_pred = y_pred.reshape(1, -1)[0]\n",
    "    over_threshold = y_true >= 0.1\n",
    "    return np.mean(np.abs(y_true[over_threshold] - y_pred[over_threshold]))\n",
    "\n",
    "def fscore(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true.detach().numpy()), np.array(y_pred.detach().numpy())\n",
    "    y_true = y_true.reshape(1, -1)[0]\n",
    "    y_pred = y_pred.reshape(1, -1)[0]\n",
    "    remove_NAs = y_true >= 0\n",
    "    y_true = np.where(y_true[remove_NAs] >= 0.1, 1, 0)\n",
    "    y_pred = np.where(y_pred[remove_NAs] >= 0.1, 1, 0)\n",
    "    return(f1_score(y_true, y_pred))\n",
    "\n",
    "def maeOverFscore(y_true, y_pred):\n",
    "    return mae(y_true, y_pred) / (fscore(y_true, y_pred) + 1e-07)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **File info**\n",
    "**ex. subset_010462_01**\n",
    "> **orbit 010462**\n",
    "\n",
    "> **subset 01**\n",
    "\n",
    "> **ortbit 별로 subset 개수는 다를 수 있고 연속적이지 않을 수도 있음**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>orbit</th>\n",
       "      <th>orbit_subset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../D_WEATHER//input/train/subset_010462_01.npy</td>\n",
       "      <td>10462</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../D_WEATHER//input/train/subset_010462_02.npy</td>\n",
       "      <td>10462</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../D_WEATHER//input/train/subset_010462_03.npy</td>\n",
       "      <td>10462</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../D_WEATHER//input/train/subset_010462_04.npy</td>\n",
       "      <td>10462</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../D_WEATHER//input/train/subset_010462_05.npy</td>\n",
       "      <td>10462</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             path  orbit  orbit_subset\n",
       "0  ../D_WEATHER//input/train/subset_010462_01.npy  10462             1\n",
       "1  ../D_WEATHER//input/train/subset_010462_02.npy  10462             2\n",
       "2  ../D_WEATHER//input/train/subset_010462_03.npy  10462             3\n",
       "3  ../D_WEATHER//input/train/subset_010462_04.npy  10462             4\n",
       "4  ../D_WEATHER//input/train/subset_010462_05.npy  10462             5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_df = pd.read_csv(\"../D_WEATHER//input/train_df.csv\")\n",
    "te_df = pd.read_csv(\"../D_WEATHER/input/test_df.csv\")\n",
    "tr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((73825, 3), (2520, 3))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = tr_df[:int(len(tr_df)*0.967)]\n",
    "valid_df = tr_df[int(len(tr_df)*0.967):]\n",
    "\n",
    "train_df.shape, valid_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Weather_Dataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        \n",
    "        self.image_list = []\n",
    "        self.label_list = []\n",
    "\n",
    "        for file in self.df['path']:\n",
    "            data = np.load(file)\n",
    "            image = data[:,:,:9] # use 14 channels except target\n",
    "            image = np.transpose(image, (2,0,1))\n",
    "            image = image.astype(np.float32)\n",
    "            self.image_list.append(image)\n",
    "            \n",
    "            label = data[:,:,-1].reshape(40,40,1)\n",
    "            label = np.transpose(label, (2,0,1))\n",
    "            self.label_list.append(label)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        image = self.image_list[idx]\n",
    "        label = self.label_list[idx]\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def worker_init(worker_id):\n",
    "#     np.random.seed(SEED)\n",
    "\n",
    "def build_dataloader(df, batch_size, shuffle=False):\n",
    "    dataset = Weather_Dataset(df)\n",
    "    dataloader = DataLoader(\n",
    "                            dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=shuffle,\n",
    "                            num_workers=0,\n",
    "#                             worker_init_fn=worker_init\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "def build_te_dataloader(df, batch_size, shuffle=False):\n",
    "    dataset = Test_Dataset(df)\n",
    "    dataloader = DataLoader(\n",
    "                            dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=shuffle,\n",
    "                            num_workers=0,\n",
    "#                             worker_init_fn=worker_init\n",
    "                            )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetX2(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=True):\n",
    "        super(UNetX2, self).__init__()\n",
    "        self.n_channels = n_channels # \n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = Down(512, 1024 // factor)\n",
    "        self.up1 = Up(1024, 512, bilinear)\n",
    "        self.up2 = Up(512, 256, bilinear)\n",
    "        self.up3 = Up(256, 128, bilinear)\n",
    "        self.up4 = Up(128, 64 * factor, bilinear)\n",
    "        self.outc = OutConv(64, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1) #64\n",
    "        x3 = self.down2(x2) #128\n",
    "        x4 = self.down3(x3) #256\n",
    "        x5 = self.down4(x4) #512\n",
    "        x = self.up1(x5, x4) #1024\n",
    "        x = self.up2(x, x3) #512\n",
    "        x = self.up3(x, x2) #256\n",
    "        x = self.up4(x, x1) #128 - 64\n",
    "        ####################\n",
    "        x2 = self.down1(x) #64\n",
    "        x3 = self.down2(x2) #128\n",
    "        x4 = self.down3(x3) #256\n",
    "        x5 = self.down4(x4) #512\n",
    "        x = self.up1(x5, x4) #1024\n",
    "        x = self.up2(x, x3) #512\n",
    "        x = self.up3(x, x2) #256\n",
    "        x = self.up4(x, x1) #128 - 64\n",
    "        ###\n",
    "        logits = self.outc(x) # 64\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels // 2, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels , in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        diffY = torch.tensor([x2.size()[2] - x1.size()[2]])\n",
    "        diffX = torch.tensor([x2.size()[3] - x1.size()[3]])\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        # if you have padding issues, see\n",
    "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
    "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "batch_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = build_dataloader(train_df, batch_size, shuffle=True)\n",
    "valid_loader = build_dataloader(valid_df, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enable gpu use\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "seed_everything(SEED)\n",
    "\n",
    "device = 'cuda:0'\n",
    "use_gpu = cuda.is_available()\n",
    "if use_gpu:\n",
    "    print(\"enable gpu use\")\n",
    "else:\n",
    "    print(\"enable cpu for debugging\")\n",
    "\n",
    "model = UNetX2(n_channels=9, n_classes=1, bilinear=False) # if bilinear = True -> non deterministic : not recommended\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 40, 40]           5,248\n",
      "       BatchNorm2d-2           [-1, 64, 40, 40]             128\n",
      "              ReLU-3           [-1, 64, 40, 40]               0\n",
      "            Conv2d-4           [-1, 64, 40, 40]          36,928\n",
      "       BatchNorm2d-5           [-1, 64, 40, 40]             128\n",
      "              ReLU-6           [-1, 64, 40, 40]               0\n",
      "        DoubleConv-7           [-1, 64, 40, 40]               0\n",
      "         MaxPool2d-8           [-1, 64, 20, 20]               0\n",
      "            Conv2d-9          [-1, 128, 20, 20]          73,856\n",
      "      BatchNorm2d-10          [-1, 128, 20, 20]             256\n",
      "             ReLU-11          [-1, 128, 20, 20]               0\n",
      "           Conv2d-12          [-1, 128, 20, 20]         147,584\n",
      "      BatchNorm2d-13          [-1, 128, 20, 20]             256\n",
      "             ReLU-14          [-1, 128, 20, 20]               0\n",
      "       DoubleConv-15          [-1, 128, 20, 20]               0\n",
      "             Down-16          [-1, 128, 20, 20]               0\n",
      "        MaxPool2d-17          [-1, 128, 10, 10]               0\n",
      "           Conv2d-18          [-1, 256, 10, 10]         295,168\n",
      "      BatchNorm2d-19          [-1, 256, 10, 10]             512\n",
      "             ReLU-20          [-1, 256, 10, 10]               0\n",
      "           Conv2d-21          [-1, 256, 10, 10]         590,080\n",
      "      BatchNorm2d-22          [-1, 256, 10, 10]             512\n",
      "             ReLU-23          [-1, 256, 10, 10]               0\n",
      "       DoubleConv-24          [-1, 256, 10, 10]               0\n",
      "             Down-25          [-1, 256, 10, 10]               0\n",
      "        MaxPool2d-26            [-1, 256, 5, 5]               0\n",
      "           Conv2d-27            [-1, 512, 5, 5]       1,180,160\n",
      "      BatchNorm2d-28            [-1, 512, 5, 5]           1,024\n",
      "             ReLU-29            [-1, 512, 5, 5]               0\n",
      "           Conv2d-30            [-1, 512, 5, 5]       2,359,808\n",
      "      BatchNorm2d-31            [-1, 512, 5, 5]           1,024\n",
      "             ReLU-32            [-1, 512, 5, 5]               0\n",
      "       DoubleConv-33            [-1, 512, 5, 5]               0\n",
      "             Down-34            [-1, 512, 5, 5]               0\n",
      "        MaxPool2d-35            [-1, 512, 2, 2]               0\n",
      "           Conv2d-36           [-1, 1024, 2, 2]       4,719,616\n",
      "      BatchNorm2d-37           [-1, 1024, 2, 2]           2,048\n",
      "             ReLU-38           [-1, 1024, 2, 2]               0\n",
      "           Conv2d-39           [-1, 1024, 2, 2]       9,438,208\n",
      "      BatchNorm2d-40           [-1, 1024, 2, 2]           2,048\n",
      "             ReLU-41           [-1, 1024, 2, 2]               0\n",
      "       DoubleConv-42           [-1, 1024, 2, 2]               0\n",
      "             Down-43           [-1, 1024, 2, 2]               0\n",
      "  ConvTranspose2d-44            [-1, 512, 4, 4]       2,097,664\n",
      "           Conv2d-45            [-1, 512, 5, 5]       4,719,104\n",
      "      BatchNorm2d-46            [-1, 512, 5, 5]           1,024\n",
      "             ReLU-47            [-1, 512, 5, 5]               0\n",
      "           Conv2d-48            [-1, 512, 5, 5]       2,359,808\n",
      "      BatchNorm2d-49            [-1, 512, 5, 5]           1,024\n",
      "             ReLU-50            [-1, 512, 5, 5]               0\n",
      "       DoubleConv-51            [-1, 512, 5, 5]               0\n",
      "               Up-52            [-1, 512, 5, 5]               0\n",
      "  ConvTranspose2d-53          [-1, 256, 10, 10]         524,544\n",
      "           Conv2d-54          [-1, 256, 10, 10]       1,179,904\n",
      "      BatchNorm2d-55          [-1, 256, 10, 10]             512\n",
      "             ReLU-56          [-1, 256, 10, 10]               0\n",
      "           Conv2d-57          [-1, 256, 10, 10]         590,080\n",
      "      BatchNorm2d-58          [-1, 256, 10, 10]             512\n",
      "             ReLU-59          [-1, 256, 10, 10]               0\n",
      "       DoubleConv-60          [-1, 256, 10, 10]               0\n",
      "               Up-61          [-1, 256, 10, 10]               0\n",
      "  ConvTranspose2d-62          [-1, 128, 20, 20]         131,200\n",
      "           Conv2d-63          [-1, 128, 20, 20]         295,040\n",
      "      BatchNorm2d-64          [-1, 128, 20, 20]             256\n",
      "             ReLU-65          [-1, 128, 20, 20]               0\n",
      "           Conv2d-66          [-1, 128, 20, 20]         147,584\n",
      "      BatchNorm2d-67          [-1, 128, 20, 20]             256\n",
      "             ReLU-68          [-1, 128, 20, 20]               0\n",
      "       DoubleConv-69          [-1, 128, 20, 20]               0\n",
      "               Up-70          [-1, 128, 20, 20]               0\n",
      "  ConvTranspose2d-71           [-1, 64, 40, 40]          32,832\n",
      "           Conv2d-72           [-1, 64, 40, 40]          73,792\n",
      "      BatchNorm2d-73           [-1, 64, 40, 40]             128\n",
      "             ReLU-74           [-1, 64, 40, 40]               0\n",
      "           Conv2d-75           [-1, 64, 40, 40]          36,928\n",
      "      BatchNorm2d-76           [-1, 64, 40, 40]             128\n",
      "             ReLU-77           [-1, 64, 40, 40]               0\n",
      "       DoubleConv-78           [-1, 64, 40, 40]               0\n",
      "               Up-79           [-1, 64, 40, 40]               0\n",
      "        MaxPool2d-80           [-1, 64, 20, 20]               0\n",
      "           Conv2d-81          [-1, 128, 20, 20]          73,856\n",
      "      BatchNorm2d-82          [-1, 128, 20, 20]             256\n",
      "             ReLU-83          [-1, 128, 20, 20]               0\n",
      "           Conv2d-84          [-1, 128, 20, 20]         147,584\n",
      "      BatchNorm2d-85          [-1, 128, 20, 20]             256\n",
      "             ReLU-86          [-1, 128, 20, 20]               0\n",
      "       DoubleConv-87          [-1, 128, 20, 20]               0\n",
      "             Down-88          [-1, 128, 20, 20]               0\n",
      "        MaxPool2d-89          [-1, 128, 10, 10]               0\n",
      "           Conv2d-90          [-1, 256, 10, 10]         295,168\n",
      "      BatchNorm2d-91          [-1, 256, 10, 10]             512\n",
      "             ReLU-92          [-1, 256, 10, 10]               0\n",
      "           Conv2d-93          [-1, 256, 10, 10]         590,080\n",
      "      BatchNorm2d-94          [-1, 256, 10, 10]             512\n",
      "             ReLU-95          [-1, 256, 10, 10]               0\n",
      "       DoubleConv-96          [-1, 256, 10, 10]               0\n",
      "             Down-97          [-1, 256, 10, 10]               0\n",
      "        MaxPool2d-98            [-1, 256, 5, 5]               0\n",
      "           Conv2d-99            [-1, 512, 5, 5]       1,180,160\n",
      "     BatchNorm2d-100            [-1, 512, 5, 5]           1,024\n",
      "            ReLU-101            [-1, 512, 5, 5]               0\n",
      "          Conv2d-102            [-1, 512, 5, 5]       2,359,808\n",
      "     BatchNorm2d-103            [-1, 512, 5, 5]           1,024\n",
      "            ReLU-104            [-1, 512, 5, 5]               0\n",
      "      DoubleConv-105            [-1, 512, 5, 5]               0\n",
      "            Down-106            [-1, 512, 5, 5]               0\n",
      "       MaxPool2d-107            [-1, 512, 2, 2]               0\n",
      "          Conv2d-108           [-1, 1024, 2, 2]       4,719,616\n",
      "     BatchNorm2d-109           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-110           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-111           [-1, 1024, 2, 2]       9,438,208\n",
      "     BatchNorm2d-112           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-113           [-1, 1024, 2, 2]               0\n",
      "      DoubleConv-114           [-1, 1024, 2, 2]               0\n",
      "            Down-115           [-1, 1024, 2, 2]               0\n",
      " ConvTranspose2d-116            [-1, 512, 4, 4]       2,097,664\n",
      "          Conv2d-117            [-1, 512, 5, 5]       4,719,104\n",
      "     BatchNorm2d-118            [-1, 512, 5, 5]           1,024\n",
      "            ReLU-119            [-1, 512, 5, 5]               0\n",
      "          Conv2d-120            [-1, 512, 5, 5]       2,359,808\n",
      "     BatchNorm2d-121            [-1, 512, 5, 5]           1,024\n",
      "            ReLU-122            [-1, 512, 5, 5]               0\n",
      "      DoubleConv-123            [-1, 512, 5, 5]               0\n",
      "              Up-124            [-1, 512, 5, 5]               0\n",
      " ConvTranspose2d-125          [-1, 256, 10, 10]         524,544\n",
      "          Conv2d-126          [-1, 256, 10, 10]       1,179,904\n",
      "     BatchNorm2d-127          [-1, 256, 10, 10]             512\n",
      "            ReLU-128          [-1, 256, 10, 10]               0\n",
      "          Conv2d-129          [-1, 256, 10, 10]         590,080\n",
      "     BatchNorm2d-130          [-1, 256, 10, 10]             512\n",
      "            ReLU-131          [-1, 256, 10, 10]               0\n",
      "      DoubleConv-132          [-1, 256, 10, 10]               0\n",
      "              Up-133          [-1, 256, 10, 10]               0\n",
      " ConvTranspose2d-134          [-1, 128, 20, 20]         131,200\n",
      "          Conv2d-135          [-1, 128, 20, 20]         295,040\n",
      "     BatchNorm2d-136          [-1, 128, 20, 20]             256\n",
      "            ReLU-137          [-1, 128, 20, 20]               0\n",
      "          Conv2d-138          [-1, 128, 20, 20]         147,584\n",
      "     BatchNorm2d-139          [-1, 128, 20, 20]             256\n",
      "            ReLU-140          [-1, 128, 20, 20]               0\n",
      "      DoubleConv-141          [-1, 128, 20, 20]               0\n",
      "              Up-142          [-1, 128, 20, 20]               0\n",
      " ConvTranspose2d-143           [-1, 64, 40, 40]          32,832\n",
      "          Conv2d-144           [-1, 64, 40, 40]          73,792\n",
      "     BatchNorm2d-145           [-1, 64, 40, 40]             128\n",
      "            ReLU-146           [-1, 64, 40, 40]               0\n",
      "          Conv2d-147           [-1, 64, 40, 40]          36,928\n",
      "     BatchNorm2d-148           [-1, 64, 40, 40]             128\n",
      "            ReLU-149           [-1, 64, 40, 40]               0\n",
      "      DoubleConv-150           [-1, 64, 40, 40]               0\n",
      "              Up-151           [-1, 64, 40, 40]               0\n",
      "          Conv2d-152            [-1, 1, 40, 40]             577\n",
      "         OutConv-153            [-1, 1, 40, 40]               0\n",
      "================================================================\n",
      "Total params: 62,051,969\n",
      "Trainable params: 62,051,969\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.05\n",
      "Forward/backward pass size (MB): 43.94\n",
      "Params size (MB): 236.71\n",
      "Estimated Total Size (MB): 280.71\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model, input_size=(9,40,40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training\n",
      "================ ༼ つ ◕_◕ ༽つ BEST epoch : 1, MOF : 2.8146054639840177 \n",
      "E 1/200 tr_loss: 44.74203 tr_mae: 1.87819 tr_fs: 0.53167 tr_mof: 4.14861 val_loss: 67.92583 val_mae: 1.44039 val_fs: 0.51176 val_mof: 2.81461 lr: 0.001000 elapsed: 154\n",
      "================ ༼ つ ◕_◕ ༽つ BEST epoch : 2, MOF : 2.5245898254647967 \n",
      "E 2/200 tr_loss: 44.13590 tr_mae: 1.67777 tr_fs: 0.63269 tr_mof: 2.65587 val_loss: 67.91354 val_mae: 1.39849 val_fs: 0.55376 val_mof: 2.52459 lr: 0.001000 elapsed: 157\n",
      "E 3/200 tr_loss: 43.80403 tr_mae: 1.57415 tr_fs: 0.66437 tr_mof: 2.37181 val_loss: 67.92707 val_mae: 1.72045 val_fs: 0.25551 val_mof: 7.00667 lr: 0.001000 elapsed: 155\n",
      "E 4/200 tr_loss: 43.54637 tr_mae: 1.51016 tr_fs: 0.69131 tr_mof: 2.18604 val_loss: 68.17601 val_mae: 1.33588 val_fs: 0.23486 val_mof: 5.76153 lr: 0.001000 elapsed: 155\n",
      "E 5/200 tr_loss: 44.12064 tr_mae: 1.48453 tr_fs: 0.69807 tr_mof: 2.12864 val_loss: 68.49149 val_mae: 1.32259 val_fs: 0.23923 val_mof: 5.63568 lr: 0.001000 elapsed: 155\n",
      "E 6/200 tr_loss: 44.12463 tr_mae: 1.53192 tr_fs: 0.67229 tr_mof: 2.28763 val_loss: 68.49422 val_mae: 1.62105 val_fs: 0.12752 val_mof: 13.05534 lr: 0.001000 elapsed: 154\n",
      "================ ༼ つ ◕_◕ ༽つ BEST epoch : 7, MOF : 2.253259110210745 \n",
      "E 7/200 tr_loss: 43.54558 tr_mae: 1.50099 tr_fs: 0.68779 tr_mof: 2.18726 val_loss: 67.90179 val_mae: 1.29825 val_fs: 0.57712 val_mof: 2.25326 lr: 0.001000 elapsed: 154\n",
      "E 8/200 tr_loss: 43.54161 tr_mae: 1.45679 tr_fs: 0.70255 tr_mof: 2.07430 val_loss: 67.92545 val_mae: 1.35785 val_fs: 0.50029 val_mof: 2.77019 lr: 0.001000 elapsed: 154\n",
      "E 9/200 tr_loss: 44.11729 tr_mae: 1.44630 tr_fs: 0.70805 tr_mof: 2.04405 val_loss: 67.91097 val_mae: 1.47061 val_fs: 0.49129 val_mof: 3.00411 lr: 0.001000 elapsed: 155\n",
      "E 10/200 tr_loss: 43.53989 tr_mae: 1.43649 tr_fs: 0.71131 tr_mof: 2.02010 val_loss: 67.92664 val_mae: 1.29256 val_fs: 0.36948 val_mof: 3.60284 lr: 0.001000 elapsed: 154\n",
      "E 11/200 tr_loss: 44.69197 tr_mae: 1.42509 tr_fs: 0.71695 tr_mof: 1.98909 val_loss: 68.26707 val_mae: 2.14600 val_fs: 0.18700 val_mof: 11.99929 lr: 0.001000 elapsed: 154\n",
      "E 12/200 tr_loss: 43.53892 tr_mae: 1.42152 tr_fs: 0.71614 tr_mof: 1.98558 val_loss: 67.90214 val_mae: 1.30431 val_fs: 0.57694 val_mof: 2.26201 lr: 0.001000 elapsed: 153\n",
      "================ ༼ つ ◕_◕ ༽つ BEST epoch : 13, MOF : 2.1269485685432774 \n",
      "E 13/200 tr_loss: 43.53778 tr_mae: 1.40530 tr_fs: 0.72378 tr_mof: 1.94237 val_loss: 67.90820 val_mae: 1.17891 val_fs: 0.55317 val_mof: 2.12695 lr: 0.001000 elapsed: 153\n",
      "E 14/200 tr_loss: 43.53679 tr_mae: 1.39231 tr_fs: 0.72970 tr_mof: 1.90896 val_loss: 67.90014 val_mae: 1.25868 val_fs: 0.59081 val_mof: 2.13312 lr: 0.001000 elapsed: 154\n",
      "E 15/200 tr_loss: 44.11266 tr_mae: 1.38856 tr_fs: 0.73099 tr_mof: 1.90049 val_loss: 67.90151 val_mae: 1.30265 val_fs: 0.52063 val_mof: 2.50965 lr: 0.001000 elapsed: 153\n",
      "E 16/200 tr_loss: 43.57622 tr_mae: 1.40640 tr_fs: 0.73037 tr_mof: 1.92645 val_loss: 68.49992 val_mae: 1.32160 val_fs: 0.14418 val_mof: 9.36524 lr: 0.001000 elapsed: 153\n",
      "E 17/200 tr_loss: 43.95108 tr_mae: 1.38115 tr_fs: 0.73578 tr_mof: 1.87753 val_loss: 68.11827 val_mae: 1.33137 val_fs: 0.36343 val_mof: 3.80482 lr: 0.001000 elapsed: 154\n",
      "E 18/200 tr_loss: 44.69274 tr_mae: 1.43584 tr_fs: 0.71290 tr_mof: 2.01838 val_loss: 67.91515 val_mae: 1.33566 val_fs: 0.37050 val_mof: 3.70226 lr: 0.001000 elapsed: 153\n",
      "E 19/200 tr_loss: 44.11364 tr_mae: 1.40695 tr_fs: 0.72841 tr_mof: 1.93217 val_loss: 68.47231 val_mae: 1.66752 val_fs: 0.10412 val_mof: 16.72179 lr: 0.001000 elapsed: 154\n",
      "================ ༼ つ ◕_◕ ༽つ BEST epoch : 20, MOF : 1.8793536479445758 \n",
      "E 20/200 tr_loss: 44.06671 tr_mae: 1.37015 tr_fs: 0.73970 tr_mof: 1.85307 val_loss: 67.89677 val_mae: 1.20671 val_fs: 0.64145 val_mof: 1.87935 lr: 0.000500 elapsed: 153\n",
      "E 21/200 tr_loss: 43.53339 tr_mae: 1.35339 tr_fs: 0.74813 tr_mof: 1.80933 val_loss: 67.98132 val_mae: 1.14578 val_fs: 0.23840 val_mof: 4.94069 lr: 0.000500 elapsed: 153\n",
      "E 22/200 tr_loss: 43.53332 tr_mae: 1.35362 tr_fs: 0.74847 tr_mof: 1.80864 val_loss: 67.99007 val_mae: 1.14769 val_fs: 0.26370 val_mof: 4.40839 lr: 0.000500 elapsed: 153\n",
      "E 23/200 tr_loss: 43.53326 tr_mae: 1.35130 tr_fs: 0.74999 tr_mof: 1.80191 val_loss: 67.91046 val_mae: 1.18908 val_fs: 0.57946 val_mof: 2.05553 lr: 0.000500 elapsed: 153\n",
      "E 24/200 tr_loss: 44.42589 tr_mae: 1.34474 tr_fs: 0.75109 tr_mof: 1.79042 val_loss: 67.90330 val_mae: 1.30948 val_fs: 0.65605 val_mof: 1.99085 lr: 0.000500 elapsed: 153\n",
      "E 25/200 tr_loss: 43.53272 tr_mae: 1.34288 tr_fs: 0.75181 tr_mof: 1.78651 val_loss: 67.98427 val_mae: 1.20354 val_fs: 0.36691 val_mof: 3.38249 lr: 0.000500 elapsed: 152\n",
      "================ ༼ つ ◕_◕ ༽つ BEST epoch : 26, MOF : 1.843490994765712 \n",
      "E 26/200 tr_loss: 44.10872 tr_mae: 1.34139 tr_fs: 0.75119 tr_mof: 1.78570 val_loss: 67.90046 val_mae: 1.22173 val_fs: 0.66169 val_mof: 1.84349 lr: 0.000500 elapsed: 152\n",
      "E 27/200 tr_loss: 43.53201 tr_mae: 1.33469 tr_fs: 0.75502 tr_mof: 1.76775 val_loss: 67.90582 val_mae: 1.18341 val_fs: 0.59726 val_mof: 1.99282 lr: 0.000500 elapsed: 153\n",
      "E 28/200 tr_loss: 43.87313 tr_mae: 1.33215 tr_fs: 0.75745 tr_mof: 1.75888 val_loss: 67.90829 val_mae: 1.17132 val_fs: 0.61034 val_mof: 1.93515 lr: 0.000500 elapsed: 153\n",
      "E 29/200 tr_loss: 44.10794 tr_mae: 1.32756 tr_fs: 0.75683 tr_mof: 1.75379 val_loss: 67.90733 val_mae: 1.41810 val_fs: 0.49201 val_mof: 2.89967 lr: 0.000500 elapsed: 152\n",
      "E 30/200 tr_loss: 43.74249 tr_mae: 1.32666 tr_fs: 0.75837 tr_mof: 1.74946 val_loss: 67.90644 val_mae: 1.12347 val_fs: 0.56344 val_mof: 2.00194 lr: 0.000500 elapsed: 153\n",
      "================ ༼ つ ◕_◕ ༽つ BEST epoch : 31, MOF : 1.7781485141861837 \n",
      "E 31/200 tr_loss: 43.53147 tr_mae: 1.32665 tr_fs: 0.75850 tr_mof: 1.74905 val_loss: 67.90223 val_mae: 1.17692 val_fs: 0.66130 val_mof: 1.77815 lr: 0.000500 elapsed: 153\n",
      "================ ༼ つ ◕_◕ ༽つ BEST epoch : 32, MOF : 1.6357709161940224 \n",
      "E 32/200 tr_loss: 44.10766 tr_mae: 1.32420 tr_fs: 0.75934 tr_mof: 1.74371 val_loss: 67.89609 val_mae: 1.13689 val_fs: 0.69432 val_mof: 1.63577 lr: 0.000500 elapsed: 154\n",
      "E 33/200 tr_loss: 43.53115 tr_mae: 1.32083 tr_fs: 0.76042 tr_mof: 1.73709 val_loss: 67.90092 val_mae: 1.29417 val_fs: 0.63373 val_mof: 2.03903 lr: 0.000500 elapsed: 154\n",
      "E 34/200 tr_loss: 44.68343 tr_mae: 1.31625 tr_fs: 0.76143 tr_mof: 1.72901 val_loss: 67.89639 val_mae: 1.16783 val_fs: 0.67837 val_mof: 1.72000 lr: 0.000500 elapsed: 152\n",
      "E 35/200 tr_loss: 43.53058 tr_mae: 1.31433 tr_fs: 0.76266 tr_mof: 1.72345 val_loss: 67.89702 val_mae: 1.19247 val_fs: 0.66017 val_mof: 1.80512 lr: 0.000500 elapsed: 152\n",
      "E 36/200 tr_loss: 44.10708 tr_mae: 1.31705 tr_fs: 0.76124 tr_mof: 1.73000 val_loss: 68.05364 val_mae: 1.57259 val_fs: 0.19916 val_mof: 8.10077 lr: 0.000500 elapsed: 152\n",
      "E 37/200 tr_loss: 43.53116 tr_mae: 1.32451 tr_fs: 0.75987 tr_mof: 1.74296 val_loss: 67.89777 val_mae: 1.22063 val_fs: 0.66327 val_mof: 1.83769 lr: 0.000500 elapsed: 153\n",
      "E 38/200 tr_loss: 43.53061 tr_mae: 1.31632 tr_fs: 0.76245 tr_mof: 1.72649 val_loss: 67.93282 val_mae: 1.06221 val_fs: 0.40421 val_mof: 2.65990 lr: 0.000500 elapsed: 153\n",
      "E 39/200 tr_loss: 44.10635 tr_mae: 1.30499 tr_fs: 0.76716 tr_mof: 1.70100 val_loss: 67.89393 val_mae: 1.16679 val_fs: 0.66965 val_mof: 1.73967 lr: 0.000250 elapsed: 152\n",
      "E 40/200 tr_loss: 44.10592 tr_mae: 1.29954 tr_fs: 0.76854 tr_mof: 1.69085 val_loss: 67.89437 val_mae: 1.15498 val_fs: 0.68504 val_mof: 1.68323 lr: 0.000250 elapsed: 152\n",
      "E 41/200 tr_loss: 43.77726 tr_mae: 1.29849 tr_fs: 0.76846 tr_mof: 1.68962 val_loss: 67.89749 val_mae: 1.22389 val_fs: 0.63272 val_mof: 1.93287 lr: 0.000250 elapsed: 152\n",
      "E 42/200 tr_loss: 44.09295 tr_mae: 1.29809 tr_fs: 0.76984 tr_mof: 1.68638 val_loss: 67.90314 val_mae: 1.31767 val_fs: 0.66071 val_mof: 1.98944 lr: 0.000250 elapsed: 152\n",
      "E 43/200 tr_loss: 43.52943 tr_mae: 1.29839 tr_fs: 0.76975 tr_mof: 1.68657 val_loss: 68.05459 val_mae: 1.15502 val_fs: 0.19974 val_mof: 5.87145 lr: 0.000250 elapsed: 152\n",
      "================ ༼ つ ◕_◕ ༽つ BEST epoch : 44, MOF : 1.6189656110620583 \n",
      "E 44/200 tr_loss: 43.52915 tr_mae: 1.29425 tr_fs: 0.77095 tr_mof: 1.67859 val_loss: 67.89839 val_mae: 1.09566 val_fs: 0.67503 val_mof: 1.61897 lr: 0.000250 elapsed: 152\n",
      "E 45/200 tr_loss: 44.06132 tr_mae: 1.29364 tr_fs: 0.77029 tr_mof: 1.67941 val_loss: 67.92160 val_mae: 1.14586 val_fs: 0.58324 val_mof: 1.97629 lr: 0.000250 elapsed: 152\n",
      "E 46/200 tr_loss: 43.52936 tr_mae: 1.29595 tr_fs: 0.76974 tr_mof: 1.68375 val_loss: 67.90009 val_mae: 1.17793 val_fs: 0.64396 val_mof: 1.83045 lr: 0.000250 elapsed: 152\n",
      "================ ༼ つ ◕_◕ ༽つ BEST epoch : 47, MOF : 1.5608743320619152 \n",
      "E 47/200 tr_loss: 44.00012 tr_mae: 1.29103 tr_fs: 0.77165 tr_mof: 1.67295 val_loss: 67.89576 val_mae: 1.12216 val_fs: 0.71736 val_mof: 1.56087 lr: 0.000250 elapsed: 152\n",
      "E 48/200 tr_loss: 43.52867 tr_mae: 1.28940 tr_fs: 0.77180 tr_mof: 1.67070 val_loss: 67.90369 val_mae: 1.15427 val_fs: 0.61230 val_mof: 1.88722 lr: 0.000250 elapsed: 152\n",
      "E 49/200 tr_loss: 44.00811 tr_mae: 1.28861 tr_fs: 0.77246 tr_mof: 1.66844 val_loss: 67.97518 val_mae: 1.16383 val_fs: 0.28850 val_mof: 4.14483 lr: 0.000250 elapsed: 152\n",
      "E 50/200 tr_loss: 43.52858 tr_mae: 1.28796 tr_fs: 0.77095 tr_mof: 1.67079 val_loss: 67.98248 val_mae: 1.15520 val_fs: 0.51511 val_mof: 2.25836 lr: 0.000250 elapsed: 152\n",
      "E 51/200 tr_loss: 43.52975 tr_mae: 1.30060 tr_fs: 0.76822 tr_mof: 1.69299 val_loss: 67.90098 val_mae: 1.18414 val_fs: 0.64770 val_mof: 1.82893 lr: 0.000250 elapsed: 152\n",
      "E 52/200 tr_loss: 44.10504 tr_mae: 1.28794 tr_fs: 0.77221 tr_mof: 1.66750 val_loss: 67.90038 val_mae: 1.16649 val_fs: 0.61987 val_mof: 1.88799 lr: 0.000250 elapsed: 152\n",
      "E 53/200 tr_loss: 43.52862 tr_mae: 1.28670 tr_fs: 0.77260 tr_mof: 1.66572 val_loss: 67.93911 val_mae: 1.08867 val_fs: 0.37205 val_mof: 3.02046 lr: 0.000250 elapsed: 152\n",
      "================ ༼ つ ◕_◕ ༽つ BEST epoch : 54, MOF : 1.5130938352257626 \n",
      "E 54/200 tr_loss: 43.52796 tr_mae: 1.27664 tr_fs: 0.77503 tr_mof: 1.64695 val_loss: 67.89293 val_mae: 1.09376 val_fs: 0.72150 val_mof: 1.51309 lr: 0.000125 elapsed: 152\n",
      "E 55/200 tr_loss: 44.10446 tr_mae: 1.28087 tr_fs: 0.77540 tr_mof: 1.65159 val_loss: 67.89251 val_mae: 1.12720 val_fs: 0.70459 val_mof: 1.59694 lr: 0.000125 elapsed: 152\n",
      "E 56/200 tr_loss: 43.79150 tr_mae: 1.27556 tr_fs: 0.77619 tr_mof: 1.64332 val_loss: 67.89605 val_mae: 1.12703 val_fs: 0.67257 val_mof: 1.67654 lr: 0.000125 elapsed: 152\n",
      "E 57/200 tr_loss: 43.52781 tr_mae: 1.27601 tr_fs: 0.77629 tr_mof: 1.64405 val_loss: 67.90637 val_mae: 1.06374 val_fs: 0.58010 val_mof: 1.83513 lr: 0.000125 elapsed: 152\n",
      "E 58/200 tr_loss: 43.52775 tr_mae: 1.27392 tr_fs: 0.77527 tr_mof: 1.64299 val_loss: 67.90031 val_mae: 1.12983 val_fs: 0.71081 val_mof: 1.58560 lr: 0.000125 elapsed: 152\n",
      "E 59/200 tr_loss: 43.52782 tr_mae: 1.27440 tr_fs: 0.77674 tr_mof: 1.64060 val_loss: 67.89613 val_mae: 1.10827 val_fs: 0.65053 val_mof: 1.70627 lr: 0.000125 elapsed: 152\n",
      "E 60/200 tr_loss: 44.10362 tr_mae: 1.26936 tr_fs: 0.77681 tr_mof: 1.63389 val_loss: 67.90082 val_mae: 1.27425 val_fs: 0.68205 val_mof: 1.86346 lr: 0.000125 elapsed: 152\n",
      "E 61/200 tr_loss: 44.22268 tr_mae: 1.26761 tr_fs: 0.77690 tr_mof: 1.63120 val_loss: 67.89377 val_mae: 1.10805 val_fs: 0.72743 val_mof: 1.52044 lr: 0.000063 elapsed: 152\n",
      "E 62/200 tr_loss: 43.52727 tr_mae: 1.26774 tr_fs: 0.77780 tr_mof: 1.62996 val_loss: 67.89323 val_mae: 1.10125 val_fs: 0.72348 val_mof: 1.51950 lr: 0.000063 elapsed: 153\n",
      "E 63/200 tr_loss: 44.10332 tr_mae: 1.26534 tr_fs: 0.77919 tr_mof: 1.62375 val_loss: 67.89275 val_mae: 1.09998 val_fs: 0.72053 val_mof: 1.52403 lr: 0.000063 elapsed: 152\n",
      "================ ༼ つ ◕_◕ ༽つ BEST epoch : 64, MOF : 1.4998619266009683 \n",
      "E 64/200 tr_loss: 43.52709 tr_mae: 1.26446 tr_fs: 0.77928 tr_mof: 1.62244 val_loss: 67.89192 val_mae: 1.09241 val_fs: 0.72705 val_mof: 1.49986 lr: 0.000063 elapsed: 152\n",
      "E 65/200 tr_loss: 44.10327 tr_mae: 1.26617 tr_fs: 0.77952 tr_mof: 1.62424 val_loss: 67.89241 val_mae: 1.09061 val_fs: 0.72501 val_mof: 1.50112 lr: 0.000063 elapsed: 152\n",
      "E 66/200 tr_loss: 44.02625 tr_mae: 1.26003 tr_fs: 0.77971 tr_mof: 1.61572 val_loss: 67.89225 val_mae: 1.10981 val_fs: 0.71823 val_mof: 1.54256 lr: 0.000063 elapsed: 153\n",
      "E 67/200 tr_loss: 43.52696 tr_mae: 1.26476 tr_fs: 0.77929 tr_mof: 1.62296 val_loss: 67.89250 val_mae: 1.11510 val_fs: 0.71466 val_mof: 1.55762 lr: 0.000063 elapsed: 152\n",
      "================ ༼ つ ◕_◕ ༽つ BEST epoch : 68, MOF : 1.481304047296387 \n",
      "E 68/200 tr_loss: 43.52673 tr_mae: 1.26245 tr_fs: 0.77997 tr_mof: 1.61850 val_loss: 67.89448 val_mae: 1.08211 val_fs: 0.72904 val_mof: 1.48130 lr: 0.000063 elapsed: 152\n",
      "E 69/200 tr_loss: 43.52667 tr_mae: 1.25813 tr_fs: 0.78039 tr_mof: 1.61224 val_loss: 67.89297 val_mae: 1.11293 val_fs: 0.72416 val_mof: 1.53454 lr: 0.000063 elapsed: 153\n",
      "E 70/200 tr_loss: 43.52684 tr_mae: 1.26168 tr_fs: 0.78068 tr_mof: 1.61578 val_loss: 67.89248 val_mae: 1.08061 val_fs: 0.72696 val_mof: 1.48399 lr: 0.000063 elapsed: 152\n",
      "E 71/200 tr_loss: 44.10280 tr_mae: 1.25853 tr_fs: 0.78106 tr_mof: 1.61152 val_loss: 67.89297 val_mae: 1.11372 val_fs: 0.72765 val_mof: 1.52811 lr: 0.000063 elapsed: 152\n",
      "E 72/200 tr_loss: 44.10290 tr_mae: 1.25782 tr_fs: 0.78076 tr_mof: 1.61063 val_loss: 67.89258 val_mae: 1.11456 val_fs: 0.72219 val_mof: 1.54068 lr: 0.000063 elapsed: 152\n",
      "E 73/200 tr_loss: 43.52674 tr_mae: 1.25651 tr_fs: 0.78112 tr_mof: 1.60822 val_loss: 67.89686 val_mae: 1.06799 val_fs: 0.70378 val_mof: 1.51420 lr: 0.000063 elapsed: 153\n",
      "E 74/200 tr_loss: 44.10269 tr_mae: 1.25727 tr_fs: 0.78057 tr_mof: 1.61065 val_loss: 67.89538 val_mae: 1.15232 val_fs: 0.71402 val_mof: 1.61134 lr: 0.000063 elapsed: 152\n",
      "E 75/200 tr_loss: 43.52624 tr_mae: 1.25369 tr_fs: 0.78180 tr_mof: 1.60336 val_loss: 67.89289 val_mae: 1.10088 val_fs: 0.73082 val_mof: 1.50365 lr: 0.000031 elapsed: 152\n",
      "E 76/200 tr_loss: 43.52634 tr_mae: 1.25323 tr_fs: 0.78246 tr_mof: 1.60144 val_loss: 67.89269 val_mae: 1.10130 val_fs: 0.72384 val_mof: 1.51935 lr: 0.000031 elapsed: 152\n",
      "================ ༼ つ ◕_◕ ༽つ BEST epoch : 77, MOF : 1.4620076221559 \n",
      "E 77/200 tr_loss: 43.52633 tr_mae: 1.25347 tr_fs: 0.78256 tr_mof: 1.60163 val_loss: 67.89170 val_mae: 1.07768 val_fs: 0.73585 val_mof: 1.46201 lr: 0.000031 elapsed: 152\n",
      "E 78/200 tr_loss: 43.52628 tr_mae: 1.25492 tr_fs: 0.78269 tr_mof: 1.60326 val_loss: 67.89162 val_mae: 1.08972 val_fs: 0.72899 val_mof: 1.49259 lr: 0.000031 elapsed: 152\n",
      "E 79/200 tr_loss: 43.52601 tr_mae: 1.25041 tr_fs: 0.78335 tr_mof: 1.59630 val_loss: 67.89160 val_mae: 1.09562 val_fs: 0.71981 val_mof: 1.51971 lr: 0.000031 elapsed: 152\n",
      "E 80/200 tr_loss: 43.52606 tr_mae: 1.25077 tr_fs: 0.78299 tr_mof: 1.59733 val_loss: 67.89197 val_mae: 1.09317 val_fs: 0.72792 val_mof: 1.49908 lr: 0.000031 elapsed: 152\n",
      "E 81/200 tr_loss: 44.10228 tr_mae: 1.25068 tr_fs: 0.78303 tr_mof: 1.59705 val_loss: 67.89182 val_mae: 1.09597 val_fs: 0.72406 val_mof: 1.51115 lr: 0.000031 elapsed: 152\n",
      "E 82/200 tr_loss: 43.52607 tr_mae: 1.25235 tr_fs: 0.78285 tr_mof: 1.59964 val_loss: 67.89191 val_mae: 1.08549 val_fs: 0.73486 val_mof: 1.47466 lr: 0.000031 elapsed: 152\n",
      "E 83/200 tr_loss: 43.52639 tr_mae: 1.25326 tr_fs: 0.78291 tr_mof: 1.60062 val_loss: 67.89202 val_mae: 1.10349 val_fs: 0.71068 val_mof: 1.55030 lr: 0.000031 elapsed: 152\n",
      "E 84/200 tr_loss: 43.52589 tr_mae: 1.24696 tr_fs: 0.78349 tr_mof: 1.59127 val_loss: 67.89146 val_mae: 1.08525 val_fs: 0.73390 val_mof: 1.47631 lr: 0.000016 elapsed: 154\n",
      "E 85/200 tr_loss: 43.52571 tr_mae: 1.24803 tr_fs: 0.78373 tr_mof: 1.59225 val_loss: 67.89179 val_mae: 1.09119 val_fs: 0.73111 val_mof: 1.49015 lr: 0.000016 elapsed: 152\n",
      "E 86/200 tr_loss: 45.18625 tr_mae: 1.24828 tr_fs: 0.78363 tr_mof: 1.59282 val_loss: 67.89169 val_mae: 1.09226 val_fs: 0.72806 val_mof: 1.49787 lr: 0.000016 elapsed: 152\n",
      "E 87/200 tr_loss: 43.52602 tr_mae: 1.24779 tr_fs: 0.78385 tr_mof: 1.59132 val_loss: 67.89142 val_mae: 1.08990 val_fs: 0.72756 val_mof: 1.49554 lr: 0.000016 elapsed: 152\n",
      "E 88/200 tr_loss: 44.10188 tr_mae: 1.24452 tr_fs: 0.78426 tr_mof: 1.58679 val_loss: 67.89225 val_mae: 1.08656 val_fs: 0.73567 val_mof: 1.47442 lr: 0.000016 elapsed: 152\n",
      "E 89/200 tr_loss: 43.68332 tr_mae: 1.24455 tr_fs: 0.78402 tr_mof: 1.58730 val_loss: 67.89227 val_mae: 1.09437 val_fs: 0.72967 val_mof: 1.49732 lr: 0.000016 elapsed: 152\n",
      "E 90/200 tr_loss: 43.52557 tr_mae: 1.24463 tr_fs: 0.78458 tr_mof: 1.58625 val_loss: 67.89148 val_mae: 1.09004 val_fs: 0.72921 val_mof: 1.49233 lr: 0.000008 elapsed: 152\n",
      "E 91/200 tr_loss: 44.10198 tr_mae: 1.24404 tr_fs: 0.78408 tr_mof: 1.58646 val_loss: 67.89146 val_mae: 1.08634 val_fs: 0.73117 val_mof: 1.48332 lr: 0.000008 elapsed: 152\n",
      "E 92/200 tr_loss: 43.52565 tr_mae: 1.24523 tr_fs: 0.78449 tr_mof: 1.58721 val_loss: 67.89155 val_mae: 1.08203 val_fs: 0.73409 val_mof: 1.47159 lr: 0.000008 elapsed: 152\n",
      "E 93/200 tr_loss: 43.52566 tr_mae: 1.24510 tr_fs: 0.78486 tr_mof: 1.58628 val_loss: 67.89133 val_mae: 1.08709 val_fs: 0.73079 val_mof: 1.48503 lr: 0.000008 elapsed: 152\n",
      "E 94/200 tr_loss: 43.52575 tr_mae: 1.24579 tr_fs: 0.78484 tr_mof: 1.58739 val_loss: 67.89144 val_mae: 1.08313 val_fs: 0.73306 val_mof: 1.47509 lr: 0.000008 elapsed: 152\n",
      "E 95/200 tr_loss: 43.52565 tr_mae: 1.24502 tr_fs: 0.78433 tr_mof: 1.58711 val_loss: 67.89189 val_mae: 1.08415 val_fs: 0.73535 val_mof: 1.47191 lr: 0.000008 elapsed: 152\n",
      "E 96/200 tr_loss: 43.52552 tr_mae: 1.24279 tr_fs: 0.78421 tr_mof: 1.58468 val_loss: 67.89138 val_mae: 1.08594 val_fs: 0.73070 val_mof: 1.48370 lr: 0.000004 elapsed: 152\n",
      "E 97/200 tr_loss: 43.52552 tr_mae: 1.24491 tr_fs: 0.78480 tr_mof: 1.58637 val_loss: 67.89159 val_mae: 1.08627 val_fs: 0.73322 val_mof: 1.47902 lr: 0.000004 elapsed: 152\n",
      "E 98/200 tr_loss: 44.10186 tr_mae: 1.24193 tr_fs: 0.78467 tr_mof: 1.58237 val_loss: 67.89144 val_mae: 1.08567 val_fs: 0.73131 val_mof: 1.48212 lr: 0.000004 elapsed: 152\n",
      "E 99/200 tr_loss: 44.10173 tr_mae: 1.24128 tr_fs: 0.78474 tr_mof: 1.58162 val_loss: 67.89179 val_mae: 1.08951 val_fs: 0.73211 val_mof: 1.48572 lr: 0.000004 elapsed: 152\n",
      "E 100/200 tr_loss: 43.52539 tr_mae: 1.23970 tr_fs: 0.78527 tr_mof: 1.57858 val_loss: 67.89146 val_mae: 1.08433 val_fs: 0.73227 val_mof: 1.47828 lr: 0.000004 elapsed: 152\n",
      "E 101/200 tr_loss: 43.52560 tr_mae: 1.24245 tr_fs: 0.78489 tr_mof: 1.58283 val_loss: 67.89173 val_mae: 1.08793 val_fs: 0.73198 val_mof: 1.48387 lr: 0.000004 elapsed: 152\n",
      "E 102/200 tr_loss: 44.10169 tr_mae: 1.24293 tr_fs: 0.78463 tr_mof: 1.58397 val_loss: 67.89146 val_mae: 1.08266 val_fs: 0.73393 val_mof: 1.47275 lr: 0.000002 elapsed: 152\n",
      "E 103/200 tr_loss: 43.84251 tr_mae: 1.24187 tr_fs: 0.78491 tr_mof: 1.58199 val_loss: 67.89159 val_mae: 1.08478 val_fs: 0.73417 val_mof: 1.47512 lr: 0.000002 elapsed: 152\n",
      "E 104/200 tr_loss: 43.52550 tr_mae: 1.24288 tr_fs: 0.78533 tr_mof: 1.58257 val_loss: 67.89195 val_mae: 1.08968 val_fs: 0.73242 val_mof: 1.48532 lr: 0.000002 elapsed: 152\n",
      "E 105/200 tr_loss: 44.10196 tr_mae: 1.24650 tr_fs: 0.78470 tr_mof: 1.58844 val_loss: 67.89137 val_mae: 1.08379 val_fs: 0.73285 val_mof: 1.47642 lr: 0.000002 elapsed: 152\n",
      "E 106/200 tr_loss: 43.52564 tr_mae: 1.24433 tr_fs: 0.78510 tr_mof: 1.58504 val_loss: 67.89148 val_mae: 1.08457 val_fs: 0.73300 val_mof: 1.47719 lr: 0.000002 elapsed: 152\n",
      "E 107/200 tr_loss: 43.52590 tr_mae: 1.24651 tr_fs: 0.78531 tr_mof: 1.58714 val_loss: 67.89120 val_mae: 1.08473 val_fs: 0.73125 val_mof: 1.48095 lr: 0.000002 elapsed: 152\n",
      "E 108/200 tr_loss: 44.67811 tr_mae: 1.24402 tr_fs: 0.78522 tr_mof: 1.58416 val_loss: 67.89151 val_mae: 1.08613 val_fs: 0.73116 val_mof: 1.48300 lr: 0.000001 elapsed: 152\n",
      "E 109/200 tr_loss: 44.10172 tr_mae: 1.24432 tr_fs: 0.78466 tr_mof: 1.58596 val_loss: 67.89162 val_mae: 1.08453 val_fs: 0.73382 val_mof: 1.47543 lr: 0.000001 elapsed: 152\n",
      "E 110/200 tr_loss: 43.97067 tr_mae: 1.24509 tr_fs: 0.78479 tr_mof: 1.58649 val_loss: 67.89168 val_mae: 1.08491 val_fs: 0.73358 val_mof: 1.47647 lr: 0.000001 elapsed: 152\n",
      "E 111/200 tr_loss: 44.10169 tr_mae: 1.24292 tr_fs: 0.78526 tr_mof: 1.58271 val_loss: 67.89160 val_mae: 1.08758 val_fs: 0.73145 val_mof: 1.48444 lr: 0.000001 elapsed: 152\n",
      "E 112/200 tr_loss: 43.52537 tr_mae: 1.24207 tr_fs: 0.78514 tr_mof: 1.58205 val_loss: 67.89159 val_mae: 1.08549 val_fs: 0.73230 val_mof: 1.47985 lr: 0.000001 elapsed: 152\n",
      "E 113/200 tr_loss: 43.52544 tr_mae: 1.24220 tr_fs: 0.78544 tr_mof: 1.58154 val_loss: 67.89144 val_mae: 1.08701 val_fs: 0.73052 val_mof: 1.48559 lr: 0.000001 elapsed: 152\n",
      "E 114/200 tr_loss: 43.52539 tr_mae: 1.24549 tr_fs: 0.78479 tr_mof: 1.58707 val_loss: 67.89168 val_mae: 1.08410 val_fs: 0.73453 val_mof: 1.47344 lr: 0.000000 elapsed: 152\n",
      "E 115/200 tr_loss: 43.52576 tr_mae: 1.24460 tr_fs: 0.78499 tr_mof: 1.58547 val_loss: 67.89144 val_mae: 1.08497 val_fs: 0.73178 val_mof: 1.48019 lr: 0.000000 elapsed: 152\n",
      "E 116/200 tr_loss: 43.95974 tr_mae: 1.24135 tr_fs: 0.78498 tr_mof: 1.58111 val_loss: 67.89155 val_mae: 1.08544 val_fs: 0.73200 val_mof: 1.48038 lr: 0.000000 elapsed: 152\n",
      "E 117/200 tr_loss: 44.67815 tr_mae: 1.24289 tr_fs: 0.78524 tr_mof: 1.58279 val_loss: 67.89141 val_mae: 1.08717 val_fs: 0.73016 val_mof: 1.48651 lr: 0.000000 elapsed: 152\n",
      "E 118/200 tr_loss: 43.52571 tr_mae: 1.24405 tr_fs: 0.78515 tr_mof: 1.58421 val_loss: 67.89132 val_mae: 1.08644 val_fs: 0.73004 val_mof: 1.48575 lr: 0.000000 elapsed: 152\n",
      "E 119/200 tr_loss: 43.52546 tr_mae: 1.24195 tr_fs: 0.78545 tr_mof: 1.58126 val_loss: 67.89149 val_mae: 1.08505 val_fs: 0.73243 val_mof: 1.47900 lr: 0.000000 elapsed: 151\n",
      "E 120/200 tr_loss: 43.52545 tr_mae: 1.24201 tr_fs: 0.78516 tr_mof: 1.58170 val_loss: 67.89160 val_mae: 1.08632 val_fs: 0.73233 val_mof: 1.48092 lr: 0.000000 elapsed: 152\n",
      "E 121/200 tr_loss: 43.75564 tr_mae: 1.24277 tr_fs: 0.78515 tr_mof: 1.58256 val_loss: 67.89159 val_mae: 1.08435 val_fs: 0.73350 val_mof: 1.47591 lr: 0.000000 elapsed: 152\n",
      "E 122/200 tr_loss: 43.52554 tr_mae: 1.24404 tr_fs: 0.78507 tr_mof: 1.58463 val_loss: 67.89157 val_mae: 1.08519 val_fs: 0.73260 val_mof: 1.47882 lr: 0.000000 elapsed: 152\n",
      "E 123/200 tr_loss: 43.52584 tr_mae: 1.24333 tr_fs: 0.78525 tr_mof: 1.58291 val_loss: 67.89124 val_mae: 1.08629 val_fs: 0.72936 val_mof: 1.48689 lr: 0.000000 elapsed: 152\n",
      "E 124/200 tr_loss: 43.52582 tr_mae: 1.24394 tr_fs: 0.78542 tr_mof: 1.58346 val_loss: 67.89127 val_mae: 1.08885 val_fs: 0.72821 val_mof: 1.49277 lr: 0.000000 elapsed: 152\n",
      "E 125/200 tr_loss: 43.52537 tr_mae: 1.24194 tr_fs: 0.78512 tr_mof: 1.58167 val_loss: 67.89154 val_mae: 1.08492 val_fs: 0.73307 val_mof: 1.47749 lr: 0.000000 elapsed: 151\n",
      "E 126/200 tr_loss: 44.10183 tr_mae: 1.24373 tr_fs: 0.78515 tr_mof: 1.58379 val_loss: 67.89165 val_mae: 1.08385 val_fs: 0.73444 val_mof: 1.47329 lr: 0.000000 elapsed: 151\n",
      "E 127/200 tr_loss: 44.67796 tr_mae: 1.24096 tr_fs: 0.78515 tr_mof: 1.58022 val_loss: 67.89179 val_mae: 1.08666 val_fs: 0.73336 val_mof: 1.47932 lr: 0.000000 elapsed: 152\n",
      "E 128/200 tr_loss: 43.52544 tr_mae: 1.24148 tr_fs: 0.78546 tr_mof: 1.58056 val_loss: 67.89159 val_mae: 1.08680 val_fs: 0.73177 val_mof: 1.48273 lr: 0.000000 elapsed: 152\n",
      "E 129/200 tr_loss: 43.52536 tr_mae: 1.24197 tr_fs: 0.78524 tr_mof: 1.58179 val_loss: 67.89184 val_mae: 1.08356 val_fs: 0.73550 val_mof: 1.47078 lr: 0.000000 elapsed: 152\n",
      "E 130/200 tr_loss: 43.52551 tr_mae: 1.24256 tr_fs: 0.78546 tr_mof: 1.58178 val_loss: 67.89149 val_mae: 1.08660 val_fs: 0.73131 val_mof: 1.48336 lr: 0.000000 elapsed: 152\n",
      "E 131/200 tr_loss: 44.10170 tr_mae: 1.24129 tr_fs: 0.78507 tr_mof: 1.58085 val_loss: 67.89150 val_mae: 1.08484 val_fs: 0.73220 val_mof: 1.47917 lr: 0.000000 elapsed: 152\n",
      "E 132/200 tr_loss: 43.76499 tr_mae: 1.24232 tr_fs: 0.78527 tr_mof: 1.58187 val_loss: 67.89153 val_mae: 1.08679 val_fs: 0.73139 val_mof: 1.48348 lr: 0.000000 elapsed: 152\n",
      "E 133/200 tr_loss: 43.52535 tr_mae: 1.23953 tr_fs: 0.78561 tr_mof: 1.57751 val_loss: 67.89139 val_mae: 1.08578 val_fs: 0.73091 val_mof: 1.48310 lr: 0.000000 elapsed: 152\n",
      "E 134/200 tr_loss: 44.67813 tr_mae: 1.24314 tr_fs: 0.78520 tr_mof: 1.58291 val_loss: 67.89141 val_mae: 1.08436 val_fs: 0.73202 val_mof: 1.47884 lr: 0.000000 elapsed: 152\n",
      "E 135/200 tr_loss: 43.52556 tr_mae: 1.24323 tr_fs: 0.78505 tr_mof: 1.58357 val_loss: 67.89146 val_mae: 1.08216 val_fs: 0.73370 val_mof: 1.47249 lr: 0.000000 elapsed: 152\n",
      "E 136/200 tr_loss: 43.52560 tr_mae: 1.24431 tr_fs: 0.78511 tr_mof: 1.58496 val_loss: 67.89144 val_mae: 1.08548 val_fs: 0.73147 val_mof: 1.48147 lr: 0.000000 elapsed: 152\n",
      "E 137/200 tr_loss: 44.10186 tr_mae: 1.24379 tr_fs: 0.78507 tr_mof: 1.58408 val_loss: 67.89134 val_mae: 1.08590 val_fs: 0.73043 val_mof: 1.48418 lr: 0.000000 elapsed: 152\n",
      "E 138/200 tr_loss: 43.52536 tr_mae: 1.24189 tr_fs: 0.78497 tr_mof: 1.58208 val_loss: 67.89176 val_mae: 1.08487 val_fs: 0.73448 val_mof: 1.47457 lr: 0.000000 elapsed: 152\n",
      "E 139/200 tr_loss: 44.28788 tr_mae: 1.24071 tr_fs: 0.78518 tr_mof: 1.57992 val_loss: 67.89161 val_mae: 1.08768 val_fs: 0.73113 val_mof: 1.48519 lr: 0.000000 elapsed: 152\n",
      "E 140/200 tr_loss: 43.52588 tr_mae: 1.24635 tr_fs: 0.78524 tr_mof: 1.58703 val_loss: 67.89129 val_mae: 1.08557 val_fs: 0.73032 val_mof: 1.48392 lr: 0.000000 elapsed: 152\n",
      "E 141/200 tr_loss: 44.10169 tr_mae: 1.24045 tr_fs: 0.78499 tr_mof: 1.57992 val_loss: 67.89166 val_mae: 1.08620 val_fs: 0.73309 val_mof: 1.47921 lr: 0.000000 elapsed: 152\n",
      "E 142/200 tr_loss: 44.10169 tr_mae: 1.24253 tr_fs: 0.78516 tr_mof: 1.58241 val_loss: 67.89163 val_mae: 1.08594 val_fs: 0.73270 val_mof: 1.47965 lr: 0.000000 elapsed: 152\n",
      "E 143/200 tr_loss: 43.96283 tr_mae: 1.24407 tr_fs: 0.78503 tr_mof: 1.58444 val_loss: 67.89131 val_mae: 1.08377 val_fs: 0.73174 val_mof: 1.47860 lr: 0.000000 elapsed: 151\n",
      "E 144/200 tr_loss: 43.52531 tr_mae: 1.24102 tr_fs: 0.78517 tr_mof: 1.58040 val_loss: 67.89162 val_mae: 1.08468 val_fs: 0.73346 val_mof: 1.47636 lr: 0.000000 elapsed: 152\n",
      "E 145/200 tr_loss: 43.52537 tr_mae: 1.24242 tr_fs: 0.78465 tr_mof: 1.58322 val_loss: 67.89176 val_mae: 1.08462 val_fs: 0.73399 val_mof: 1.47525 lr: 0.000000 elapsed: 152\n",
      "E 146/200 tr_loss: 43.52549 tr_mae: 1.24334 tr_fs: 0.78499 tr_mof: 1.58395 val_loss: 67.89135 val_mae: 1.08440 val_fs: 0.73165 val_mof: 1.47965 lr: 0.000000 elapsed: 151\n",
      "E 147/200 tr_loss: 43.52549 tr_mae: 1.24242 tr_fs: 0.78509 tr_mof: 1.58249 val_loss: 67.89137 val_mae: 1.08587 val_fs: 0.73062 val_mof: 1.48374 lr: 0.000000 elapsed: 152\n",
      "E 148/200 tr_loss: 44.67823 tr_mae: 1.24419 tr_fs: 0.78520 tr_mof: 1.58447 val_loss: 67.89162 val_mae: 1.08653 val_fs: 0.73202 val_mof: 1.48183 lr: 0.000000 elapsed: 152\n",
      "E 149/200 tr_loss: 44.10156 tr_mae: 1.24020 tr_fs: 0.78503 tr_mof: 1.57968 val_loss: 67.89230 val_mae: 1.08856 val_fs: 0.73634 val_mof: 1.47586 lr: 0.000000 elapsed: 152\n",
      "E 150/200 tr_loss: 43.52527 tr_mae: 1.24196 tr_fs: 0.78412 tr_mof: 1.58395 val_loss: 67.89267 val_mae: 1.08879 val_fs: 0.73781 val_mof: 1.47325 lr: 0.000000 elapsed: 152\n",
      "E 151/200 tr_loss: 43.52529 tr_mae: 1.23970 tr_fs: 0.78494 tr_mof: 1.57913 val_loss: 67.89161 val_mae: 1.08642 val_fs: 0.73242 val_mof: 1.48089 lr: 0.000000 elapsed: 152\n",
      "E 152/200 tr_loss: 44.10178 tr_mae: 1.24296 tr_fs: 0.78527 tr_mof: 1.58274 val_loss: 67.89181 val_mae: 1.08677 val_fs: 0.73390 val_mof: 1.47834 lr: 0.000000 elapsed: 152\n",
      "E 153/200 tr_loss: 43.52534 tr_mae: 1.24162 tr_fs: 0.78500 tr_mof: 1.58180 val_loss: 67.89181 val_mae: 1.08811 val_fs: 0.73346 val_mof: 1.48109 lr: 0.000000 elapsed: 152\n",
      "E 154/200 tr_loss: 44.10173 tr_mae: 1.24230 tr_fs: 0.78508 tr_mof: 1.58226 val_loss: 67.89139 val_mae: 1.08578 val_fs: 0.73129 val_mof: 1.48231 lr: 0.000000 elapsed: 151\n",
      "E 155/200 tr_loss: 44.10161 tr_mae: 1.24198 tr_fs: 0.78503 tr_mof: 1.58194 val_loss: 67.89193 val_mae: 1.08799 val_fs: 0.73375 val_mof: 1.48033 lr: 0.000000 elapsed: 152\n",
      "E 156/200 tr_loss: 43.52537 tr_mae: 1.24114 tr_fs: 0.78529 tr_mof: 1.58009 val_loss: 67.89174 val_mae: 1.08626 val_fs: 0.73354 val_mof: 1.47836 lr: 0.000000 elapsed: 152\n",
      "E 157/200 tr_loss: 43.52540 tr_mae: 1.24202 tr_fs: 0.78529 tr_mof: 1.58159 val_loss: 67.89163 val_mae: 1.08639 val_fs: 0.73219 val_mof: 1.48128 lr: 0.000000 elapsed: 152\n",
      "E 158/200 tr_loss: 44.10181 tr_mae: 1.24483 tr_fs: 0.78522 tr_mof: 1.58538 val_loss: 67.89159 val_mae: 1.08539 val_fs: 0.73317 val_mof: 1.47796 lr: 0.000000 elapsed: 152\n",
      "E 159/200 tr_loss: 43.52530 tr_mae: 1.24031 tr_fs: 0.78501 tr_mof: 1.57974 val_loss: 67.89177 val_mae: 1.08428 val_fs: 0.73476 val_mof: 1.47326 lr: 0.000000 elapsed: 152\n",
      "E 160/200 tr_loss: 45.25424 tr_mae: 1.23993 tr_fs: 0.78487 tr_mof: 1.57927 val_loss: 67.89181 val_mae: 1.09025 val_fs: 0.73089 val_mof: 1.48923 lr: 0.000000 elapsed: 152\n",
      "E 161/200 tr_loss: 43.74468 tr_mae: 1.24274 tr_fs: 0.78515 tr_mof: 1.58262 val_loss: 67.89149 val_mae: 1.08341 val_fs: 0.73285 val_mof: 1.47585 lr: 0.000000 elapsed: 152\n",
      "E 162/200 tr_loss: 43.52548 tr_mae: 1.24282 tr_fs: 0.78534 tr_mof: 1.58246 val_loss: 67.89166 val_mae: 1.08813 val_fs: 0.73124 val_mof: 1.48561 lr: 0.000000 elapsed: 152\n",
      "E 163/200 tr_loss: 43.75994 tr_mae: 1.24029 tr_fs: 0.78517 tr_mof: 1.57930 val_loss: 67.89143 val_mae: 1.08388 val_fs: 0.73214 val_mof: 1.47792 lr: 0.000000 elapsed: 152\n",
      "E 164/200 tr_loss: 44.48551 tr_mae: 1.24063 tr_fs: 0.78544 tr_mof: 1.57906 val_loss: 67.89140 val_mae: 1.08585 val_fs: 0.73058 val_mof: 1.48379 lr: 0.000000 elapsed: 152\n",
      "E 165/200 tr_loss: 43.52557 tr_mae: 1.24408 tr_fs: 0.78511 tr_mof: 1.58453 val_loss: 67.89135 val_mae: 1.08245 val_fs: 0.73361 val_mof: 1.47300 lr: 0.000000 elapsed: 152\n",
      "E 166/200 tr_loss: 44.10183 tr_mae: 1.24129 tr_fs: 0.78509 tr_mof: 1.58075 val_loss: 67.89145 val_mae: 1.08486 val_fs: 0.73162 val_mof: 1.48029 lr: 0.000000 elapsed: 152\n",
      "E 167/200 tr_loss: 44.10154 tr_mae: 1.23916 tr_fs: 0.78477 tr_mof: 1.57875 val_loss: 67.89193 val_mae: 1.08787 val_fs: 0.73371 val_mof: 1.48027 lr: 0.000000 elapsed: 152\n",
      "E 168/200 tr_loss: 43.52549 tr_mae: 1.24328 tr_fs: 0.78524 tr_mof: 1.58315 val_loss: 67.89153 val_mae: 1.08207 val_fs: 0.73462 val_mof: 1.47051 lr: 0.000000 elapsed: 152\n",
      "E 169/200 tr_loss: 43.52563 tr_mae: 1.24402 tr_fs: 0.78549 tr_mof: 1.58381 val_loss: 67.89134 val_mae: 1.08526 val_fs: 0.73132 val_mof: 1.48149 lr: 0.000000 elapsed: 152\n",
      "E 170/200 tr_loss: 43.52556 tr_mae: 1.24281 tr_fs: 0.78547 tr_mof: 1.58229 val_loss: 67.89154 val_mae: 1.08604 val_fs: 0.73189 val_mof: 1.48141 lr: 0.000000 elapsed: 151\n",
      "E 171/200 tr_loss: 43.52553 tr_mae: 1.24259 tr_fs: 0.78494 tr_mof: 1.58286 val_loss: 67.89158 val_mae: 1.08468 val_fs: 0.73340 val_mof: 1.47653 lr: 0.000000 elapsed: 152\n",
      "E 172/200 tr_loss: 43.90387 tr_mae: 1.24128 tr_fs: 0.78513 tr_mof: 1.58083 val_loss: 67.89191 val_mae: 1.08658 val_fs: 0.73480 val_mof: 1.47628 lr: 0.000000 elapsed: 152\n",
      "E 173/200 tr_loss: 43.52541 tr_mae: 1.24019 tr_fs: 0.78481 tr_mof: 1.57995 val_loss: 67.89156 val_mae: 1.08435 val_fs: 0.73340 val_mof: 1.47602 lr: 0.000000 elapsed: 152\n",
      "E 174/200 tr_loss: 44.10158 tr_mae: 1.24012 tr_fs: 0.78487 tr_mof: 1.57991 val_loss: 67.89152 val_mae: 1.08482 val_fs: 0.73271 val_mof: 1.47808 lr: 0.000000 elapsed: 152\n",
      "E 175/200 tr_loss: 43.52538 tr_mae: 1.24143 tr_fs: 0.78505 tr_mof: 1.58125 val_loss: 67.89167 val_mae: 1.08483 val_fs: 0.73339 val_mof: 1.47674 lr: 0.000000 elapsed: 152\n",
      "E 176/200 tr_loss: 43.52539 tr_mae: 1.24278 tr_fs: 0.78525 tr_mof: 1.58234 val_loss: 67.89152 val_mae: 1.08368 val_fs: 0.73328 val_mof: 1.47536 lr: 0.000000 elapsed: 152\n",
      "E 177/200 tr_loss: 44.07425 tr_mae: 1.24151 tr_fs: 0.78499 tr_mof: 1.58143 val_loss: 67.89164 val_mae: 1.08392 val_fs: 0.73408 val_mof: 1.47411 lr: 0.000000 elapsed: 152\n",
      "E 178/200 tr_loss: 44.05768 tr_mae: 1.24127 tr_fs: 0.78524 tr_mof: 1.58047 val_loss: 67.89169 val_mae: 1.08530 val_fs: 0.73333 val_mof: 1.47750 lr: 0.000000 elapsed: 152\n",
      "E 179/200 tr_loss: 43.52539 tr_mae: 1.24206 tr_fs: 0.78499 tr_mof: 1.58220 val_loss: 67.89148 val_mae: 1.08598 val_fs: 0.73177 val_mof: 1.48158 lr: 0.000000 elapsed: 152\n",
      "E 180/200 tr_loss: 43.52541 tr_mae: 1.23970 tr_fs: 0.78529 tr_mof: 1.57837 val_loss: 67.89160 val_mae: 1.08897 val_fs: 0.73069 val_mof: 1.48790 lr: 0.000000 elapsed: 152\n",
      "E 181/200 tr_loss: 44.10159 tr_mae: 1.23978 tr_fs: 0.78490 tr_mof: 1.57914 val_loss: 67.89203 val_mae: 1.08733 val_fs: 0.73418 val_mof: 1.47858 lr: 0.000000 elapsed: 152\n",
      "E 182/200 tr_loss: 43.52546 tr_mae: 1.24251 tr_fs: 0.78540 tr_mof: 1.58212 val_loss: 67.89140 val_mae: 1.08826 val_fs: 0.72919 val_mof: 1.49000 lr: 0.000000 elapsed: 152\n",
      "E 183/200 tr_loss: 43.52540 tr_mae: 1.24256 tr_fs: 0.78497 tr_mof: 1.58280 val_loss: 67.89150 val_mae: 1.08580 val_fs: 0.73198 val_mof: 1.48088 lr: 0.000000 elapsed: 151\n",
      "E 184/200 tr_loss: 43.52571 tr_mae: 1.24355 tr_fs: 0.78517 tr_mof: 1.58364 val_loss: 67.89130 val_mae: 1.08707 val_fs: 0.72981 val_mof: 1.48702 lr: 0.000000 elapsed: 151\n",
      "E 185/200 tr_loss: 44.10170 tr_mae: 1.24184 tr_fs: 0.78516 tr_mof: 1.58154 val_loss: 67.89168 val_mae: 1.08653 val_fs: 0.73249 val_mof: 1.48085 lr: 0.000000 elapsed: 152\n",
      "E 186/200 tr_loss: 43.90401 tr_mae: 1.24286 tr_fs: 0.78524 tr_mof: 1.58295 val_loss: 67.89157 val_mae: 1.08709 val_fs: 0.73169 val_mof: 1.48327 lr: 0.000000 elapsed: 152\n",
      "E 187/200 tr_loss: 44.66071 tr_mae: 1.24145 tr_fs: 0.78523 tr_mof: 1.58079 val_loss: 67.89145 val_mae: 1.08818 val_fs: 0.72981 val_mof: 1.48862 lr: 0.000000 elapsed: 152\n",
      "E 188/200 tr_loss: 43.52536 tr_mae: 1.24112 tr_fs: 0.78499 tr_mof: 1.58093 val_loss: 67.89192 val_mae: 1.08848 val_fs: 0.73332 val_mof: 1.48189 lr: 0.000000 elapsed: 151\n",
      "E 189/200 tr_loss: 43.52544 tr_mae: 1.24365 tr_fs: 0.78533 tr_mof: 1.58361 val_loss: 67.89140 val_mae: 1.08498 val_fs: 0.73158 val_mof: 1.48061 lr: 0.000000 elapsed: 152\n",
      "E 190/200 tr_loss: 43.52580 tr_mae: 1.24722 tr_fs: 0.78508 tr_mof: 1.58854 val_loss: 67.89127 val_mae: 1.08420 val_fs: 0.73160 val_mof: 1.47946 lr: 0.000000 elapsed: 152\n",
      "E 191/200 tr_loss: 43.52554 tr_mae: 1.24398 tr_fs: 0.78529 tr_mof: 1.58401 val_loss: 67.89143 val_mae: 1.08502 val_fs: 0.73166 val_mof: 1.48048 lr: 0.000000 elapsed: 152\n",
      "E 192/200 tr_loss: 43.52558 tr_mae: 1.24245 tr_fs: 0.78524 tr_mof: 1.58210 val_loss: 67.89144 val_mae: 1.08534 val_fs: 0.73180 val_mof: 1.48066 lr: 0.000000 elapsed: 152\n",
      "E 193/200 tr_loss: 44.41873 tr_mae: 1.24368 tr_fs: 0.78495 tr_mof: 1.58413 val_loss: 67.89149 val_mae: 1.08541 val_fs: 0.73197 val_mof: 1.48037 lr: 0.000000 elapsed: 152\n",
      "E 194/200 tr_loss: 43.52541 tr_mae: 1.24220 tr_fs: 0.78544 tr_mof: 1.58132 val_loss: 67.89157 val_mae: 1.08431 val_fs: 0.73323 val_mof: 1.47631 lr: 0.000000 elapsed: 151\n",
      "E 195/200 tr_loss: 44.38187 tr_mae: 1.24640 tr_fs: 0.78504 tr_mof: 1.58762 val_loss: 67.89130 val_mae: 1.08728 val_fs: 0.72930 val_mof: 1.48841 lr: 0.000000 elapsed: 151\n",
      "E 196/200 tr_loss: 43.52553 tr_mae: 1.24178 tr_fs: 0.78514 tr_mof: 1.58134 val_loss: 67.89144 val_mae: 1.08382 val_fs: 0.73242 val_mof: 1.47729 lr: 0.000000 elapsed: 152\n",
      "E 197/200 tr_loss: 44.10162 tr_mae: 1.24047 tr_fs: 0.78501 tr_mof: 1.58004 val_loss: 67.89179 val_mae: 1.08692 val_fs: 0.73376 val_mof: 1.47883 lr: 0.000000 elapsed: 152\n",
      "E 198/200 tr_loss: 43.52542 tr_mae: 1.24276 tr_fs: 0.78510 tr_mof: 1.58277 val_loss: 67.89142 val_mae: 1.08406 val_fs: 0.73273 val_mof: 1.47705 lr: 0.000000 elapsed: 151\n",
      "E 199/200 tr_loss: 44.67805 tr_mae: 1.24264 tr_fs: 0.78523 tr_mof: 1.58268 val_loss: 67.89143 val_mae: 1.08557 val_fs: 0.73155 val_mof: 1.48146 lr: 0.000000 elapsed: 152\n",
      "E 200/200 tr_loss: 43.52529 tr_mae: 1.24092 tr_fs: 0.78529 tr_mof: 1.58016 val_loss: 67.89201 val_mae: 1.08617 val_fs: 0.73569 val_mof: 1.47394 lr: 0.000000 elapsed: 152\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr, weight_decay=0.00025)\n",
    "# optimizer = AdamW(model.parameters(), 2.5e-4, weight_decay=0.000025)\n",
    "#optimizer = optim.SGD(model.parameters(), args.lr, momentum=0.9, weight_decay=0.025)\n",
    "\n",
    "###### SCHEDULER #######\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n",
    "\n",
    "#eta_min = 0.00001\n",
    "#T_max = 10\n",
    "#T_mult = 1\n",
    "#restart_decay = 0.97\n",
    "#scheduler = CosineAnnealingWithRestartsLR(optimizer, T_max=T_max, eta_min=eta_min, T_mult=T_mult, restart_decay=restart_decay)\n",
    "\n",
    "#scheduler = StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss() \n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "def to_numpy(t):\n",
    "    return t.cpu().detach().numpy()\n",
    "\n",
    "best_mae_score = 999\n",
    "best_f_score = 999\n",
    "best_mof_score = 999\n",
    "grad_clip_step = 100\n",
    "grad_clip = 100\n",
    "step = 0\n",
    "# accumulation_step = 2\n",
    "EPOCH = 200\n",
    "\n",
    "model_fname = '../D_WEATHER/weight/unetx2_ch9_shuffle_ho0.967.pt'\n",
    "# log file\n",
    "log_df = pd.DataFrame(columns=['epoch_idx', 'train_loss', 'train_mae', 'train_fs', 'train_mof', 'valid_loss', 'valid_mae', 'valid_fs', 'valid_mof'])\n",
    "\n",
    "print(\"start training\")\n",
    "\n",
    "for epoch_idx in range(1, EPOCH + 1):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss = 0\n",
    "    train_mae = 0\n",
    "    train_fs = 0\n",
    "    train_mof = 0 \n",
    "#     train_total_correct = 0\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for batch_idx, (image, labels) in enumerate(train_loader):\n",
    "        if use_gpu:\n",
    "            image = image.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "        output = model(image)\n",
    "        loss = criterion(output, labels)\n",
    "        mae_score = mae(labels.cpu(), output.cpu())\n",
    "        f_score = fscore(labels.cpu(), output.cpu())\n",
    "        mof_score = maeOverFscore(labels.cpu(), output.cpu())\n",
    "\n",
    "        # gradient explosion prevention\n",
    "        if step > grad_clip_step:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "\n",
    "        step += 1\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        train_loss += loss.item() / len(train_loader)\n",
    "        train_mae += mae_score.item() / len(train_loader)\n",
    "        train_fs += f_score.item() / len(train_loader)\n",
    "        train_mof += mof_score.item() / len(train_loader)\n",
    "\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    valid_mae = 0\n",
    "    valid_fs = 0\n",
    "    valid_mof = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (image, labels) in enumerate(valid_loader):\n",
    "            if use_gpu:\n",
    "                image = image.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "            output = model(image)\n",
    "            loss = criterion(output, labels)\n",
    "            mae_score = mae(labels.cpu(), output.cpu())\n",
    "            f_score = fscore(labels.cpu(), output.cpu())\n",
    "            mof_score = maeOverFscore(labels.cpu(), output.cpu())\n",
    "\n",
    "#             output_prob = F.sigmoid(output)\n",
    "\n",
    "            predict_vector = to_numpy(output)\n",
    "\n",
    "            valid_loss += loss.item() / len(valid_loader)\n",
    "            valid_mae += mae_score.item() / len(valid_loader)\n",
    "            valid_fs += f_score.item() / len(valid_loader)\n",
    "            valid_mof += mof_score.item() / len(valid_loader)\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "    # checkpoint\n",
    "    if valid_mof < best_mof_score:\n",
    "        best_mof_score = valid_mof\n",
    "#         print(\"Improved !! \")\n",
    "        torch.save(model.state_dict(), model_fname)\n",
    "        print(\"================ ༼ つ ◕_◕ ༽つ BEST epoch : {}, MOF : {} \".format(epoch_idx, best_mof_score))\n",
    "        #file_save_name = 'best_acc' + '_' + str(num_fold)\n",
    "        #print(file_save_name)\n",
    "#     else:\n",
    "#         print(\"val acc has not improved\")\n",
    "\n",
    "    lr = [_['lr'] for _ in optimizer.param_groups]\n",
    "\n",
    "    #if args.scheduler == 'plateau':\n",
    "    scheduler.step(valid_mof)\n",
    "    #else:\n",
    "    #    scheduler.step()\n",
    "\n",
    "    # nsml.save(epoch_idx)\n",
    "\n",
    "    print(\"E {}/{} tr_loss: {:.5f} tr_mae: {:.5f} tr_fs: {:.5f} tr_mof: {:.5f} val_loss: {:.5f} val_mae: {:.5f} val_fs: {:.5f} val_mof: {:.5f} lr: {:.6f} elapsed: {:.0f}\".format(\n",
    "           epoch_idx, EPOCH, train_loss, train_mae, train_fs, train_mof, valid_loss, valid_mae, valid_fs, valid_mof, lr[0], elapsed))\n",
    "            #epoch_idx, args.epochs, train_loss, valid_loss, val_acc, lr[0], elapsed\n",
    "    # log file element\n",
    "#     log = []\n",
    "    log_data = [epoch_idx, train_loss, train_mae, train_fs, train_mof, valid_loss, valid_mae, valid_fs, valid_mof]\n",
    "#     log.append(log_data)\n",
    "    log_df.loc[epoch_idx] = log_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df.to_csv(\"../D_WEATHER/log/unetx2_ch9_shuffle_ho0.967.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test_Dataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        \n",
    "        self.image_list = []\n",
    "#         self.label_list = []\n",
    "\n",
    "        for file in self.df['path']:\n",
    "            data = np.load(file)\n",
    "#             image = data[:,:,:]\n",
    "            image = data[:,:,:9]#.reshape(40,40,-1)\n",
    "            image = np.transpose(image, (2,0,1))\n",
    "            image = image.astype(np.float32)\n",
    "            self.image_list.append(image)\n",
    "            \n",
    "#             label = data[:,:,-1].reshape(-1)\n",
    "#             self.label_list.append(label)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        image = self.image_list[idx]\n",
    "#         label = self.label_list[idx]\n",
    "        \n",
    "        return image#, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = build_te_dataloader(te_df, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2416, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loader.dataset.df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 40, 40)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 40, 40)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loader.dataset[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict values check :  [-1.45988958e-03 -2.75963498e-03 -3.00486805e-04 ... -2.01881267e-06\n",
      "  1.17579475e-06  4.57767674e-05]\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(model_fname))\n",
    "model.eval()\n",
    "predictions = np.zeros((len(test_loader.dataset), 1600))\n",
    "with torch.no_grad():\n",
    "    for i, image in enumerate(test_loader):\n",
    "        image = image.to(device)\n",
    "        output = model(image)\n",
    "        \n",
    "        predictions[i*batch_size: (i+1)*batch_size] = output.detach().cpu().numpy().reshape(-1, 1600)\n",
    "print(\"predict values check : \",predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2416, 1600)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.45988958e-03, -2.75963498e-03, -3.00486805e-04, ...,\n",
       "       -2.01881267e-06,  1.17579475e-06,  4.57767674e-05])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv(\"../D_WEATHER/input/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>px_1</th>\n",
       "      <th>px_2</th>\n",
       "      <th>px_3</th>\n",
       "      <th>px_4</th>\n",
       "      <th>px_5</th>\n",
       "      <th>px_6</th>\n",
       "      <th>px_7</th>\n",
       "      <th>px_8</th>\n",
       "      <th>px_9</th>\n",
       "      <th>...</th>\n",
       "      <th>px_1591</th>\n",
       "      <th>px_1592</th>\n",
       "      <th>px_1593</th>\n",
       "      <th>px_1594</th>\n",
       "      <th>px_1595</th>\n",
       "      <th>px_1596</th>\n",
       "      <th>px_1597</th>\n",
       "      <th>px_1598</th>\n",
       "      <th>px_1599</th>\n",
       "      <th>px_1600</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>029858_01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>029858_02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>029858_03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>029858_05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>029858_07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1601 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  px_1  px_2  px_3  px_4  px_5  px_6  px_7  px_8  px_9  ...  \\\n",
       "0  029858_01   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "1  029858_02   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "2  029858_03   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "3  029858_05   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "4  029858_07   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "\n",
       "   px_1591  px_1592  px_1593  px_1594  px_1595  px_1596  px_1597  px_1598  \\\n",
       "0      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "1      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "2      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "3      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "4      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "   px_1599  px_1600  \n",
       "0      0.0      0.0  \n",
       "1      0.0      0.0  \n",
       "2      0.0      0.0  \n",
       "3      0.0      0.0  \n",
       "4      0.0      0.0  \n",
       "\n",
       "[5 rows x 1601 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.iloc[:,1:] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>px_1</th>\n",
       "      <th>px_2</th>\n",
       "      <th>px_3</th>\n",
       "      <th>px_4</th>\n",
       "      <th>px_5</th>\n",
       "      <th>px_6</th>\n",
       "      <th>px_7</th>\n",
       "      <th>px_8</th>\n",
       "      <th>px_9</th>\n",
       "      <th>...</th>\n",
       "      <th>px_1591</th>\n",
       "      <th>px_1592</th>\n",
       "      <th>px_1593</th>\n",
       "      <th>px_1594</th>\n",
       "      <th>px_1595</th>\n",
       "      <th>px_1596</th>\n",
       "      <th>px_1597</th>\n",
       "      <th>px_1598</th>\n",
       "      <th>px_1599</th>\n",
       "      <th>px_1600</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>029858_01</td>\n",
       "      <td>-0.001460</td>\n",
       "      <td>-0.002760</td>\n",
       "      <td>-0.000300</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>-0.000465</td>\n",
       "      <td>-0.003905</td>\n",
       "      <td>0.003824</td>\n",
       "      <td>0.008850</td>\n",
       "      <td>0.001254</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>029858_02</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>029858_03</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.001456</td>\n",
       "      <td>0.075137</td>\n",
       "      <td>0.075360</td>\n",
       "      <td>0.007247</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>029858_05</td>\n",
       "      <td>-0.000056</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>029858_07</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>...</td>\n",
       "      <td>1.545871</td>\n",
       "      <td>2.178585</td>\n",
       "      <td>1.545741</td>\n",
       "      <td>0.642400</td>\n",
       "      <td>0.609874</td>\n",
       "      <td>1.825723</td>\n",
       "      <td>4.686034</td>\n",
       "      <td>4.102203</td>\n",
       "      <td>2.479179</td>\n",
       "      <td>1.916396</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1601 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id      px_1      px_2      px_3      px_4      px_5      px_6  \\\n",
       "0  029858_01 -0.001460 -0.002760 -0.000300  0.000060 -0.000465 -0.003905   \n",
       "1  029858_02 -0.000021  0.000004  0.000014  0.000014  0.000014  0.000014   \n",
       "2  029858_03  0.000014  0.000051  0.001456  0.075137  0.075360  0.007247   \n",
       "3  029858_05 -0.000056 -0.000006  0.000014  0.000014  0.000014  0.000014   \n",
       "4  029858_07  0.000014  0.000014  0.000014  0.000014  0.000014  0.000014   \n",
       "\n",
       "       px_7      px_8      px_9  ...   px_1591   px_1592   px_1593   px_1594  \\\n",
       "0  0.003824  0.008850  0.001254  ...  0.000015  0.000015  0.000013  0.000011   \n",
       "1  0.000014  0.000014  0.000015  ...  0.000014  0.000014  0.000014  0.000014   \n",
       "2  0.000122  0.000015  0.000015  ...  0.000014  0.000014  0.000014  0.000013   \n",
       "3  0.000014  0.000014  0.000014  ...  0.000014  0.000014  0.000014  0.000014   \n",
       "4  0.000014  0.000014  0.000014  ...  1.545871  2.178585  1.545741  0.642400   \n",
       "\n",
       "    px_1595   px_1596   px_1597   px_1598   px_1599   px_1600  \n",
       "0  0.000013  0.000013  0.000015 -0.000002  0.000001  0.000046  \n",
       "1  0.000014  0.000014  0.000020  0.000011  0.000005  0.000016  \n",
       "2  0.000014  0.000013  0.000021  0.000013  0.000005  0.000012  \n",
       "3  0.000014  0.000013  0.000016  0.000011  0.000010  0.000017  \n",
       "4  0.609874  1.825723  4.686034  4.102203  2.479179  1.916396  \n",
       "\n",
       "[5 rows x 1601 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('../D_WEATHER/sub/unetx2_ch9_shuffle_ho0.967.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sub = sub.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1600/1600 [00:01<00:00, 1332.72it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm.tqdm(range(1,1601)):\n",
    "    new_sub.loc[new_sub[new_sub.columns[i]]<0, new_sub.columns[i]] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>px_1</th>\n",
       "      <th>px_2</th>\n",
       "      <th>px_3</th>\n",
       "      <th>px_4</th>\n",
       "      <th>px_5</th>\n",
       "      <th>px_6</th>\n",
       "      <th>px_7</th>\n",
       "      <th>px_8</th>\n",
       "      <th>px_9</th>\n",
       "      <th>px_10</th>\n",
       "      <th>...</th>\n",
       "      <th>px_1591</th>\n",
       "      <th>px_1592</th>\n",
       "      <th>px_1593</th>\n",
       "      <th>px_1594</th>\n",
       "      <th>px_1595</th>\n",
       "      <th>px_1596</th>\n",
       "      <th>px_1597</th>\n",
       "      <th>px_1598</th>\n",
       "      <th>px_1599</th>\n",
       "      <th>px_1600</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.104913</td>\n",
       "      <td>0.111849</td>\n",
       "      <td>0.117341</td>\n",
       "      <td>0.139682</td>\n",
       "      <td>0.154402</td>\n",
       "      <td>0.160311</td>\n",
       "      <td>0.155682</td>\n",
       "      <td>0.158670</td>\n",
       "      <td>0.150576</td>\n",
       "      <td>0.136430</td>\n",
       "      <td>...</td>\n",
       "      <td>0.141868</td>\n",
       "      <td>0.131865</td>\n",
       "      <td>0.137651</td>\n",
       "      <td>0.132617</td>\n",
       "      <td>0.124986</td>\n",
       "      <td>0.129675</td>\n",
       "      <td>0.127915</td>\n",
       "      <td>0.124093</td>\n",
       "      <td>0.134919</td>\n",
       "      <td>0.118554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.709942</td>\n",
       "      <td>0.824579</td>\n",
       "      <td>0.825175</td>\n",
       "      <td>0.958694</td>\n",
       "      <td>1.209006</td>\n",
       "      <td>1.318852</td>\n",
       "      <td>1.250994</td>\n",
       "      <td>1.233046</td>\n",
       "      <td>1.189687</td>\n",
       "      <td>1.164012</td>\n",
       "      <td>...</td>\n",
       "      <td>0.804105</td>\n",
       "      <td>0.679419</td>\n",
       "      <td>0.765830</td>\n",
       "      <td>0.891037</td>\n",
       "      <td>0.956082</td>\n",
       "      <td>0.931358</td>\n",
       "      <td>0.851681</td>\n",
       "      <td>0.787558</td>\n",
       "      <td>0.947178</td>\n",
       "      <td>0.841641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.003111</td>\n",
       "      <td>-0.006097</td>\n",
       "      <td>-0.008363</td>\n",
       "      <td>-0.020100</td>\n",
       "      <td>-0.009402</td>\n",
       "      <td>-0.008624</td>\n",
       "      <td>-0.011399</td>\n",
       "      <td>-0.019055</td>\n",
       "      <td>-0.011426</td>\n",
       "      <td>-0.018408</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017196</td>\n",
       "      <td>-0.006861</td>\n",
       "      <td>-0.005564</td>\n",
       "      <td>-0.010010</td>\n",
       "      <td>-0.015025</td>\n",
       "      <td>-0.012802</td>\n",
       "      <td>-0.024759</td>\n",
       "      <td>-0.008956</td>\n",
       "      <td>-0.005438</td>\n",
       "      <td>-0.002782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.000033</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>17.829464</td>\n",
       "      <td>24.760633</td>\n",
       "      <td>24.395767</td>\n",
       "      <td>19.312544</td>\n",
       "      <td>40.705914</td>\n",
       "      <td>44.983555</td>\n",
       "      <td>36.911903</td>\n",
       "      <td>35.060871</td>\n",
       "      <td>38.008114</td>\n",
       "      <td>32.354378</td>\n",
       "      <td>...</td>\n",
       "      <td>20.393888</td>\n",
       "      <td>9.897181</td>\n",
       "      <td>18.444706</td>\n",
       "      <td>22.834747</td>\n",
       "      <td>25.352310</td>\n",
       "      <td>21.755194</td>\n",
       "      <td>22.447390</td>\n",
       "      <td>19.458200</td>\n",
       "      <td>25.406208</td>\n",
       "      <td>25.454906</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 1600 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              px_1         px_2         px_3         px_4         px_5  \\\n",
       "count  2416.000000  2416.000000  2416.000000  2416.000000  2416.000000   \n",
       "mean      0.104913     0.111849     0.117341     0.139682     0.154402   \n",
       "std       0.709942     0.824579     0.825175     0.958694     1.209006   \n",
       "min      -0.003111    -0.006097    -0.008363    -0.020100    -0.009402   \n",
       "25%      -0.000033    -0.000002     0.000014     0.000014     0.000014   \n",
       "50%       0.000014     0.000014     0.000014     0.000014     0.000014   \n",
       "75%       0.000014     0.000015     0.000015     0.000016     0.000016   \n",
       "max      17.829464    24.760633    24.395767    19.312544    40.705914   \n",
       "\n",
       "              px_6         px_7         px_8         px_9        px_10  ...  \\\n",
       "count  2416.000000  2416.000000  2416.000000  2416.000000  2416.000000  ...   \n",
       "mean      0.160311     0.155682     0.158670     0.150576     0.136430  ...   \n",
       "std       1.318852     1.250994     1.233046     1.189687     1.164012  ...   \n",
       "min      -0.008624    -0.011399    -0.019055    -0.011426    -0.018408  ...   \n",
       "25%       0.000014     0.000014     0.000014     0.000014     0.000014  ...   \n",
       "50%       0.000014     0.000014     0.000014     0.000014     0.000014  ...   \n",
       "75%       0.000015     0.000015     0.000015     0.000015     0.000015  ...   \n",
       "max      44.983555    36.911903    35.060871    38.008114    32.354378  ...   \n",
       "\n",
       "           px_1591      px_1592      px_1593      px_1594      px_1595  \\\n",
       "count  2416.000000  2416.000000  2416.000000  2416.000000  2416.000000   \n",
       "mean      0.141868     0.131865     0.137651     0.132617     0.124986   \n",
       "std       0.804105     0.679419     0.765830     0.891037     0.956082   \n",
       "min      -0.017196    -0.006861    -0.005564    -0.010010    -0.015025   \n",
       "25%       0.000014     0.000014     0.000014     0.000014     0.000014   \n",
       "50%       0.000014     0.000014     0.000014     0.000014     0.000014   \n",
       "75%       0.000016     0.000016     0.000017     0.000016     0.000016   \n",
       "max      20.393888     9.897181    18.444706    22.834747    25.352310   \n",
       "\n",
       "           px_1596      px_1597      px_1598      px_1599      px_1600  \n",
       "count  2416.000000  2416.000000  2416.000000  2416.000000  2416.000000  \n",
       "mean      0.129675     0.127915     0.124093     0.134919     0.118554  \n",
       "std       0.931358     0.851681     0.787558     0.947178     0.841641  \n",
       "min      -0.012802    -0.024759    -0.008956    -0.005438    -0.002782  \n",
       "25%       0.000013     0.000012     0.000013     0.000005     0.000012  \n",
       "50%       0.000014     0.000014     0.000014     0.000014     0.000012  \n",
       "75%       0.000016     0.000016     0.000039     0.000021     0.000032  \n",
       "max      21.755194    22.447390    19.458200    25.406208    25.454906  \n",
       "\n",
       "[8 rows x 1600 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>px_1</th>\n",
       "      <th>px_2</th>\n",
       "      <th>px_3</th>\n",
       "      <th>px_4</th>\n",
       "      <th>px_5</th>\n",
       "      <th>px_6</th>\n",
       "      <th>px_7</th>\n",
       "      <th>px_8</th>\n",
       "      <th>px_9</th>\n",
       "      <th>px_10</th>\n",
       "      <th>...</th>\n",
       "      <th>px_1591</th>\n",
       "      <th>px_1592</th>\n",
       "      <th>px_1593</th>\n",
       "      <th>px_1594</th>\n",
       "      <th>px_1595</th>\n",
       "      <th>px_1596</th>\n",
       "      <th>px_1597</th>\n",
       "      <th>px_1598</th>\n",
       "      <th>px_1599</th>\n",
       "      <th>px_1600</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "      <td>2416.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.104942</td>\n",
       "      <td>0.111881</td>\n",
       "      <td>0.117359</td>\n",
       "      <td>0.139719</td>\n",
       "      <td>0.154439</td>\n",
       "      <td>0.160340</td>\n",
       "      <td>0.155714</td>\n",
       "      <td>0.158745</td>\n",
       "      <td>0.150610</td>\n",
       "      <td>0.136497</td>\n",
       "      <td>...</td>\n",
       "      <td>0.141888</td>\n",
       "      <td>0.131883</td>\n",
       "      <td>0.137661</td>\n",
       "      <td>0.132635</td>\n",
       "      <td>0.125008</td>\n",
       "      <td>0.129701</td>\n",
       "      <td>0.127943</td>\n",
       "      <td>0.124120</td>\n",
       "      <td>0.134936</td>\n",
       "      <td>0.118565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.709938</td>\n",
       "      <td>0.824574</td>\n",
       "      <td>0.825172</td>\n",
       "      <td>0.958689</td>\n",
       "      <td>1.209001</td>\n",
       "      <td>1.318849</td>\n",
       "      <td>1.250990</td>\n",
       "      <td>1.233036</td>\n",
       "      <td>1.189683</td>\n",
       "      <td>1.164004</td>\n",
       "      <td>...</td>\n",
       "      <td>0.804101</td>\n",
       "      <td>0.679416</td>\n",
       "      <td>0.765829</td>\n",
       "      <td>0.891035</td>\n",
       "      <td>0.956079</td>\n",
       "      <td>0.931354</td>\n",
       "      <td>0.851677</td>\n",
       "      <td>0.787554</td>\n",
       "      <td>0.947176</td>\n",
       "      <td>0.841639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>17.829464</td>\n",
       "      <td>24.760633</td>\n",
       "      <td>24.395767</td>\n",
       "      <td>19.312544</td>\n",
       "      <td>40.705914</td>\n",
       "      <td>44.983555</td>\n",
       "      <td>36.911903</td>\n",
       "      <td>35.060871</td>\n",
       "      <td>38.008114</td>\n",
       "      <td>32.354378</td>\n",
       "      <td>...</td>\n",
       "      <td>20.393888</td>\n",
       "      <td>9.897181</td>\n",
       "      <td>18.444706</td>\n",
       "      <td>22.834747</td>\n",
       "      <td>25.352310</td>\n",
       "      <td>21.755194</td>\n",
       "      <td>22.447390</td>\n",
       "      <td>19.458200</td>\n",
       "      <td>25.406208</td>\n",
       "      <td>25.454906</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 1600 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              px_1         px_2         px_3         px_4         px_5  \\\n",
       "count  2416.000000  2416.000000  2416.000000  2416.000000  2416.000000   \n",
       "mean      0.104942     0.111881     0.117359     0.139719     0.154439   \n",
       "std       0.709938     0.824574     0.825172     0.958689     1.209001   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000014     0.000014     0.000014   \n",
       "50%       0.000014     0.000014     0.000014     0.000014     0.000014   \n",
       "75%       0.000014     0.000015     0.000015     0.000016     0.000016   \n",
       "max      17.829464    24.760633    24.395767    19.312544    40.705914   \n",
       "\n",
       "              px_6         px_7         px_8         px_9        px_10  ...  \\\n",
       "count  2416.000000  2416.000000  2416.000000  2416.000000  2416.000000  ...   \n",
       "mean      0.160340     0.155714     0.158745     0.150610     0.136497  ...   \n",
       "std       1.318849     1.250990     1.233036     1.189683     1.164004  ...   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "25%       0.000014     0.000014     0.000014     0.000014     0.000014  ...   \n",
       "50%       0.000014     0.000014     0.000014     0.000014     0.000014  ...   \n",
       "75%       0.000015     0.000015     0.000015     0.000015     0.000015  ...   \n",
       "max      44.983555    36.911903    35.060871    38.008114    32.354378  ...   \n",
       "\n",
       "           px_1591      px_1592      px_1593      px_1594      px_1595  \\\n",
       "count  2416.000000  2416.000000  2416.000000  2416.000000  2416.000000   \n",
       "mean      0.141888     0.131883     0.137661     0.132635     0.125008   \n",
       "std       0.804101     0.679416     0.765829     0.891035     0.956079   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000014     0.000014     0.000014     0.000014     0.000014   \n",
       "50%       0.000014     0.000014     0.000014     0.000014     0.000014   \n",
       "75%       0.000016     0.000016     0.000017     0.000016     0.000016   \n",
       "max      20.393888     9.897181    18.444706    22.834747    25.352310   \n",
       "\n",
       "           px_1596      px_1597      px_1598      px_1599      px_1600  \n",
       "count  2416.000000  2416.000000  2416.000000  2416.000000  2416.000000  \n",
       "mean      0.129701     0.127943     0.124120     0.134936     0.118565  \n",
       "std       0.931354     0.851677     0.787554     0.947176     0.841639  \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "25%       0.000013     0.000012     0.000013     0.000005     0.000012  \n",
       "50%       0.000014     0.000014     0.000014     0.000014     0.000012  \n",
       "75%       0.000016     0.000016     0.000039     0.000021     0.000032  \n",
       "max      21.755194    22.447390    19.458200    25.406208    25.454906  \n",
       "\n",
       "[8 rows x 1600 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_sub.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>px_1</th>\n",
       "      <th>px_2</th>\n",
       "      <th>px_3</th>\n",
       "      <th>px_4</th>\n",
       "      <th>px_5</th>\n",
       "      <th>px_6</th>\n",
       "      <th>px_7</th>\n",
       "      <th>px_8</th>\n",
       "      <th>px_9</th>\n",
       "      <th>...</th>\n",
       "      <th>px_1591</th>\n",
       "      <th>px_1592</th>\n",
       "      <th>px_1593</th>\n",
       "      <th>px_1594</th>\n",
       "      <th>px_1595</th>\n",
       "      <th>px_1596</th>\n",
       "      <th>px_1597</th>\n",
       "      <th>px_1598</th>\n",
       "      <th>px_1599</th>\n",
       "      <th>px_1600</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>029858_01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003824</td>\n",
       "      <td>0.008850</td>\n",
       "      <td>0.001254</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>029858_02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>029858_03</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.001456</td>\n",
       "      <td>0.075137</td>\n",
       "      <td>0.075360</td>\n",
       "      <td>0.007247</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>029858_05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>029858_07</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>...</td>\n",
       "      <td>1.545871</td>\n",
       "      <td>2.178585</td>\n",
       "      <td>1.545741</td>\n",
       "      <td>0.642400</td>\n",
       "      <td>0.609874</td>\n",
       "      <td>1.825723</td>\n",
       "      <td>4.686034</td>\n",
       "      <td>4.102203</td>\n",
       "      <td>2.479179</td>\n",
       "      <td>1.916396</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1601 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id      px_1      px_2      px_3      px_4      px_5      px_6  \\\n",
       "0  029858_01  0.000000  0.000000  0.000000  0.000060  0.000000  0.000000   \n",
       "1  029858_02  0.000000  0.000004  0.000014  0.000014  0.000014  0.000014   \n",
       "2  029858_03  0.000014  0.000051  0.001456  0.075137  0.075360  0.007247   \n",
       "3  029858_05  0.000000  0.000000  0.000014  0.000014  0.000014  0.000014   \n",
       "4  029858_07  0.000014  0.000014  0.000014  0.000014  0.000014  0.000014   \n",
       "\n",
       "       px_7      px_8      px_9  ...   px_1591   px_1592   px_1593   px_1594  \\\n",
       "0  0.003824  0.008850  0.001254  ...  0.000015  0.000015  0.000013  0.000011   \n",
       "1  0.000014  0.000014  0.000015  ...  0.000014  0.000014  0.000014  0.000014   \n",
       "2  0.000122  0.000015  0.000015  ...  0.000014  0.000014  0.000014  0.000013   \n",
       "3  0.000014  0.000014  0.000014  ...  0.000014  0.000014  0.000014  0.000014   \n",
       "4  0.000014  0.000014  0.000014  ...  1.545871  2.178585  1.545741  0.642400   \n",
       "\n",
       "    px_1595   px_1596   px_1597   px_1598   px_1599   px_1600  \n",
       "0  0.000013  0.000013  0.000015  0.000000  0.000001  0.000046  \n",
       "1  0.000014  0.000014  0.000020  0.000011  0.000005  0.000016  \n",
       "2  0.000014  0.000013  0.000021  0.000013  0.000005  0.000012  \n",
       "3  0.000014  0.000013  0.000016  0.000011  0.000010  0.000017  \n",
       "4  0.609874  1.825723  4.686034  4.102203  2.479179  1.916396  \n",
       "\n",
       "[5 rows x 1601 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sub.to_csv('../D_WEATHER/sub/unetx2_ch9_shuffle_ho0.967_postpro.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit ('pytorch': conda)",
   "language": "python",
   "name": "python37564bitpytorchconda133dde54c45c40c2946593d30b593426"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
